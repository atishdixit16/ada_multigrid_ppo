{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_singlegrid_quarter'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5131bd3ba8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f513078d9e8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 378      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071611784 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.86        |\n",
      "|    explained_variance   | -0.64       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03454525 |\n",
      "|    clip_fraction        | 0.447      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00374    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.701      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03573232 |\n",
      "|    clip_fraction        | 0.458      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00124    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00301    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030601244 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00539    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.713       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040795945 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0882     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00229     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047549043 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045685787 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.729      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04374043 |\n",
      "|    clip_fraction        | 0.481      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0557    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05085764 |\n",
      "|    clip_fraction        | 0.485      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041601393 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.09        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042629927 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.11        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048662223 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06281271 |\n",
      "|    clip_fraction        | 0.494      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.2        |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 487         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052917052 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0925     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.736       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061812293 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.736      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05109284 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.3        |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048280817 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048708476 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.066      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052609287 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.4         |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054532938 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059489477 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00108     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 481       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0585882 |\n",
      "|    clip_fraction        | 0.552     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.44      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0294   |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.0482   |\n",
      "|    std                  | 0.178     |\n",
      "|    value_loss           | 0.00137   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05957352 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052519597 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058969725 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.61        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000243   |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.772      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06373503 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06501575 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.68       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051936734 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0719     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07438328 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.79       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06692326 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062172912 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06330013 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.98       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0391    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059190728 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06229939 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.08       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0794    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.784      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06865124 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05645574 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.789       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061112154 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.31        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0685     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07695669 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.36       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.792     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0773824 |\n",
      "|    clip_fraction        | 0.581     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.41      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0663   |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00117   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068699375 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.48        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07169143 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064957775 |\n",
      "|    clip_fraction        | 0.582       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.63        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.802      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07421793 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.72       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.805    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 471      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 5        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.066572 |\n",
      "|    clip_fraction        | 0.578    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.78     |\n",
      "|    explained_variance   | 0.979    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0416  |\n",
      "|    n_updates            | 880      |\n",
      "|    policy_gradient_loss | -0.0332  |\n",
      "|    std                  | 0.168    |\n",
      "|    value_loss           | 0.00111  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05657038 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061324097 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.87        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0639     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058894586 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.94        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.81      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 461       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0796905 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.01      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0314    |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0328   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.0011    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07091652 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07064302 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.23       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0015    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08055522 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.31       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0719471 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.4       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0498   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0322   |\n",
      "|    std                  | 0.164     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05825283 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0432    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 485       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0747148 |\n",
      "|    clip_fraction        | 0.6       |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.6       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0635   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.000956  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06105361 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.7        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080220796 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.84        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000874    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07097624 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0304    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000877   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079861596 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.07        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.000821    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05854218 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.17       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000831   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07405409 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062092163 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000828    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 495        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08321391 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.47       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0973    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.821     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 486       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0797853 |\n",
      "|    clip_fraction        | 0.597     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.54      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0424   |\n",
      "|    n_updates            | 1260      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    std                  | 0.156     |\n",
      "|    value_loss           | 0.00075   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06940863 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06982125 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07479205 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.77       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000728   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078808546 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.88        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000739    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09472382 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07766579 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07774629 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09312196 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084135614 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0731     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000654    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09430981 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06655766 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.18       |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07956515 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0185     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09485544 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0838    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079657935 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073997006 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000635    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08437343 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0393    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06543742 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082806155 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000566    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 496        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07732544 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545779 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 483       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0909723 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.017    |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.000665  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06803899 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09224341 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.084     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09557831 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0776     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08764621 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06973883 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0317    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09591163 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07945833 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08959212 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0307     |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08115378 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0843     |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09341939 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0452     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085557364 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000725    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087084375 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0723     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000744    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08045194 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09031649 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00342    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10655248 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000745   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076930925 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000739    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09141867 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000761   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09557353 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00978   |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000705   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10180227 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00713    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000715   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09479523 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104896165 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00076     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 482       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0965658 |\n",
      "|    clip_fraction        | 0.596     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0297   |\n",
      "|    n_updates            | 2120      |\n",
      "|    policy_gradient_loss | -0.014    |\n",
      "|    std                  | 0.137     |\n",
      "|    value_loss           | 0.000733  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08427891 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0863    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091382466 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000771    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08589108 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000773   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09710945 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07873072 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0633    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000739   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 492       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1032151 |\n",
      "|    clip_fraction        | 0.619     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.5      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00966  |\n",
      "|    n_updates            | 2240      |\n",
      "|    policy_gradient_loss | -0.0144   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.000749  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10436082 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08836809 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0815    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000748   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10017518 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110102855 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00522     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000783    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11439452 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097527206 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0787     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000759    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094823785 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0681     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000748    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10260036 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000679   |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000726   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10746379 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00271   |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0960034 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0429    |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.000749  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09843139 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10269375 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0455    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000687   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11153872 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09057574 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.171      |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10295546 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08684306 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09297703 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0846    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11521183 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093786836 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000678    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12384792 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0664    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08256439 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 502         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114128135 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0501     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000657    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11560547 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000692   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10054098 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000914  |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10726335 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.062     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12114243 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00123   |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 480       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0877466 |\n",
      "|    clip_fraction        | 0.629     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00922  |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | -0.0133   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000655  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 493        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11197393 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0395     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09639653 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0095    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09956314 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11231403 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.00897   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11161347 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00173   |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000632   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11063252 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13026857 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108715095 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0243     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000605    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10717616 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0576     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09885539 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10712805 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000598   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12762815 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09470865 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00956   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103132464 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.074       |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000613    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101558305 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000541    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12612131 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0194     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10015452 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000491   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104859255 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000528    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10505147 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0138     |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08835331 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00161    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120234504 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000595    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14252767 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.137      |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12324281 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00556   |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09344385 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11015282 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000585   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12996532 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0811    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000632   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12119062 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00483   |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000606   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 471       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1326338 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0214   |\n",
      "|    n_updates            | 3340      |\n",
      "|    policy_gradient_loss | -0.0127   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000604  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10063958 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105232455 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000544    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13289519 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000554   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14009188 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0299     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106534086 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0233      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000593    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12383628 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0205     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13293326 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000569   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 495        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12874034 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0493    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00985   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102585986 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0678      |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.00949    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000558    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11886346 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0011     |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000525   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 500       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1263232 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0492    |\n",
      "|    n_updates            | 3560      |\n",
      "|    policy_gradient_loss | -0.0125   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000591  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12632208 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12508969 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00828   |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00787   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116241656 |\n",
      "|    clip_fraction        | 0.651       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000656    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13543378 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0608    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15880701 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 492       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1279564 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0298   |\n",
      "|    n_updates            | 3680      |\n",
      "|    policy_gradient_loss | -0.00955  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000654  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14241174 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12520567 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13614336 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12245824 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0667    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.00888   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000533   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13467968 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.001     |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000596   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15040314 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00884   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11392144 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0701    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13120103 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14423244 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00197    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000535   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12315979 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0061    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000518   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 493        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11912737 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00053    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107154645 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0366      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.118       |\n",
      "|    value_loss           | 0.000526    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11781256 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0668    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00943   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000485   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12395649 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0726    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000493   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 489       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1315845 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0109    |\n",
      "|    n_updates            | 3980      |\n",
      "|    policy_gradient_loss | -0.012    |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000483  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 493       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1375643 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0249    |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | -0.00728  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000476  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13425244 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00842   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00052    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15066127 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.013     |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00449   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000543   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12949494 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00599   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000554   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12689438 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000502   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13922736 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12816682 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00953   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000562   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13755736 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0465     |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00733   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000506   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11007158 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0275     |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.00914   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000583   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13327986 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15438573 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.00179   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15328646 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0405    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00508   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13109505 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0608    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00997   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15505585 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | 0.0093     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 459       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1686921 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0744   |\n",
      "|    n_updates            | 4280      |\n",
      "|    policy_gradient_loss | 0.000648  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000611  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12952463 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15118603 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.000518  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.0006     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 461       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1588575 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.043    |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | -0.00608  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000523  |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 465       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1533542 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0422    |\n",
      "|    n_updates            | 4360      |\n",
      "|    policy_gradient_loss | -0.0045   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000496  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11873174 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0303     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14040482 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0821    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000484   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13254589 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00612   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000472   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12839374 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00564   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000485   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14273496 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0293     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00597   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15217331 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | 0.000557   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13355605 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15182748 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00392   |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | 0.0071     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000557   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13312103 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0204     |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00956   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14434923 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0506    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14114994 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00051    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15153308 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00832   |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00862   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000476   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16146854 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00213   |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15369646 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00895   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000527   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13503084 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0031    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000496   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12632504 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0224    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12348163 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00102    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00992   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 469       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1271522 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0355   |\n",
      "|    n_updates            | 4720      |\n",
      "|    policy_gradient_loss | -0.00905  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000552  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12810726 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0437     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00612   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14923681 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000534   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13260241 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00641    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00863   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000593   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13384359 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000549   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14160173 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00926   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13641375 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15245292 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0923     |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00543   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000533   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 480       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1517981 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00182  |\n",
      "|    n_updates            | 4880      |\n",
      "|    policy_gradient_loss | -0.00175  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000524  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 472       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1272742 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0193   |\n",
      "|    n_updates            | 4900      |\n",
      "|    policy_gradient_loss | -0.0101   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.00054   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14229235 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0242     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.009     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 479      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 5        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.138464 |\n",
      "|    clip_fraction        | 0.678    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.99     |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.106    |\n",
      "|    n_updates            | 4940     |\n",
      "|    policy_gradient_loss | -0.0141  |\n",
      "|    std                  | 0.118    |\n",
      "|    value_loss           | 0.000587 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12842569 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00369   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14368112 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000593   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 485       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1120499 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0233    |\n",
      "|    n_updates            | 5000      |\n",
      "|    policy_gradient_loss | -0.00781  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000592  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15900846 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0709     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00872   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 481       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1563725 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0184   |\n",
      "|    n_updates            | 5040      |\n",
      "|    policy_gradient_loss | -0.000979 |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000579  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13434361 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00572   |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14663962 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00741   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000634   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13733523 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0549    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.00668   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14497772 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00969   |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00713   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124090075 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 5140        |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    std                  | 0.116       |\n",
      "|    value_loss           | 0.000642    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 470       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1532217 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.9      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0552   |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | -0.00424  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000613  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12910223 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0664    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14046276 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0476     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000653   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11718845 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000705   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15176877 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.000219  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000711   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123950675 |\n",
      "|    clip_fraction        | 0.666       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00582    |\n",
      "|    n_updates            | 5260        |\n",
      "|    policy_gradient_loss | -0.00968    |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 0.000703    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12510598 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000716   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12657161 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0281     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11397278 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00571    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000752   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15765336 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00767   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16496542 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 493       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1559593 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.142     |\n",
      "|    n_updates            | 5380      |\n",
      "|    policy_gradient_loss | -0.00774  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000685  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11978308 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00173    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000721   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15195362 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0129    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.000593   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000643   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14578012 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00945   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14324418 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00389   |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00833   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12632452 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0465    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00754   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000594   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15148054 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0083    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15663953 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.000805   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000643   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13304894 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119695924 |\n",
      "|    clip_fraction        | 0.672       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16.2        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0509      |\n",
      "|    n_updates            | 5560        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 0.000615    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15105557 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | 0.00731    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000634   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15931568 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.00888   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000635   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13688594 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0331    |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000635   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15046401 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00317    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.00291    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15417556 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.023      |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00994   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000609   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 483       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1531308 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0627    |\n",
      "|    n_updates            | 5680      |\n",
      "|    policy_gradient_loss | -0.00192  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000593  |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15124717 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00914   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119537935 |\n",
      "|    clip_fraction        | 0.676       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16.2        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0221      |\n",
      "|    n_updates            | 5720        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 0.000638    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14477381 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00619   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12747976 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0388    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00694   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13561633 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00659   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 496        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13890228 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15535854 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00867   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15289554 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00189   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000598   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f511c0e6c88> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f510414dac8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.671     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 445       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1526892 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0864   |\n",
      "|    n_updates            | 5860      |\n",
      "|    policy_gradient_loss | 0.000714  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000644  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051376868 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.0716     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.701       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048658594 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033352904 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.044      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040718857 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0528     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.702      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03411284 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.071     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00229    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.711       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037490245 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.99        |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00201     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041925915 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.726       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043623336 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04546948 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.09       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0383     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030160943 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00162     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05025809 |\n",
      "|    clip_fraction        | 0.492      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.19       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00154    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.735       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054097105 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.22        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04217591 |\n",
      "|    clip_fraction        | 0.499      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.23       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0801    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05191766 |\n",
      "|    clip_fraction        | 0.498      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048750035 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.2         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.731     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0551486 |\n",
      "|    clip_fraction        | 0.51      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.24      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0647   |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.0444   |\n",
      "|    std                  | 0.18      |\n",
      "|    value_loss           | 0.0014    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05785197 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.29       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.725      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04822092 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.33       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04862503 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057611883 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06000798 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.49       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0761    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.741       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054572217 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.55        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06736399 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056733884 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.67        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05385695 |\n",
      "|    clip_fraction        | 0.535      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.744     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0693997 |\n",
      "|    clip_fraction        | 0.538     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.74      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.046    |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    std                  | 0.176     |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05284319 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.81       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 448        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04929874 |\n",
      "|    clip_fraction        | 0.553      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062127255 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066382244 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061261117 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.756     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 457       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0666964 |\n",
      "|    clip_fraction        | 0.57      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.97      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0621   |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0435   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.755     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 460       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0651351 |\n",
      "|    clip_fraction        | 0.567     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.01      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0809   |\n",
      "|    n_updates            | 660       |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 450         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061286975 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0698     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06732507 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.06       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056050707 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.1         |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0856     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00117     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06630887 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07023197 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.22       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0823    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05852921 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07357506 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.38       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07846562 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.44       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0383    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.773     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 469       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0608586 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.51      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067037866 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.55        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07450347 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.58       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076563425 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.62        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.785       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075655535 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.69        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06556271 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.79       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.791     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 478       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0922491 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.88      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07615379 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.98       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072590634 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081946425 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.1         |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07336445 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.2        |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08942877 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.808     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 486       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0684288 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.37      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0287   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0337   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098124005 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083682366 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.58        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000931    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06746669 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0417     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000966   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.819       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076223746 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.77        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000954    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08127213 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.89       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07096313 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9          |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078128785 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.11        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.000906    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07393008 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0154    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072332405 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.34        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000809    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07787631 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00189   |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07440407 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07948859 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07599194 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.69       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09329804 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.81       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07452371 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.91       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08580129 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063462935 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000764    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090517186 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000729    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062073715 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000666    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09799924 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078958824 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07313873 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06864514 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0566    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079242826 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000562    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087600395 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00061     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09132691 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0557     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08345769 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000602   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08290584 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08842277 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00493   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09605677 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000648   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08552261 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00556   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 476       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0883674 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.3      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.000851 |\n",
      "|    n_updates            | 1720      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000606  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10462477 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00396    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086456716 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000611    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09217997 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11325834 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00588    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078990184 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00487    |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00066     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08605505 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09499385 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094972 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09100573 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00163    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1063107 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0178    |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000532  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09971459 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10190296 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0762    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.00856   |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000501   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09537418 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0355     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08605897 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0558     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.00897   |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000447   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08055689 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0698    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000507   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102010116 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0786      |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.00944    |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000517    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11134573 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000496   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069924295 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000476    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087580346 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0266     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000522    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10397639 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0693     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000507   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08610662 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0541    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000484   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10369092 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10255647 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095619395 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.00949    |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000481    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09710902 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00492   |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000448   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09665093 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000445   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088464595 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000481    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080206715 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0498     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.00816    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09300933 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00046    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 467       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1070544 |\n",
      "|    clip_fraction        | 0.625     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.5      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0543   |\n",
      "|    n_updates            | 2320      |\n",
      "|    policy_gradient_loss | -0.011    |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.000462  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077351786 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0852      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00047     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1158361 |\n",
      "|    clip_fraction        | 0.628     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.6      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0632    |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.00045   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08036518 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.00481   |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10976436 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0828     |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090672694 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0542     |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000401    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 489       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0989247 |\n",
      "|    clip_fraction        | 0.636     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0752   |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -0.00798  |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.000407  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10392656 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0242     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10424793 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.00715   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000408   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08237382 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.00918   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11793838 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000396   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10751344 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.00832   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 457         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117026135 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0696     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000379    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10412538 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.00924   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000412   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09825138 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.00676   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09832188 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0066     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09013585 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.00315   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000389   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08722331 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.134      |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.00901   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095423535 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0054     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000367    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106288336 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000397    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 462      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 5        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.108979 |\n",
      "|    clip_fraction        | 0.645    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.2     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0169   |\n",
      "|    n_updates            | 2720     |\n",
      "|    policy_gradient_loss | -0.00995 |\n",
      "|    std                  | 0.131    |\n",
      "|    value_loss           | 0.000361 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09893228 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.00638   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000368   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11492322 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.00882   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000345   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09267527 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00178    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.00428   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000384   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09241109 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.00599   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13047092 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0807    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000399   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08944152 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0368     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.00628   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10434566 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.00816   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00044    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09886112 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.00915   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11348655 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00665   |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.00627   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000364   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 478       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1006038 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.5      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0108   |\n",
      "|    n_updates            | 2920      |\n",
      "|    policy_gradient_loss | -0.00659  |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000366  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11530091 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 483       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1054457 |\n",
      "|    clip_fraction        | 0.649     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.6      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0265    |\n",
      "|    n_updates            | 2960      |\n",
      "|    policy_gradient_loss | -0.00541  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000371  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11767058 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0628    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.00651   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10498135 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0469    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.00569   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101382926 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.0074     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000372    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11670282 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0212     |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00456   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000365   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10641563 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00929    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.00392   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000367   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115012944 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0048     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000395    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116192654 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.00754    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000392    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09686359 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0216     |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.00473   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000394   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094161294 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0938      |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000346    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13022193 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.00556   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000346   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10159105 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.00638   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000346   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10909806 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00165   |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.00388   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000333   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09761438 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.063      |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.000895  |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000359   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12745106 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.00642   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00033    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15319836 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0546    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.00521   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000301   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10968764 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0538     |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0052    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10678874 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00783    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.00679   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11193012 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.00341   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000282   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12051119 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.00747   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000265   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11361285 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0611    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.00733   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000244   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13064581 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.00477   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00031    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12300295 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.00529   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000291   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11544156 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0662     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00472   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000286   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12993802 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.00209   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000293   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 459       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1466918 |\n",
      "|    clip_fraction        | 0.655     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00277  |\n",
      "|    n_updates            | 3460      |\n",
      "|    policy_gradient_loss | -0.00364  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000321  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10825517 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0531    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.00133   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000336   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09902036 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0131     |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00765   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000325   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16354816 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.00714   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000345   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12761782 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00774   |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000351   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13448146 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0313    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.00706   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15064421 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000858  |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.00731   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000368   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12508078 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.012      |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00036    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12094939 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00457    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.00155   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123509504 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.121       |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.000512   |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000356    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12373998 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0778    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00643   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000401   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124638036 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0211     |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000341    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14059265 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00758   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000371   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12921223 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000359   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14021525 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00257   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000378   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14614621 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0765    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.00698   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000352   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122405544 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0025      |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.00578    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000346    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14181708 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13916786 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0367     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00246   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13912371 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00464   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00034    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1279476 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00298  |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | -0.000129 |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000362  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11434211 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0051    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000321   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14372389 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00713   |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00529   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000366   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12097436 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00558   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000381   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16320813 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0617    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | 0.00332    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12996046 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0917    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0052    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00042    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14058565 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0582    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00644   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000407   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13104811 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00413   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000442   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14026472 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0991     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00249   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000435   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15134487 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0114    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | 0.0038     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000403   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12580928 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0235     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -9.94e-05  |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000418   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 472       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1379755 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0651   |\n",
      "|    n_updates            | 4080      |\n",
      "|    policy_gradient_loss | -0.000842 |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000393  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12778679 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00366   |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00215   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00035    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15272656 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00806   |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00239   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000381   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12341398 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0819    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00228   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 477       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1342648 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.041    |\n",
      "|    n_updates            | 4160      |\n",
      "|    policy_gradient_loss | -0.000102 |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000367  |\n",
      "---------------------------------------\n",
      "Early stopping at step 4 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 11 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15253326 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | 0.0259     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000341   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15319279 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0468     |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | 0.00658    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 465       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1407346 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0304   |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | -0.00368  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000353  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13223848 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00233   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000318   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14123976 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000338   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11378028 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00537   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000305   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 434        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11066325 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00371   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000339   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 450         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103334345 |\n",
      "|    clip_fraction        | 0.653       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 4320        |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000326    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 430        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10543759 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0295     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00101   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000318   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12465942 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0365     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00574   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 444        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13528901 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00291   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 463       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1544713 |\n",
      "|    clip_fraction        | 0.685     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0183   |\n",
      "|    n_updates            | 4400      |\n",
      "|    policy_gradient_loss | -0.00547  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.00032   |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 444       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1513575 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0807    |\n",
      "|    n_updates            | 4420      |\n",
      "|    policy_gradient_loss | 0.00028   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000309  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121201396 |\n",
      "|    clip_fraction        | 0.667       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00132     |\n",
      "|    n_updates            | 4440        |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000313    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 480       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1557719 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0263   |\n",
      "|    n_updates            | 4460      |\n",
      "|    policy_gradient_loss | 0.000304  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000323  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12887283 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.029      |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00362   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00033    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13764973 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00565   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000305   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13952419 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0059     |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00159   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000301   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13066408 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00512    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00333   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000324   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15120563 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.024      |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | 0.00328    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000285   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 485       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1480408 |\n",
      "|    clip_fraction        | 0.686     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0301   |\n",
      "|    n_updates            | 4580      |\n",
      "|    policy_gradient_loss | -0.00637  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000288  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16038007 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | 0.000522   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15083149 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0339     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | 0.00219    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000304   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13557234 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0441    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00355   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000293   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 481       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1596065 |\n",
      "|    clip_fraction        | 0.676     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0628    |\n",
      "|    n_updates            | 4660      |\n",
      "|    policy_gradient_loss | -0.00175  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000313  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13669121 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00761   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000276   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14985888 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00116   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000277   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111140296 |\n",
      "|    clip_fraction        | 0.669       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0498     |\n",
      "|    n_updates            | 4720        |\n",
      "|    policy_gradient_loss | -0.000498   |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000321    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14106269 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0168    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00279   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000293   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15826362 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00982   |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00527   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000324   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15194133 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.271      |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | 0.00312    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000315   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16609952 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0312     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | 0.00489    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000295   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 489       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1459821 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0508   |\n",
      "|    n_updates            | 4820      |\n",
      "|    policy_gradient_loss | 0.000763  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.00029   |\n",
      "---------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 13 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15016574 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | 0.0136     |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00029    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13410874 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00423   |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00769   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00027    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15119132 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0232     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | 0.00287    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000263   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13651621 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0213     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.000948  |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000229   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15327945 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.000275  |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000273   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 479       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1574755 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0277    |\n",
      "|    n_updates            | 4940      |\n",
      "|    policy_gradient_loss | -0.00264  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000251  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15501444 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0372     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00549   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000271   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14883462 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0546    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00672   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000254   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13330153 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00461   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000283   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13552494 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000474  |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.000827  |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000253   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12765913 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.00231   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000279   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12372836 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00146   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000285   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 464       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1511856 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00933  |\n",
      "|    n_updates            | 5080      |\n",
      "|    policy_gradient_loss | 0.00143   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.00029   |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 477       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.154977  |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00869  |\n",
      "|    n_updates            | 5100      |\n",
      "|    policy_gradient_loss | -0.000835 |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000309  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15305206 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00622   |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | 0.00593    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000301   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16114828 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | 0.00915    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000294   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 486       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1552861 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00271  |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | 0.00423   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000296  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15473276 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0581     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | 0.00439    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00031    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 476       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1538102 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0497    |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | 0.00701   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000261  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15095685 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0525     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0053    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000282   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15871903 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.00314   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000288   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15073097 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | 0.00767    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14507274 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0569     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00463   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000297   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13691644 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000589  |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00384   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000277   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12084217 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0582    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | 0.000126   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000302   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 491       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1521569 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.996     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0298   |\n",
      "|    n_updates            | 5340      |\n",
      "|    policy_gradient_loss | 0.000948  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000283  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12904707 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0913     |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | 0.00287    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000305   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11967107 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00894   |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | 0.00026    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000295   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15080734 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00771    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000298   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12884995 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.00424   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000279   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15837541 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.088      |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | 0.00308    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000295   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15231259 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0373     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | 0.00618    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000271   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12385485 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00582   |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00584   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14199036 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0265    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000274   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15342042 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0438    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000279   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13875285 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0672    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.00575   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000299   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14004448 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0516     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000274   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12813899 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00418    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00546   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000308   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15837774 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | 0.00868    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000308   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15140715 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0485     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | 0.00838    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00031    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15300623 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.003      |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000315   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13320509 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00289    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00565   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 502        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14657673 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0721    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00773   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 504        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15256979 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00929    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | 0.00151    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000409   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12842372 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00358   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15096645 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.158      |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | 0.00633    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000442   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12563357 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.839     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1524726 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0192    |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | -0.00156  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000363  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.839     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506806 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0785    |\n",
      "|    n_updates            | 5800      |\n",
      "|    policy_gradient_loss | 0.00449   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000351  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15180068 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0315    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00326   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15736212 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00713    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | 0.00566    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000341   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f511c0bbb38> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f51040bbb00>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 440        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15199842 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00998   |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | 0.00238    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000344   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.674      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06958654 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.89       |\n",
      "|    explained_variance   | -0.376     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0125     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047490202 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0861     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.711      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03339555 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.94       |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00295    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 449         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044822954 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037756953 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.718       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040763944 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043451317 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00177     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.719       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 487         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041048933 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048675895 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.727      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04694677 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046830624 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.734     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 434       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0459747 |\n",
      "|    clip_fraction        | 0.503     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.98      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0434   |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00152   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036458228 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 503        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04491026 |\n",
      "|    clip_fraction        | 0.519      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.11       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046822768 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0657     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.741      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04647106 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056340653 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0644     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053496473 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060202397 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0763     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05329554 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.35       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045318723 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0645     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06090635 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.44       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056513917 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.5         |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.762     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 469       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0578928 |\n",
      "|    clip_fraction        | 0.548     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.57      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.177     |\n",
      "|    value_loss           | 0.0013    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 450        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05858321 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056070495 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.65        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055896807 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0704     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06903891 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.74       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057420634 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.77        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05457897 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.83       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054109592 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058915943 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.02        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00935     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06029724 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.07       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07435803 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.17       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06099558 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05521869 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0398    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.783     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 454       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0634351 |\n",
      "|    clip_fraction        | 0.578     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.36      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0338   |\n",
      "|    n_updates            | 740       |\n",
      "|    policy_gradient_loss | -0.0407   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07324021 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.46       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.787       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770976 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.6         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06914873 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.64       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06435315 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.68       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072419755 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0428     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06710909 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.85       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068342045 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.91        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07601108 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.05       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 458        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06550066 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0342     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093846254 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0915     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070667684 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08304366 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.48       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.799     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 472       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0701161 |\n",
      "|    clip_fraction        | 0.599     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.59      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0212   |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.0361   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07460266 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.65       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 473       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0819386 |\n",
      "|    clip_fraction        | 0.595     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.68      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.161     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07067679 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.73       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.804       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077644095 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.81        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.805       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086551845 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.89        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000756   |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08583589 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0376    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07476324 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.01       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00233    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09169489 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.08       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07583239 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08412103 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.29       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081914276 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0575     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09545218 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.45       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08993331 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07361041 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08226754 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0878    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075846694 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000996    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10608163 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.97       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00673   |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08148558 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093813166 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000882    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09656456 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770022 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0517     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000919    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089062504 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000908    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07812403 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0799     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000899   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07990335 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000896   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08633542 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09878737 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0727    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08626648 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08954557 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00716   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09657636 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08019618 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0501    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07889671 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00167    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08969821 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00949   |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09659344 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0113     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08663081 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10572666 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097353496 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000679    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081000626 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000712    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10313401 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0191     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10554282 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07711329 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08266686 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000715   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07703554 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0502    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09163483 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08696411 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 471       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1004765 |\n",
      "|    clip_fraction        | 0.632     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.012    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0177   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.000621  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095734775 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000596    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09113147 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08722533 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08641001 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000495  |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083861575 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0452     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000579    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10983171 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091132686 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.112       |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00057     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10521436 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081894204 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000604    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10069241 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00517   |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09575027 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0087    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11269144 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.826     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 485       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0776418 |\n",
      "|    clip_fraction        | 0.629     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.5      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00813  |\n",
      "|    n_updates            | 2160      |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00062   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092709616 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00431    |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09191588 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09007683 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000707   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086091906 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00065     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08884716 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000626   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09109971 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000637   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.828     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 467       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0986528 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0367   |\n",
      "|    n_updates            | 2300      |\n",
      "|    policy_gradient_loss | -0.0197   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.000703  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10439227 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00061    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11013589 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0143    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000562   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11004744 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0944    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.829     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 479       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0993855 |\n",
      "|    clip_fraction        | 0.632     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.1      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0407   |\n",
      "|    n_updates            | 2380      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.000542  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 497        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10887082 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10297306 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00372    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000642   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111961566 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.0007      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12133334 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0657    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000668   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 464       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1227139 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0905   |\n",
      "|    n_updates            | 2480      |\n",
      "|    policy_gradient_loss | -0.0162   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000598  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11136971 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0337     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11422545 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00359   |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000587   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11930071 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0762     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000615   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11685958 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00831    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00066    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.829     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1076777 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00598   |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | -0.0174   |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000621  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12141217 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10490868 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13672182 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11083965 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000679   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108590245 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0519     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000686    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11785881 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.02       |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110379145 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000731    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11782424 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10538509 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00101   |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000655   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 479       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0986459 |\n",
      "|    clip_fraction        | 0.643     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.058    |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.000634  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13287468 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00915    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1371363 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00179  |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.000626  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 489       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1382039 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0555   |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | -0.0139   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000718  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12578069 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109912336 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000704    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11482956 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000668   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09676291 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00968   |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11170079 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0777     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000619   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12626664 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0853    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10388508 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0748    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11777978 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0433    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000639   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14544372 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000539   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11275466 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0168     |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00992   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000571   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11190176 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0727     |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000567   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11994068 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0487     |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11052835 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00058    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12882173 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12614968 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115993395 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000581    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13489851 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0346    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1268475 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0249   |\n",
      "|    n_updates            | 3200      |\n",
      "|    policy_gradient_loss | -0.0155   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000599  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13420618 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13089369 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000495   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120229505 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00411    |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0091     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000551    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13815863 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0193    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13161398 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0346    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000558   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14351714 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0186    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000594   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15255761 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000929  |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.00572   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13187185 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0103     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000624   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13459025 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00567    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15607914 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000239  |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.0006     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13152951 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0166     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00061    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15100957 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00385   |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.00791   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15792447 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0093    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15471815 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0683    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.00259   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15110579 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00428   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15845576 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0494     |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | 0.00177    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000706   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15307416 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00856   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000693   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14030582 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 480       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1542321 |\n",
      "|    clip_fraction        | 0.651     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0208   |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | 0.00283   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000745  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15813588 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00521   |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | 0.00302    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15287887 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | 0.000262   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16164716 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00835   |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | 0.00332    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15375671 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | 0.0138     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 482       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1537338 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0172   |\n",
      "|    n_updates            | 3680      |\n",
      "|    policy_gradient_loss | -0.00945  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.00078   |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15058291 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0683     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000783   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15899917 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0346    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | 0.0104     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00084    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15431638 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0862     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00223   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000802   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15299687 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.00845   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1507295 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0989    |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | -0.0112   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000774  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14025971 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00836   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 495        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15503654 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.073     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00873   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12510164 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000797   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1261611 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.000151  |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | -0.0143   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000826  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15085207 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0477     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | 0.000277   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000908   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15645882 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00737    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00883   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000866   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15321049 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00871   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000922   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14062122 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0372    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000831   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15131809 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0751     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.00212   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15230756 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00118    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00469   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000857   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16479978 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0202    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000827   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13005634 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.014      |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000891   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16321424 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00196   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15156944 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0018    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14951794 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000932   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15389761 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00449    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00189   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15191606 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00999   |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00135   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 461       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1262124 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0133   |\n",
      "|    n_updates            | 4140      |\n",
      "|    policy_gradient_loss | -0.00932  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000808  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15731676 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.00414   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15615442 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00381    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.00324   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12395322 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0072    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15016016 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00721   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000752   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15332079 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.000368  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14814167 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00966   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000739   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13931406 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0212     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00824   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14218016 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0664    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00381   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000683   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14335798 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0175    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13838632 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15415512 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.034     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00152   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 12 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15534635 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | 0.0106     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00082    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13181052 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00766   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14056103 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00848   |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00659   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000793   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15958634 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00803   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12828526 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00187    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00838   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15903075 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00642   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000847   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15028232 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0555    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00305   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16741665 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00984   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000784   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 477       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1271716 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.149     |\n",
      "|    n_updates            | 4540      |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000764  |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15581076 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0253    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.00863   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15112099 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0707    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000852   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15026185 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | 0.0045     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000872   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15941098 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.00555   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13400614 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0744    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000828   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16097212 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00746   |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00701   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16115366 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14917476 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0327     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000813   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15648866 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0217     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | 0.00305    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 14 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15042455 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | 0.0088     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000785   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16612223 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0581    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00709   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12259223 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15367013 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0571     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00331   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00081    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15089884 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00244    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00653   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000761   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13573818 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00842   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15270163 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15102175 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00813   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15382865 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.041     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00823   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000637   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13249587 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.057      |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.008     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14009276 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00552   |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00667   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15222529 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000618   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15784094 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.016     |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00235    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13225578 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0327     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00967   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14902945 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00737   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13194528 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15653078 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00651   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 483       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1537039 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0158    |\n",
      "|    n_updates            | 5080      |\n",
      "|    policy_gradient_loss | 0.00395   |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000745  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16046341 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.00341   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13510063 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00143   |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15774587 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0641    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00248   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000678   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 14 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1521443 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0457    |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | 0.01      |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000647  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13864528 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0366    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12802263 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0556     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.00475   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15845658 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0232    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00445   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14725396 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15417175 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00241   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14189689 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0635     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000768   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15071882 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0565     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00342   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 486       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1545786 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.046     |\n",
      "|    n_updates            | 5320      |\n",
      "|    policy_gradient_loss | 0.00597   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000693  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15157643 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0305     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00311   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000669   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15505609 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00345   |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | 0.00283    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000689   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15113673 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0396    |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16208902 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00788   |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.00201   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000787   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16026454 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.00236    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15989316 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00248    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.000772  |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000821   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16168526 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00248   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000813   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14954042 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0688     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00937   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15092719 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00458   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 488       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1568278 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.137     |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | 0.000405  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000812  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15185183 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00491   |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.00331   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15282337 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0159     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00289    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 487       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1583271 |\n",
      "|    clip_fraction        | 0.678     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0218   |\n",
      "|    n_updates            | 5580      |\n",
      "|    policy_gradient_loss | -0.00617  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000774  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15310608 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00493   |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0022    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1514442 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.062    |\n",
      "|    n_updates            | 5620      |\n",
      "|    policy_gradient_loss | 0.000493  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000719  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15198985 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00971   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13594474 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0232     |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00551   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 487       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1520572 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0159   |\n",
      "|    n_updates            | 5680      |\n",
      "|    policy_gradient_loss | 0.00991   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000709  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13410991 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0603     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00301   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15218496 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00174    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000773   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15381716 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0605    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.000917  |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15018006 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.0061    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 14 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15981206 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0338     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | 0.0105     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12467007 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.00873   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15236965 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00736   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 494        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15252535 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00404   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.3, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25],\n",
    "                                episode_limit_array=[150000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
