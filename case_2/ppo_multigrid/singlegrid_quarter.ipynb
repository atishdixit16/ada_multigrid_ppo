{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_singlegrid_quarter'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fd63c3d3ef0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fd63b392f28>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 135      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.679     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0707188 |\n",
      "|    clip_fraction        | 0.445     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.85      |\n",
      "|    explained_variance   | -0.64     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0213   |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0366   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.0156    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03175534 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00377    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035791736 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.87        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031586558 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00767    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.704       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 751         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038644783 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0755     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 742         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042874157 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00205     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.715       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042074747 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.712       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046469264 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.06        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.717    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 753      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.049085 |\n",
      "|    clip_fraction        | 0.493    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.11     |\n",
      "|    explained_variance   | 0.961    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0788  |\n",
      "|    n_updates            | 180      |\n",
      "|    policy_gradient_loss | -0.0467  |\n",
      "|    std                  | 0.181    |\n",
      "|    value_loss           | 0.00163  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04640566 |\n",
      "|    clip_fraction        | 0.504      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.15       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.718      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04995772 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 746         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055400204 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.728      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05184882 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.27       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05387343 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.31       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0763    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05276501 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.36       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052379556 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0798     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05409802 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.46       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.746       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054557253 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0715     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04908336 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.58       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050528366 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06235838 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0886    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06051864 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 752        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05880314 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0926    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 715        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04857253 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 737        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07528596 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061827272 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0699     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.773      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 752        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06836298 |\n",
      "|    clip_fraction        | 0.545      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05624243 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056877136 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.74        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061973475 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07079506 |\n",
      "|    clip_fraction        | 0.573      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.84       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0771    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.78     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 758      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.064237 |\n",
      "|    clip_fraction        | 0.559    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.92     |\n",
      "|    explained_variance   | 0.971    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0718  |\n",
      "|    n_updates            | 640      |\n",
      "|    policy_gradient_loss | -0.0414  |\n",
      "|    std                  | 0.174    |\n",
      "|    value_loss           | 0.00137  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065170564 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060446948 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.98        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06369358 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06640868 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.14       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.783       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065608464 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.22        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07560213 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.27       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069437705 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00325    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07151332 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.41       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 726        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06294056 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.45       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071153745 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.5         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07178465 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06554037 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07847029 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.78       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0585    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06510444 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.86       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07782781 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.91       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061063003 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0656     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07259835 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.09       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07358284 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.25       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.801       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064116515 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07665865 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08085267 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07606259 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072046354 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00567     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08402011 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0942    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068703726 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.94        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08006798 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07962311 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.13       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000755   |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000951   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 782         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064977705 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.21        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000974    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07093988 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.28       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0852    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084386334 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000869    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07447239 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.44       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00909    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000836   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08865658 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281404 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083601676 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00136    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000843    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072260655 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.72        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0823     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000846    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09999265 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.83       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000853   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.92       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.818     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 758       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0800878 |\n",
      "|    clip_fraction        | 0.615     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.99      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0538   |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0258   |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.000854  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086556196 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.000774    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08638317 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091239884 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000851    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.82      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0784907 |\n",
      "|    clip_fraction        | 0.604     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0808   |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.00081   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081656635 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000784    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281746 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.092     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07756476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000834   |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07710326 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000222   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082571015 |\n",
      "|    clip_fraction        | 0.6         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0713     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000768    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09877135 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0825    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06918831 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000692   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090791225 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0536     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000722    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08319668 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0869     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.824     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 759       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0894752 |\n",
      "|    clip_fraction        | 0.601     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00128  |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.0211   |\n",
      "|    std                  | 0.146     |\n",
      "|    value_loss           | 0.000713  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07546101 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000146   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000757   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10213168 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0851    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 807         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072928384 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.00085     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105452314 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.000761    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09592082 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10572268 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0814404 |\n",
      "|    clip_fraction        | 0.617     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0501   |\n",
      "|    n_updates            | 1820      |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000744  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08519796 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095315434 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0511     |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000696    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09649859 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0323     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000707   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104618594 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000784    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.826     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0859244 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.5      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0623   |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -0.0175   |\n",
      "|    std                  | 0.142     |\n",
      "|    value_loss           | 0.000707  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08383379 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10461812 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.91e-05   |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08968534 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.016     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09872158 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08073708 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06542909 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0285     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08363346 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09170034 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0566     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095518105 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0556     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000728    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09115486 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000711   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09365664 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000714   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 801         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090897754 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0614     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000748    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08829075 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000716   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 785         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096776985 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000665    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093520984 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000763    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09178828 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000692   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091054276 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00726     |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000697    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07460666 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0768     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08595003 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0379     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09709382 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0187    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00068    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10674095 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08734362 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097796395 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0355     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000604    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09962393 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10346468 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000567   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09157182 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10549698 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000246   |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08515687 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.069     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1017768 |\n",
      "|    clip_fraction        | 0.636     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0964    |\n",
      "|    n_updates            | 2500      |\n",
      "|    policy_gradient_loss | -0.0178   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.000543  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 824        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09016152 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 762       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1137263 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0133   |\n",
      "|    n_updates            | 2540      |\n",
      "|    policy_gradient_loss | -0.0153   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.00047   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14032719 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10380657 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 828        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10117066 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0393    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000491   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10510018 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000543   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10837741 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09100894 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 5.55e-05   |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1051476 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0463    |\n",
      "|    n_updates            | 2680      |\n",
      "|    policy_gradient_loss | -0.0152   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.000579  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.83     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 749      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.088402 |\n",
      "|    clip_fraction        | 0.629    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.3     |\n",
      "|    explained_variance   | 0.991    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0302   |\n",
      "|    n_updates            | 2700     |\n",
      "|    policy_gradient_loss | -0.0167  |\n",
      "|    std                  | 0.131    |\n",
      "|    value_loss           | 0.000506 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100917615 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0794     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000479    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10884969 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00806    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000465   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08364071 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000492   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 823         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102970935 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0539     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000486    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 799         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112689994 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000496    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11083932 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0334     |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11438098 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000514   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098548576 |\n",
      "|    clip_fraction        | 0.641       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0553     |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000435    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10556503 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000464   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09040365 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000498   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10920863 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.00745   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000474   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0890076 |\n",
      "|    clip_fraction        | 0.634     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0103   |\n",
      "|    n_updates            | 2940      |\n",
      "|    policy_gradient_loss | -0.0124   |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000465  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100399375 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000533    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09576799 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.00995   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000483   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.832    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 792      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.127802 |\n",
      "|    clip_fraction        | 0.641    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.9     |\n",
      "|    explained_variance   | 0.992    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0305  |\n",
      "|    n_updates            | 3000     |\n",
      "|    policy_gradient_loss | -0.0147  |\n",
      "|    std                  | 0.127    |\n",
      "|    value_loss           | 0.000553 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11737724 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000571   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116623774 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0587      |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000588    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1072333 |\n",
      "|    clip_fraction        | 0.637     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14        |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0297    |\n",
      "|    n_updates            | 3060      |\n",
      "|    policy_gradient_loss | -0.0166   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000577  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12221859 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000579   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095906354 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000516    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13081217 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10771732 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000569   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 823        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11365626 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0198    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10094629 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0175    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000596   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09014984 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0574    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000554   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12518862 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13679984 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112598814 |\n",
      "|    clip_fraction        | 0.651       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000649    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117901884 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.00066     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13057534 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14070337 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000642   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11231651 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.00979   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000633   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13827422 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0679    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14007315 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0559    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13404898 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0232    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 796       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1262135 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0313   |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | -0.0115   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000598  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12508996 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000555   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14138596 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00145   |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1445542 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0202   |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -0.0143   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000588  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13590142 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0667    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000576   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11673667 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0081     |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 788       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1370615 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.015    |\n",
      "|    n_updates            | 3540      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000599  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12293334 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 821        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12377636 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14050479 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.832     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1274902 |\n",
      "|    clip_fraction        | 0.646     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0285   |\n",
      "|    n_updates            | 3620      |\n",
      "|    policy_gradient_loss | -0.011    |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000563  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14146525 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00222   |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13459076 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000539   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15276682 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11865959 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00992   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000534   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13841979 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 834        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15450247 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0379     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14131191 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00054    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12915465 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 804        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12336941 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0212     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000514   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13282648 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0658     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000492   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 819         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123999216 |\n",
      "|    clip_fraction        | 0.653       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0469     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.00053     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 828        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12983152 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000534   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 807        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13168465 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00414   |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13450055 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.035      |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12602794 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000541   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 831        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11664202 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00833   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15456264 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0863     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13371137 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15607166 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0171     |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00482   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 814        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15888612 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0845     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.006     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 815        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15022412 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.006     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000543   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 814        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15278871 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00364   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 827        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12766781 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00808   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13477793 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13000226 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0027    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.0005     |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 819        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15031108 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00945   |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 820        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14737947 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00558   |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.00951   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15381068 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0499     |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 815        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15720579 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.000904  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 790        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11637714 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0122     |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00897   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14534943 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00161   |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00733   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15355687 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0783     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00213   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000536   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13355234 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00803   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14061584 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.024      |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15801245 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00849   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000602   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13833185 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16991928 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00619   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15730622 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00385   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000619   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15210827 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13497575 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000342  |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00895   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000579   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15524313 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00847   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000557   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13284439 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0068    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00517   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000567   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 824        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15855375 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00874    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0077    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13712339 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0808    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00935   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11953692 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00641   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13169703 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 826        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15113744 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0029    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 833        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15285341 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00341   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13138707 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00773   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15551594 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0674     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0052    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000693   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15057889 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0279     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -8.02e-05  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 822        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15287086 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0047    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000675   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16148768 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00469   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15398006 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0121     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | 0.00921    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 810        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15425451 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00566    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000705   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13739799 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0352     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16198109 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0467    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | 0.00104    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15440425 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00309   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15078524 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | 0.00312    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000635   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 817        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15516488 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -3.39e-05  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000598   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 801        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12951367 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00541   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14890058 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0635    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16104946 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0358     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.0011    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 814        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13687134 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00492   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 782       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1569517 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0481   |\n",
      "|    n_updates            | 4920      |\n",
      "|    policy_gradient_loss | -0.000294 |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000563  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13291463 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0393     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00648   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000587   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15986264 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0738     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | 0.00249    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15057847 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00627   |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00757    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14387201 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0184    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00727   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 771      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.15171  |\n",
      "|    clip_fraction        | 0.673    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.8     |\n",
      "|    explained_variance   | 0.992    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0493  |\n",
      "|    n_updates            | 5020     |\n",
      "|    policy_gradient_loss | -0.00498 |\n",
      "|    std                  | 0.117    |\n",
      "|    value_loss           | 0.000487 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14426586 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0051    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000557   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12720889 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00699   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13144672 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0956     |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00514   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15000172 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.00238    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13104264 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0187    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00895   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15173951 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00403   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000578   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13599482 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.022      |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00786   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000555   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15779515 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00485   |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | 0.00298    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00054    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 771       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1562821 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0505    |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | -0.00491  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000538  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15150896 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00401    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00714   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16624613 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00357    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | 0.00032    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15404591 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00147   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 762       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1538264 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | -0.00524  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000544  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15160497 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00335   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000533   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15064302 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00862    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00787   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 845        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15409823 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0978     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00245   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15613508 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | 0.0042     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13583604 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00327   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000597   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 769       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1401161 |\n",
      "|    clip_fraction        | 0.683     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0546    |\n",
      "|    n_updates            | 5400      |\n",
      "|    policy_gradient_loss | -0.00539  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000598  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15053587 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00217   |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.00336    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15078953 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0431    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00284   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15752418 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00309   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 823       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1515577 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0167   |\n",
      "|    n_updates            | 5480      |\n",
      "|    policy_gradient_loss | -0.00014  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000648  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15503863 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | 0.000703   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15338129 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0411     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000714   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.154298 |\n",
      "|    clip_fraction        | 0.67     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 16.1     |\n",
      "|    explained_variance   | 0.99     |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00775  |\n",
      "|    n_updates            | 5540     |\n",
      "|    policy_gradient_loss | -0.00053 |\n",
      "|    std                  | 0.115    |\n",
      "|    value_loss           | 0.000601 |\n",
      "--------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1563604 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00472   |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | -0.00527  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000633  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 755        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14383826 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0721    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00413   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15159349 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0349     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | 0.00099    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15258761 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0327     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00209   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000562   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120959915 |\n",
      "|    clip_fraction        | 0.68        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0384     |\n",
      "|    n_updates            | 5640        |\n",
      "|    policy_gradient_loss | -0.00589    |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 0.000519    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15192513 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0291     |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | 0.00607    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16022559 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00919   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000534   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 809        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15985155 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | 0.00894    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15505925 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00507   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000525   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 792       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1491471 |\n",
      "|    clip_fraction        | 0.678     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0275   |\n",
      "|    n_updates            | 5740      |\n",
      "|    policy_gradient_loss | -0.00713  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000508  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15724137 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000493   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1546841 |\n",
      "|    clip_fraction        | 0.682     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.038    |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | -0.00638  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000479  |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15100607 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | 0.00807    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000494   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13727768 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0719     |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.000336  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000494   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13585357 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0725    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00293   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000461   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fd63b678400> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fd63b2cfd68>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.671      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 140        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15800568 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0297    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.00517   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.679       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 687         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052540995 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.0716     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04818082 |\n",
      "|    clip_fraction        | 0.445      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.91       |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00364    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 717        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03540184 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.93       |\n",
      "|    explained_variance   | 0.924      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0029     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.7         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 749         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039474703 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0444     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.0028      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 721        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03461511 |\n",
      "|    clip_fraction        | 0.461      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0736    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00233    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.712     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 710       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0384032 |\n",
      "|    clip_fraction        | 0.47      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6         |\n",
      "|    explained_variance   | 0.949     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0586   |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0456   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00203   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.713       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 692         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042181153 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.723      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 700        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04004362 |\n",
      "|    clip_fraction        | 0.484      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0714    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00179    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 712         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049447075 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.12        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00648     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031107241 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.16        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0419     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 714         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046225674 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.21        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 719         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053673506 |\n",
      "|    clip_fraction        | 0.486       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0741     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00146     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 735         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036493424 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.27        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0895     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057083078 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0255      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046996646 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.26        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0409     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056324076 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 698        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05161085 |\n",
      "|    clip_fraction        | 0.517      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.34       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0528    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046237253 |\n",
      "|    clip_fraction        | 0.516       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 697         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053354573 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0609     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.736       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 692         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049200125 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.43        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.737       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053538214 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.47        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0774     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.737      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05754757 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.51       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 699         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060478866 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.57        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0425     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 709         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051982533 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0598     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 739         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052179508 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00503     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.744      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 738        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06456201 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0679    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061597742 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.73        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0711     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044842355 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.79        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0281      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.754      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07242744 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054085374 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.88        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0394     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052268445 |\n",
      "|    clip_fraction        | 0.548       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0633     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.761      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 711        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06873323 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.761      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 712        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06926413 |\n",
      "|    clip_fraction        | 0.556      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.92       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0969    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.763       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073275015 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068933085 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7           |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058051504 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0559     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 701        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06999611 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 735         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075781025 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0757     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 717         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057983406 |\n",
      "|    clip_fraction        | 0.583       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.19        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06620696 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0788    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.786     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 706       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0702068 |\n",
      "|    clip_fraction        | 0.586     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.36      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0876   |\n",
      "|    n_updates            | 820       |\n",
      "|    policy_gradient_loss | -0.0397   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00134   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05944202 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.42       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08025626 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.48       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0758    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06441654 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.57       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00604    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08018204 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.61       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 674        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08121494 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.69       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0252    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07474008 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06673454 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.96       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0222    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0358    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07334892 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.07       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 689        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07458758 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0376     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.000942   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07457799 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0615     |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000978   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06751354 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.27       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 708        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05998656 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.41       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0731    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.000911   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08040826 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.806      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07572769 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.58       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 680        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06735216 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.71       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000928   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07389768 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.84       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000848   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.81      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 735       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0721579 |\n",
      "|    clip_fraction        | 0.589     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.96      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0457   |\n",
      "|    n_updates            | 1160      |\n",
      "|    policy_gradient_loss | -0.0276   |\n",
      "|    std                  | 0.159     |\n",
      "|    value_loss           | 0.000837  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06479712 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.07       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.122      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07912575 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.15       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070950285 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0749     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0258     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00094     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07366645 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.29       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07480247 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.39       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00082    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 755        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07723531 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.43       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07527916 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0193    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 705        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06936828 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07632315 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.6        |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0667     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000764   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 711        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07640413 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.69       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0344     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05611039 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0101     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07033519 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.77       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07102599 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.8        |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06902157 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.88       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.03      |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09122075 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.96       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000606   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.822     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 730       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0759146 |\n",
      "|    clip_fraction        | 0.598     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00482  |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.019    |\n",
      "|    std                  | 0.151     |\n",
      "|    value_loss           | 0.000664  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 715         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064518355 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0443      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000638    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07935315 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000594   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07234187 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0789    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 707         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064042054 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00059     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08750352 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 719        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06739133 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0816    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.829     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 759       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0730402 |\n",
      "|    clip_fraction        | 0.609     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.6      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0135    |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -0.0173   |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 0.000601  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087128915 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000584    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 741        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06924448 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00378   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000603   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 726        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07408793 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061184037 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.000534    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10523143 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00051    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07882668 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00719    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 734        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07672254 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00139    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000504   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 726         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096666336 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000503    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 701        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09564488 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00868    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00048    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09048167 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0556    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 754         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080318056 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000423    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10271826 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00041    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 723        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08446936 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.057     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000416   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 724       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0993552 |\n",
      "|    clip_fraction        | 0.612     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0136   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000437  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083745345 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000375    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08242915 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 700         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093663886 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00453    |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000341    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07560171 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0905    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.00947   |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000334   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100135505 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0588      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000331    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09223344 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000316   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08574788 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000343   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 714         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090997204 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0482      |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000386    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10584924 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0637    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0054    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00036    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 710         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076151535 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00326    |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000341    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.84      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 715       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0956681 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.6      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0923    |\n",
      "|    n_updates            | 2120      |\n",
      "|    policy_gradient_loss | -0.00842  |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.00035   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08197681 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0856    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00037    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 734        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07890798 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0476     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08731097 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09724641 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00288    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0085    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09065713 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.013      |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.00896   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000315   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100256324 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0712     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.00872    |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000356    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09144209 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.00399   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000331   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10969168 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.019      |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.00609   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000349   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09430722 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000321   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09886184 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0715     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.00647   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000315   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11540258 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0819    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.00806   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000352   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09904971 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0094     |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.00743   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000352   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12606725 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0326     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000345   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108269215 |\n",
      "|    clip_fraction        | 0.651       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0697     |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000329    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12170553 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000368   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10573181 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.00458   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000365   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11617444 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.00691   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00039    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10379742 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.00202   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000439   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08982402 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0336     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.00541   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000417   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09250638 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.00589   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000443   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08659986 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0073    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.00759   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00043    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107380725 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000418    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 705        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11782316 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.047      |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.00417   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000381   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 727        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10294397 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.00795   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107142285 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0243      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000374    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11973319 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.00612   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000381   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10862537 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0058    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000398   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11630674 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.00653   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000329   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110150434 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.00874    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000358    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09762807 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0024     |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.00843   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000338   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10804607 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0581    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.00226   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000329   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09634822 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00605   |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.00509   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 760       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1152918 |\n",
      "|    clip_fraction        | 0.649     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0122   |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | -0.00736  |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.00035   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 751         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115566805 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0712     |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000341    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 756         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113841854 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0721     |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000326    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13365205 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0045    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00031    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11990176 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0334     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.00663   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000327   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 765       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1353822 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0513   |\n",
      "|    n_updates            | 2880      |\n",
      "|    policy_gradient_loss | -0.00394  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000325  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10487361 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0287     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000316   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123876594 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00625    |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000348    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13586827 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0377     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.006     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000316   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 738        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10611057 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.00479   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000278   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11143388 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0122     |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.00404   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000312   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13800304 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00511    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.00611   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000317   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11257736 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0409     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.00482   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000335   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13550064 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0511     |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00293   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 741        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10109194 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.00656   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000327   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 747         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117748454 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000322    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 757       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1275939 |\n",
      "|    clip_fraction        | 0.65      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.00439  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000333  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 741        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11104478 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.00264   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000363   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13246678 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00253   |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.00244   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000317   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11712515 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.00537   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000358   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12712824 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.00294   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10993391 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0212    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.00365   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000366   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118338846 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0245      |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000353    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11840381 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.00297   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000337   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104090616 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0309     |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.00034     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 756        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14231235 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00432    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.00481   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000323   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11661907 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0389     |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.00564   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000327   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 742       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1231638 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0154   |\n",
      "|    n_updates            | 3320      |\n",
      "|    policy_gradient_loss | -0.00283  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000317  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116989255 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | -0.000342   |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000291    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11691201 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0506    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.0003     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116152525 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0491     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000316    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 720         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110947154 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000308    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 721        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15407732 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0444     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00438   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000304   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12539491 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0193    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.00389   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000339   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13438928 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.058     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.00382   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00035    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 728       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1203331 |\n",
      "|    clip_fraction        | 0.65      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0496   |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -0.00353  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000375  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12328527 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00239   |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00998   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000333   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10411058 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.00443   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000359   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 755         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109347545 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0208      |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000376    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11296477 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.00138   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00036    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1365132 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0185   |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | -0.00614  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000331  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11111133 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00396   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000333   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11053224 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0368    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.00439   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117551014 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.1        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.000471   |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.000351    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 719        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13769452 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00805   |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00332   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000358   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14756997 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0011    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 756       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1247129 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0571   |\n",
      "|    n_updates            | 3700      |\n",
      "|    policy_gradient_loss | -0.00498  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000352  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1414262 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00103   |\n",
      "|    n_updates            | 3720      |\n",
      "|    policy_gradient_loss | -0.00456  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000373  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15106702 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00151   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000339   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 756        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13072823 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0086    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15212196 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00313   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000309   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12041359 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00389    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000309   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13750659 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00204   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000301   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13142397 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00509   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000278   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14746204 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.00617   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000288   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12911272 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.00153   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000309   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 735        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13938305 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0049     |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00414   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00033    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15745297 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00347    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00254   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000321   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12778556 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0152     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00345   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000312   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15246713 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0266     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.00562   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00035    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12400107 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00435   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000328   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 711        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12827137 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0744    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0038    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000296   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16494988 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0656    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00584   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000309   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 735       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1401984 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0648    |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | -0.00716  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000369  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16019732 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00242   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13824928 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | 0.00122    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13741477 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.204      |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000332   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15383878 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0524     |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00378   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000316   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.846    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 742      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.153406 |\n",
      "|    clip_fraction        | 0.653    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.6     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00672  |\n",
      "|    n_updates            | 4140     |\n",
      "|    policy_gradient_loss | -0.00174 |\n",
      "|    std                  | 0.118    |\n",
      "|    value_loss           | 0.000363 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10536897 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | 0.00111    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 754       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1279647 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0637    |\n",
      "|    n_updates            | 4180      |\n",
      "|    policy_gradient_loss | -0.00618  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000355  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14013684 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.00311   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000339   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 707        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13704632 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00416   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13612159 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00282   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15573137 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00747   |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | 0.0016     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000363   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15000379 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0634     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -4.06e-05  |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 741      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.130404 |\n",
      "|    clip_fraction        | 0.663    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.5     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.014    |\n",
      "|    n_updates            | 4300     |\n",
      "|    policy_gradient_loss | -0.00281 |\n",
      "|    std                  | 0.119    |\n",
      "|    value_loss           | 0.000424 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12668817 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00412   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 740       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1300519 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0495    |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | -0.00297  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000407  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 714        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14414857 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0198    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00304   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000354   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 731        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15836787 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0589     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00214   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000364   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13911736 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000361   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13303432 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0604    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00244   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000389   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11293821 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | 0.000417   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000395   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14761506 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0768    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00442   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000408   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 719        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16482292 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0216     |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | 0.0102     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000415   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13173808 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0433     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00206   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000389   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15224877 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0362    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | 0.00371    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000395   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16312535 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0244     |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000378   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 750       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1370054 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0478   |\n",
      "|    n_updates            | 4560      |\n",
      "|    policy_gradient_loss | -0.00415  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000376  |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 753      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.160694 |\n",
      "|    clip_fraction        | 0.659    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0211  |\n",
      "|    n_updates            | 4580     |\n",
      "|    policy_gradient_loss | 0.0104   |\n",
      "|    std                  | 0.118    |\n",
      "|    value_loss           | 0.00037  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12892497 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000765  |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | 0.000164   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000389   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13417467 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00576    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.000881  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000406   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15309963 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0515     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00156   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000394   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15783302 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0726     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | 0.0117     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000396   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13572769 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0221     |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00432   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 751       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1610116 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0177   |\n",
      "|    n_updates            | 4700      |\n",
      "|    policy_gradient_loss | 0.000875  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000436  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15568367 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00746    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | 0.00808    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00047    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 710        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15220074 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | 0.00288    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000421   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13083968 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0935     |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | 0.00103    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15228714 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0416     |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00157   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 704        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12996891 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0372     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00117   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00035    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 735        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12575188 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00676   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000336   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 742      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.151191 |\n",
      "|    clip_fraction        | 0.661    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.8     |\n",
      "|    explained_variance   | 0.995    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0379  |\n",
      "|    n_updates            | 4840     |\n",
      "|    policy_gradient_loss | -0.00585 |\n",
      "|    std                  | 0.117    |\n",
      "|    value_loss           | 0.000316 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 725       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1357991 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0323    |\n",
      "|    n_updates            | 4860      |\n",
      "|    policy_gradient_loss | -0.000568 |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000349  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 746       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1546838 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.9      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0418    |\n",
      "|    n_updates            | 4880      |\n",
      "|    policy_gradient_loss | 0.00886   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000376  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14430603 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00885   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000327   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16077904 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0466     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | 0.00335    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000353   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13319312 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00533   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000325   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16263661 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0678     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.000172  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000309   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 807        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15354268 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00799    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000334   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 712        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15299888 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0652    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | 0.00244    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15014651 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00934    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00512   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000318   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 752        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15006764 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.037      |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | 0.000862   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000306   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12453134 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0786     |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000337   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120780155 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.9        |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 5080        |\n",
      "|    policy_gradient_loss | -0.00323    |\n",
      "|    std                  | 0.116       |\n",
      "|    value_loss           | 0.000324    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15702097 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0471    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.00593    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000319   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13571914 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0447     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000313   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15868632 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | 0.000124   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000371   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 756        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13949117 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00685   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00034    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12612149 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0669    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.0071    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000364   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15716304 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | 0.00368    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15240003 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00495   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000326   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 709        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15518346 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | 0.00151    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000342   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15312311 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.000104  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 743       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1502014 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0499   |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | 0.00464   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000348  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15063384 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00435   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 755        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16419551 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00687   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16218512 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | 0.00312    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000414   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 718       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1539431 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00964  |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | 0.00845   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000413  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 763       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1562534 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.103     |\n",
      "|    n_updates            | 5380      |\n",
      "|    policy_gradient_loss | 0.0108    |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000431  |\n",
      "---------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15406315 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00355    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000411   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16933171 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0434    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.00159    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000418   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15732093 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | 0.00618    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000416   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 693        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16020855 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0555     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00224   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15443169 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00371    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.000983  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000447   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 752        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15346973 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00161   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000401   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 723        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15156771 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00135   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000384   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15495996 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | 0.00773    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15701401 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00562    |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00265    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000414   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 735        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15075693 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0922     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | 0.00315    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000381   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 707        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15998156 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0932     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | 0.00521    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15209724 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0362    |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00405   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000351   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15398967 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.00375    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000415   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 724        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15816131 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | 0.00745    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15757069 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.00616    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16272938 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00561   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000379   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1445756 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0661   |\n",
      "|    n_updates            | 5720      |\n",
      "|    policy_gradient_loss | -0.0049   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000382  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15836462 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0577    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | 0.00521    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15328582 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0417     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15800834 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0298     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | 0.0101     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15790904 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | 0.00832    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000365   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15282576 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.085      |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.00799    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000371   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 715        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15075475 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0607     |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | 0.00231    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00037    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fd63b7cd7b8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fd608093828>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.68     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 163      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 15       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.159102 |\n",
      "|    clip_fraction        | 0.657    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.9     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0362   |\n",
      "|    n_updates            | 5860     |\n",
      "|    policy_gradient_loss | 0.00156  |\n",
      "|    std                  | 0.117    |\n",
      "|    value_loss           | 0.000366 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.678       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070500866 |\n",
      "|    clip_fraction        | 0.438       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.89        |\n",
      "|    explained_variance   | -0.376      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0617     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0125      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.691       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 749         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048894115 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0965     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00368     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.707       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036515933 |\n",
      "|    clip_fraction        | 0.465       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0523     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043721087 |\n",
      "|    clip_fraction        | 0.477       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00248     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.722      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03861792 |\n",
      "|    clip_fraction        | 0.474      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.95       |\n",
      "|    explained_variance   | 0.948      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00213    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.715     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 762       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0424608 |\n",
      "|    clip_fraction        | 0.475     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.9       |\n",
      "|    explained_variance   | 0.948     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0292   |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.0446   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.00209   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.725      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04126692 |\n",
      "|    clip_fraction        | 0.5        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00184    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033490755 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.88        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 786         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053956717 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04874031 |\n",
      "|    clip_fraction        | 0.501      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00453    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055190515 |\n",
      "|    clip_fraction        | 0.5         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0392     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00146     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042960916 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040994413 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.05        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.731       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052787006 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0361     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05562014 |\n",
      "|    clip_fraction        | 0.522      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.16       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 725        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04330797 |\n",
      "|    clip_fraction        | 0.526      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05426056 |\n",
      "|    clip_fraction        | 0.524      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.22       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058766235 |\n",
      "|    clip_fraction        | 0.529       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059461903 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0015      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05777074 |\n",
      "|    clip_fraction        | 0.54       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.32       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0785    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04978593 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.33       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062838696 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0692     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.759      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06996098 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.44       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 770         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062502824 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0129      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061955065 |\n",
      "|    clip_fraction        | 0.543       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.05       |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05695828 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06161911 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0968    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06353586 |\n",
      "|    clip_fraction        | 0.567      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.67       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.099     |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.767       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061310746 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06703136 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 743         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058580585 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 738         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061955355 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.97        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0639     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07403137 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.01       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06143996 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.09       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058659125 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0905     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05473997 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.16       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061780415 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.21        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.0011      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.778      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05757151 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.3        |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0709    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.779       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060521137 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.36        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078381255 |\n",
      "|    clip_fraction        | 0.584       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.41        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0373     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0344     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06408231 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.49       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.065     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06617008 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.58       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.000982   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.788       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077911295 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.64        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0744     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 791         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076595135 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.71        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0614     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07921113 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.8        |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07660602 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.87       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.000992   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.793       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 756         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072735295 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.97        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0348     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.793       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085904405 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.06        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.046      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07603276 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 745         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055758476 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.16        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07241278 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.24       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 757         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075302675 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.28        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.000976    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.801       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063952446 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.33        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00757    |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0288     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.803     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0598032 |\n",
      "|    clip_fraction        | 0.597     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.44      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00354   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0293   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.000958  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08018909 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0058     |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07088951 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.6        |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0147    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066244096 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.63        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000968    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 745         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082179494 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.000942    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07475838 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.77       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0332     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08122401 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.87       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.81        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079303786 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.94        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0273     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000872    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 767         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061041016 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.01        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0291     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.000842    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09417999 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000855   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08384001 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.23       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0705    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 742        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07549423 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.35       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07585373 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00769    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074367106 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.58        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.000746    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07709063 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.66       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000749   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09265313 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.75       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000772   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.817     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 767       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0982478 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.82      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00386  |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    std                  | 0.153     |\n",
      "|    value_loss           | 0.000782  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083795145 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.89        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0225     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000773    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.819     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 802       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0770528 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.98      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0441   |\n",
      "|    n_updates            | 1440      |\n",
      "|    policy_gradient_loss | -0.0225   |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.000738  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 726        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08041523 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081069686 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0759     |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000731    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 809        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08976076 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 811        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10623851 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0614    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07858156 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000511  |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000626   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.823    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 758      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.078501 |\n",
      "|    clip_fraction        | 0.6      |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 10.4     |\n",
      "|    explained_variance   | 0.988    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0167  |\n",
      "|    n_updates            | 1560     |\n",
      "|    policy_gradient_loss | -0.0186  |\n",
      "|    std                  | 0.15     |\n",
      "|    value_loss           | 0.000689 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10261834 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0629    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07960223 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 741         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060925603 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00685    |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000627    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07956751 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0832    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08102087 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 779         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088500746 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08317304 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09629831 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 763         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075692445 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000602    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08220573 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 801        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11114378 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00447   |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000655   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088877186 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0345      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000675    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.827     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 749       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0916187 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0391   |\n",
      "|    n_updates            | 1820      |\n",
      "|    policy_gradient_loss | -0.0202   |\n",
      "|    std                  | 0.144     |\n",
      "|    value_loss           | 0.000681  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08424554 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000621   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10728705 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00847   |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000686   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07409489 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09120367 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0838     |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000656   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09352454 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096314244 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000602    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09177254 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000579   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09576638 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10021861 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0116    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000527   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09076708 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000583   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08977346 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.053      |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09457257 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0577    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09478041 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0215    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000628   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 802         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096846715 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00913    |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000637    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09356068 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0376     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000567   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09169562 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0248     |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00055    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 780         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099278644 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00467    |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000567    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10647581 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0952554 |\n",
      "|    clip_fraction        | 0.617     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.056    |\n",
      "|    n_updates            | 2200      |\n",
      "|    policy_gradient_loss | -0.0139   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000584  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 766       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0935892 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0526   |\n",
      "|    n_updates            | 2220      |\n",
      "|    policy_gradient_loss | -0.0168   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000619  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09470548 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0602    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.831     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1047375 |\n",
      "|    clip_fraction        | 0.615     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.2      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0471   |\n",
      "|    n_updates            | 2260      |\n",
      "|    policy_gradient_loss | -0.0108   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000574  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11501799 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000565   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09996338 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000571   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103070736 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0385     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000533    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090005554 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0788     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000535    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09344648 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09379065 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000597   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09234057 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090205066 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000582    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09582795 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000628   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11477339 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000634   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086061835 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000582    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 750         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095152594 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0446     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000576    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11053125 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10594268 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08790326 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0342     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00051    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09015091 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00052    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09463985 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000487   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122784235 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.832    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 790      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.121834 |\n",
      "|    clip_fraction        | 0.635    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.1     |\n",
      "|    explained_variance   | 0.991    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0273  |\n",
      "|    n_updates            | 2640     |\n",
      "|    policy_gradient_loss | -0.0135  |\n",
      "|    std                  | 0.132    |\n",
      "|    value_loss           | 0.000534 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09518911 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0597     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000534   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 766         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105664656 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00239     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00053     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10934372 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0412     |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000473   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11521058 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.037      |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00054    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 798        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13041958 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0415    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000506   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09139538 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0393     |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000468   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09727392 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00854   |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000452   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108527705 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00448     |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000452    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09866372 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0414    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.00755   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00044    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09864099 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0747    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000475   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10864236 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000458   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08675532 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000463   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10102578 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0792    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000501   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114506386 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000484    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 800         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103619315 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0548     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000515    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 831        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09979792 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09705772 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0758    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000501   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10194179 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0373    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000497   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13420305 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00052    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 823        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12151557 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00952   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000481   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 744        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10788579 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0244     |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.00975   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000517   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10665915 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0206     |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.00883   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000461   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11238755 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00949    |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.00913   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000464   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1207297 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.9      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0555   |\n",
      "|    n_updates            | 3120      |\n",
      "|    policy_gradient_loss | -0.0164   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000499  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12325021 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0494     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.00851   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000451   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099186435 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.036      |\n",
      "|    n_updates            | 3160        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000438    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1164428 |\n",
      "|    clip_fraction        | 0.643     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0206   |\n",
      "|    n_updates            | 3180      |\n",
      "|    policy_gradient_loss | -0.0134   |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.000462  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11407809 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000484   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13350502 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0711    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000431   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 773         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106866226 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0106     |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.00964    |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.00042     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 809        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10064503 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0284     |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.00902   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000458   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12310382 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.013      |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000451   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 756         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116378926 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000461    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12017087 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00479   |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.00898   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000386   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12619302 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000394   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10953295 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.00926   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000411   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12859043 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.00728   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00041    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1252109 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0062   |\n",
      "|    n_updates            | 3400      |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00042   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10444699 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.0094    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 795         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121167086 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0272      |\n",
      "|    n_updates            | 3440        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000399    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11247437 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0767    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.00989   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00039    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124806836 |\n",
      "|    clip_fraction        | 0.666       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000425    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11619065 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00047    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 821         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116553366 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0583     |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000467    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 784         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113549516 |\n",
      "|    clip_fraction        | 0.651       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000486    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11289439 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0126    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000474   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12768789 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0479    |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.00877   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000468   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12768553 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.024      |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000471   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15147197 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.00708   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000468   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116393566 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.07       |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000482    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 804         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119013265 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15          |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000502    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115875795 |\n",
      "|    clip_fraction        | 0.662       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 3680        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.000466    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14943373 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0425     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00847   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000461   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13855878 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000462   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 807        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11163406 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0951    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00737   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000468   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13351044 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00023    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000474   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15559682 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00877   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000441   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12004538 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0211     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000451   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 804        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12228328 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000453   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12459042 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00951   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000496   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 737        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12563719 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.00988   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 788      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.143835 |\n",
      "|    clip_fraction        | 0.662    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.3     |\n",
      "|    explained_variance   | 0.992    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.00275 |\n",
      "|    n_updates            | 3880     |\n",
      "|    policy_gradient_loss | -0.00885 |\n",
      "|    std                  | 0.119    |\n",
      "|    value_loss           | 0.000463 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14592867 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00749   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000495   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13490489 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00969   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000448   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13858004 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000448   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12901126 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000483   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 787      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.138108 |\n",
      "|    clip_fraction        | 0.663    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.3     |\n",
      "|    explained_variance   | 0.992    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0762   |\n",
      "|    n_updates            | 3980     |\n",
      "|    policy_gradient_loss | -0.013   |\n",
      "|    std                  | 0.119    |\n",
      "|    value_loss           | 0.000488 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12979588 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000505   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119840816 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 4020        |\n",
      "|    policy_gradient_loss | -0.00964    |\n",
      "|    std                  | 0.119       |\n",
      "|    value_loss           | 0.00053     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 737       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1400603 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0115    |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | -0.00902  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000509  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 809        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14799407 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0251     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00985   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 796        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15832908 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00717   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 818        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13831703 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0518    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11918423 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00162    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 807        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13884635 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000503   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12027087 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0436    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.00856   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000507   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 787         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101530015 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 4180        |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    std                  | 0.119       |\n",
      "|    value_loss           | 0.000503    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 756        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13699295 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0156    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000465   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12969771 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00939   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000479   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109543145 |\n",
      "|    clip_fraction        | 0.673       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.119       |\n",
      "|    value_loss           | 0.000468    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 817        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13723402 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00909   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000528   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11618006 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00792   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12636557 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000518   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15214412 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0316    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00353   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000536   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13338323 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0657    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12831183 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0354     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00878   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 792        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11316419 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00843   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000543   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15063398 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0682     |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00053    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 763        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13620616 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000452   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13450705 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000504   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13114196 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00604   |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000481   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 784         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124437414 |\n",
      "|    clip_fraction        | 0.671       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.8        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0707      |\n",
      "|    n_updates            | 4480        |\n",
      "|    policy_gradient_loss | -0.00916    |\n",
      "|    std                  | 0.117       |\n",
      "|    value_loss           | 0.000457    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13182506 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.073      |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000449   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13950223 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0068     |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00634   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000485   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14058924 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000463   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 736        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15242048 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00861    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0082    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13818195 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000469   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15296654 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00401   |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00915   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000493   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 790         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124648616 |\n",
      "|    clip_fraction        | 0.671       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    std                  | 0.117       |\n",
      "|    value_loss           | 0.000444    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14644878 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00669   |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000472   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12317371 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00919   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000455   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14624996 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00566    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000464   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15758708 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000459   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13091661 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000431   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 805        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15444937 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0303     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00239   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000434   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 763       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1286085 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00083   |\n",
      "|    n_updates            | 4760      |\n",
      "|    policy_gradient_loss | -0.00497  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000453  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12301411 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0074    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000415   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14400098 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000464   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 812        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14032324 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0359     |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000445   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12644085 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00794   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000504   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15791789 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | 0.00565    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 756        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15478335 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0204     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00101   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000503   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15174267 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00884   |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | 0.00486    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14966278 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0865    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.00859   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13879049 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13358827 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0367    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00816   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16447446 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0706     |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00587   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000514   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15026654 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00215   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12511976 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00932   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15039365 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0108     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1673728 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00622   |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -0.000334 |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000583  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15078768 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00333    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00658   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15224722 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.00401    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14307526 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0813    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14008209 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0419     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00996   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15424708 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0702    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | 0.00763    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000632   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13703622 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00863   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13862856 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13696328 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00249   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000555   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 749        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13077074 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0069    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 753        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15195018 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.000351  |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12789783 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0323     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00784   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000632   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14480183 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000541   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15389779 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00843    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000561   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15323156 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0413    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | 0.00384    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00057    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13407478 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00297    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000543   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14098378 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0404     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00523   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16579647 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0508    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.000894   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000562   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15292436 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 808        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13803552 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0241     |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 825        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15179947 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.054      |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | 0.00616    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000597   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 745        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12531371 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15002613 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | 0.00503    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14274624 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00133   |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00902   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15358883 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | 0.00543    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000618   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15068033 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.00438   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13463803 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00936   |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000689   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15096274 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0122     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.00567   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000641   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 758        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12671024 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0685     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000648   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 757        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15811637 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000144   |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00336   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15197186 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00157   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15629299 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0496     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.000637  |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000669   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13285998 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0138    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000612   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 806        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15148935 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.149      |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.000149   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15090251 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.032      |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00327   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15174106 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0713    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00345   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000748   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 756       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1551228 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.5      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0411    |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | -0.00131  |\n",
      "|    std                  | 0.113     |\n",
      "|    value_loss           | 0.000695  |\n",
      "---------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522079 |\n",
      "|    clip_fraction        | 0.649     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.5      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00565   |\n",
      "|    n_updates            | 5800      |\n",
      "|    policy_gradient_loss | 0.00977   |\n",
      "|    std                  | 0.113     |\n",
      "|    value_loss           | 0.000736  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 827        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15023632 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.246      |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.00633    |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15227155 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.058      |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00203   |\n",
      "|    std                  | 0.113      |\n",
      "|    value_loss           | 0.000783   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.3, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25],\n",
    "                                episode_limit_array=[150000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
