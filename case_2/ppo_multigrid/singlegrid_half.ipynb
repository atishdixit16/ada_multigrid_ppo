{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_singlegrid_half'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f2acafe2da0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2acaf7e908>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    fps             | 128      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06906624 |\n",
      "|    clip_fraction        | 0.446      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.85       |\n",
      "|    explained_variance   | -0.667     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0464     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0163     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.716       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 532         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036150116 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.83        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00572     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00425     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.718      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03927784 |\n",
      "|    clip_fraction        | 0.462      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.86       |\n",
      "|    explained_variance   | 0.911      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00354    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.723      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03716402 |\n",
      "|    clip_fraction        | 0.48       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.9        |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00308    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.726     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 542       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0383537 |\n",
      "|    clip_fraction        | 0.469     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.9       |\n",
      "|    explained_variance   | 0.931     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00858  |\n",
      "|    n_updates            | 100       |\n",
      "|    policy_gradient_loss | -0.0456   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.00279   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.737       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051582832 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0777     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04082008 |\n",
      "|    clip_fraction        | 0.475      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00485   |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00256    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.737       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 548         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049717046 |\n",
      "|    clip_fraction        | 0.493       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0616     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 535         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055880927 |\n",
      "|    clip_fraction        | 0.492       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0603     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.746      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 522        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04992205 |\n",
      "|    clip_fraction        | 0.497      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0627    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.0023     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045199838 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052505612 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0806     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00224     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.752      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 553        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06429887 |\n",
      "|    clip_fraction        | 0.497      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.25       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0451    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00214    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049838036 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053744067 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0605     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049401052 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0557     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00205     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.753       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 563         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059801735 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05376326 |\n",
      "|    clip_fraction        | 0.524      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.31       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0699    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00205    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04487168 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.34       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00208    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05781079 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0088    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00206    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.749      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06069213 |\n",
      "|    clip_fraction        | 0.526      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.37       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0743    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00206    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05857219 |\n",
      "|    clip_fraction        | 0.543      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0502    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00195    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.763      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05459968 |\n",
      "|    clip_fraction        | 0.522      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.43       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0125    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00214    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.768       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 550         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061616696 |\n",
      "|    clip_fraction        | 0.54        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0501     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.769      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 551        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04893638 |\n",
      "|    clip_fraction        | 0.531      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.57       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0433    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00193    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06294612 |\n",
      "|    clip_fraction        | 0.539      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00188    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062048696 |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.62        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061511137 |\n",
      "|    clip_fraction        | 0.54        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.67        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07006095 |\n",
      "|    clip_fraction        | 0.54       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0019     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 533         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054401554 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.67        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.784      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 545        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07033175 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.72       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0419    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00188    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07068537 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.8        |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0972    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00192    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07687889 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.87       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.002      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 547         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060934354 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0522     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071323544 |\n",
      "|    clip_fraction        | 0.561       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.98        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00194     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.787       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067683056 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.05        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.094      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06986119 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.14       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00189    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 546        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07492496 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.2        |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0754    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.0019     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06534566 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.24       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.0018     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 526        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06674926 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.32       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0686    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00179    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061329592 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.4         |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0742     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0376     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05903659 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.46       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0537    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00189    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 521        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08040294 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00168    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.798     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 534       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0778171 |\n",
      "|    clip_fraction        | 0.566     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.63      |\n",
      "|    explained_variance   | 0.963     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0288   |\n",
      "|    n_updates            | 880       |\n",
      "|    policy_gradient_loss | -0.0379   |\n",
      "|    std                  | 0.169     |\n",
      "|    value_loss           | 0.00184   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08568996 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.68       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0608    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0018     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06021639 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.75       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0375    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08318427 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.81       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0729    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07689277 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.803       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 539         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072667316 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.91        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.806      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06114968 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.04       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0747    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0354    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066651195 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.13        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0492     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00176     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 525        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07122505 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.21       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08217889 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.29       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00154    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.809    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 537      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.057171 |\n",
      "|    clip_fraction        | 0.591    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 8.36     |\n",
      "|    explained_variance   | 0.97     |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0587  |\n",
      "|    n_updates            | 1080     |\n",
      "|    policy_gradient_loss | -0.0331  |\n",
      "|    std                  | 0.163    |\n",
      "|    value_loss           | 0.00157  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06703433 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.44       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.033     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00151    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.812     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 539       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0751102 |\n",
      "|    clip_fraction        | 0.592     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.55      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0465   |\n",
      "|    n_updates            | 1120      |\n",
      "|    policy_gradient_loss | -0.0342   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.00142   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07415295 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.66       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07859747 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.71       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 548        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07231718 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.75       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.035     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08555098 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.79       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00151    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.814       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 538         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074260436 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.86        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00578    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0327     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00162     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 524        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08507002 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.93       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0657    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07354417 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.01       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07982789 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.05       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.817     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 541       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0833801 |\n",
      "|    clip_fraction        | 0.607     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.12      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0491   |\n",
      "|    n_updates            | 1300      |\n",
      "|    policy_gradient_loss | -0.0297   |\n",
      "|    std                  | 0.158     |\n",
      "|    value_loss           | 0.00141   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08363809 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.17       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0256     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07595585 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08006665 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07623573 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.32       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0336    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09063389 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0813    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 554         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090688124 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.49        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00536     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545653 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00152    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545439 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0588    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078455165 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 1480        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00152     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 495        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09051838 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.67       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00951    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 549        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08259456 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0633    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 559         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060712326 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0286     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.00155     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07892642 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.84       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.827     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 527       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0940554 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.95      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0181   |\n",
      "|    n_updates            | 1580      |\n",
      "|    policy_gradient_loss | -0.03     |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.00144   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09020698 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00319    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093783416 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0864     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091673926 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10555686 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0749     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09183925 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09275903 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0934    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545037 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07669388 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074050345 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 526        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08890591 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00329    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 530         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098492585 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0643     |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10622437 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 545        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11677681 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08945942 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.836     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 536       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0920698 |\n",
      "|    clip_fraction        | 0.619     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.9      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0111   |\n",
      "|    n_updates            | 1880      |\n",
      "|    policy_gradient_loss | -0.0254   |\n",
      "|    std                  | 0.146     |\n",
      "|    value_loss           | 0.00136   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 520       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1031193 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.031    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.00126   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09827696 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09976198 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00726   |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08904725 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10042737 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08759135 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.068     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09645357 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0168    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 548         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103304945 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0525     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10606042 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 531       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1013968 |\n",
      "|    clip_fraction        | 0.626     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.5      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0176    |\n",
      "|    n_updates            | 2080      |\n",
      "|    policy_gradient_loss | -0.0243   |\n",
      "|    std                  | 0.142     |\n",
      "|    value_loss           | 0.00143   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10593511 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00722   |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 544        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09388335 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0328    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09226479 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 550        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11161556 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00254   |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11616416 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0262    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.837     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 533       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0923036 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00979  |\n",
      "|    n_updates            | 2200      |\n",
      "|    policy_gradient_loss | -0.0204   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.00136   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104607224 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108194366 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 544        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08592955 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0222     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124801114 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00759    |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10387015 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0041    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 551         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118170224 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00649    |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 526         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113943554 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 552        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10744862 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0531    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 546         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121817686 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0725     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00117     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10990176 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09812885 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 527        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10598622 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0285    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 548         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116579995 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09161998 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11724448 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09050151 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00457    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11779865 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124856696 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000978    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 525        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11334898 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 544        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11623404 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0669    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00093    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11948198 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000976   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11372428 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0658    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000968   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 524        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09334272 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00133    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 555        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12508282 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000919   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10085436 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000834   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110072866 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000897    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10943552 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00744   |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000896   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11568298 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 539       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1298249 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0234   |\n",
      "|    n_updates            | 2780      |\n",
      "|    policy_gradient_loss | -0.0155   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000882  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 546        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11677186 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00084    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14208364 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000895   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 528         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088609494 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0601     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000823    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111688875 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0384     |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000899    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10609231 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000884   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112019084 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000872    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 546        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10499823 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0442    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13455133 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0434     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11620958 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00565   |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00087    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10023515 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0436    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000843   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 549        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11975266 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0385     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 530       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1318419 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.9      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00116  |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | -0.0166   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000781  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 526        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09912665 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00078    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13027593 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 552        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11521077 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00535   |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12832694 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000788   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09865411 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000755   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11901287 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000773   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13603038 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0176    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000841   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11143253 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 545        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11759974 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000801   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14168672 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0109     |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 548        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12388822 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00555   |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000788   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12091684 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101607636 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000762    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15173498 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.00787   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000749   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12894972 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0565    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12701091 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000716   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12482226 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0738    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 534         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122979105 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000743    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12743655 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12580389 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0951     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00883   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000786   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 532      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.156329 |\n",
      "|    clip_fraction        | 0.676    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.6     |\n",
      "|    explained_variance   | 0.988    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0782  |\n",
      "|    n_updates            | 3440     |\n",
      "|    policy_gradient_loss | -0.013   |\n",
      "|    std                  | 0.122    |\n",
      "|    value_loss           | 0.00074  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 557        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13537462 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 547       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1254997 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0345    |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000667  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11425437 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0654     |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13338758 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0643    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14856748 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00521   |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15216058 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.00953   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15831837 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.021     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.00366   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13464007 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00402   |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000644   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10813099 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.042     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000658   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12693655 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0435    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000637   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13735738 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0274    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13104834 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0331     |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000649   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12996247 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000362   |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.846    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 529      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.152271 |\n",
      "|    clip_fraction        | 0.667    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.9     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.09    |\n",
      "|    n_updates            | 3720     |\n",
      "|    policy_gradient_loss | -0.00603 |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000638 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12752607 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 527       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1563626 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.043     |\n",
      "|    n_updates            | 3760      |\n",
      "|    policy_gradient_loss | -0.000674 |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000636  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 525      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.151714 |\n",
      "|    clip_fraction        | 0.674    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15       |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0199  |\n",
      "|    n_updates            | 3780     |\n",
      "|    policy_gradient_loss | -0.0131  |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000683 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 517        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14586005 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15270066 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00801    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00742   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16935296 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0267     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000668   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14963189 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15586169 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000671   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14758226 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0219     |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000705   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14556432 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.056     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15248156 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0661     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | 0.000578   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000821   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14662662 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000811   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15127793 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000739   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15574846 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0396     |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00189   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 526        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16296008 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00263    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00106   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15142442 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00108   |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00472   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000843   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15537624 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00265   |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00521   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000808   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15740876 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.126      |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00125   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15547553 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.000778  |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000793   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 519        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14709878 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0357     |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15514599 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00406   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14864846 |\n",
      "|    clip_fraction        | 0.69       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0174     |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000857   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 529       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1519984 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00195   |\n",
      "|    n_updates            | 4180      |\n",
      "|    policy_gradient_loss | -0.00485  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000819  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15951987 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | 0.00108    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16228351 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | 0.002      |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 527       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1637162 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0412   |\n",
      "|    n_updates            | 4240      |\n",
      "|    policy_gradient_loss | 0.00152   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000763  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 545        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15529737 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0391     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00904   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15087661 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0319    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000797   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 553        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15710399 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00254   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14317232 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00938   |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16742767 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0179     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00188   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 525        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16711971 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0698     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.000728  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11919067 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0193     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000791   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15550908 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.054      |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | 0.00422    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000768   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 527        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15506193 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0208     |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15112022 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16000637 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15769668 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00837   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15255275 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.101     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15709673 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00444   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15795864 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00362   |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 547        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15233469 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0313     |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | 0.00159    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 544       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1717945 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0199   |\n",
      "|    n_updates            | 4580      |\n",
      "|    policy_gradient_loss | 0.000609  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000673  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 522        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14252004 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0556    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000768   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14167419 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000716   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15139462 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0378     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 546        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15578508 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0402    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00598   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000764   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14101784 |\n",
      "|    clip_fraction        | 0.696      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000751   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 549        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14377217 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.1        |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 527        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15112361 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | 0.00381    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000794   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 532        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14005013 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000728   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 524        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15446718 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | 0.003      |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15354344 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0433     |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00717   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000707   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 524       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1523511 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00501   |\n",
      "|    n_updates            | 4800      |\n",
      "|    policy_gradient_loss | 0.00948   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000763  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15075189 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | 0.00134    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000749   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15441309 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000726   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15159048 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0754    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16088815 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14857718 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 529        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15001655 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000795   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 523        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15001127 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00484   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 526        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13019586 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0133     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00957   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 524        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15215452 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.000878   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00077    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 524        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12624888 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0187     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000764   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 539       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1554043 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0213    |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | 0.00396   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00077   |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 521        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15105867 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | 0.000766   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 518        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15749899 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0024    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 520        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15205202 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | 0.00388    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00079    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 533        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15072173 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0632    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.00545    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 519        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15262802 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00166   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13760693 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15525672 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00169   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15111504 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00311    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00195   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000683   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15182523 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0712     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.00186   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 522        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15607056 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | 0.00154    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00073    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15658292 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.143      |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | 0.00532    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000702   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 529       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1598367 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.11      |\n",
      "|    n_updates            | 5260      |\n",
      "|    policy_gradient_loss | 0.00459   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000773  |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 536       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1534626 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0542   |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | 0.00679   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000693  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 531       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1540058 |\n",
      "|    clip_fraction        | 0.682     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0692    |\n",
      "|    n_updates            | 5300      |\n",
      "|    policy_gradient_loss | -0.00439  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.00069   |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16159587 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0908     |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | 0.002      |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 542       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1519622 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0871    |\n",
      "|    n_updates            | 5340      |\n",
      "|    policy_gradient_loss | 0.0113    |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000763  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15315184 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0216    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.00639   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 523        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15054126 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0463     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.000205  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 530        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15175958 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0429     |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00901    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15255715 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0709     |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.0131     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 542        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15479922 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00402   |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00366   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15701202 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0195     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00889   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15136221 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0175    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00107   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15589014 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 539        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15492538 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.0135     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000739   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 535        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16405521 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.00182   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 535       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1620189 |\n",
      "|    clip_fraction        | 0.685     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | -0.00522  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000704  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16189626 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0244    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00554   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000737   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15098314 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00874   |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | 0.0026     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15095901 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0739     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000737   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15050547 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0692    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00378   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 530       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1549335 |\n",
      "|    clip_fraction        | 0.681     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0329   |\n",
      "|    n_updates            | 5660      |\n",
      "|    policy_gradient_loss | -0.00275  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000689  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 534        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15222523 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0037     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.00325    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000682   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 521        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15007281 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0153     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.0057    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 537        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14077716 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0622    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15030989 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.128      |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00711   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16758797 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0306    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | 0.0029     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 528        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15300806 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0609     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00518   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15043204 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0336     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | 0.011      |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000767   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 531        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14790459 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000706   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 536        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15867665 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | 0.000117   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f2acaf237b8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2acaf8f9b0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 141        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13773409 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.116      |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000773   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 48 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 566        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05687784 |\n",
      "|    clip_fraction        | 0.413      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.91       |\n",
      "|    explained_variance   | -0.118     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0125     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 618         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049618453 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00422     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.686       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039182033 |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.908       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00359     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038288515 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.914       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00334     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.689      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 609        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04500542 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.95       |\n",
      "|    explained_variance   | 0.925      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0597    |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00296    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053980164 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0741     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00268     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 634         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038721595 |\n",
      "|    clip_fraction        | 0.483       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0595     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05065584 |\n",
      "|    clip_fraction        | 0.49       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00255    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04639576 |\n",
      "|    clip_fraction        | 0.484      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.04       |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00244    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.723       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037011918 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.05        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0666     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00225     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055627346 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.07        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00202     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044918206 |\n",
      "|    clip_fraction        | 0.492       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.08        |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0926     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00208     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.737       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 592         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051434834 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 606        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05703937 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.13       |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0045    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00207    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052733265 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.743      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04759901 |\n",
      "|    clip_fraction        | 0.513      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0699    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00197    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 604         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058725584 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0977     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00203     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.744      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 602        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05883485 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.27       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0872    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00199    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.743       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 613         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058474638 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.33        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0751     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 615         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059770774 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00203     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.743       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 607         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052185126 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.44        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0442     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049786765 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.5         |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0589     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 609         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058660187 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.55        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0675     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00206     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057744734 |\n",
      "|    clip_fraction        | 0.541       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.62        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0825     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.746      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 601        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05511003 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0505    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00196    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 613         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055983163 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0431     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.746      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06373842 |\n",
      "|    clip_fraction        | 0.535      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.71       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00196    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 602         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056881655 |\n",
      "|    clip_fraction        | 0.541       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.81        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0655     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00194     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056521334 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.84        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0474     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070039496 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.83        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0663     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.753      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05515986 |\n",
      "|    clip_fraction        | 0.541      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0367    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00186    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.753      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06941149 |\n",
      "|    clip_fraction        | 0.555      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.9        |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0768    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.759      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05632298 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.91       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0693    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00177    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.759      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06523774 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00177    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070184484 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.99        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0338     |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.758      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06601584 |\n",
      "|    clip_fraction        | 0.582      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.08       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0955    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00179    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.757      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 621        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07747306 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.12       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0309    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06393703 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.16       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0611    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.763      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07048185 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.18       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.764     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 616       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0723149 |\n",
      "|    clip_fraction        | 0.565     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.2       |\n",
      "|    explained_variance   | 0.965     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0841   |\n",
      "|    n_updates            | 800       |\n",
      "|    policy_gradient_loss | -0.0412   |\n",
      "|    std                  | 0.172     |\n",
      "|    value_loss           | 0.00168   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.767       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080028474 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.27        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.765       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062005956 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.33        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0516     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.765      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 609        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07037409 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.37       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0722    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00168    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06975866 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.41       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0396    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.765      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07147922 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.43       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00413   |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.769      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07210706 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.47       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0634    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07621059 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.53       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 580         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084320545 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.61        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0872     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0389     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00172     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06105087 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.67       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0341    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07339345 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.72       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0425    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.788      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07745431 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.78       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0788    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00189    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06847678 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.83       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00953   |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06926756 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.87       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0121    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00182    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07123047 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.89       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.038     |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.802    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 626      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.069423 |\n",
      "|    clip_fraction        | 0.603    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.99     |\n",
      "|    explained_variance   | 0.966    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.00477 |\n",
      "|    n_updates            | 1100     |\n",
      "|    policy_gradient_loss | -0.038   |\n",
      "|    std                  | 0.166    |\n",
      "|    value_loss           | 0.00168  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07139013 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.08       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0572    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0375    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08020563 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.17       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000558   |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07225831 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00152    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060202695 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.32        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0599     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081427105 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0685     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.812     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 610       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0710643 |\n",
      "|    clip_fraction        | 0.607     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.42      |\n",
      "|    explained_variance   | 0.972     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0276   |\n",
      "|    n_updates            | 1220      |\n",
      "|    policy_gradient_loss | -0.0374   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.00145   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07947978 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.51       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.814       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090003036 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.61        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066383615 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.66        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0361      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0347     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071845986 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.079      |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08040166 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.73       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00914   |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 601        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07293001 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.78       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07490275 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.81       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00872   |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 613        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08197335 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.87       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0556     |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.819     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 622       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0846211 |\n",
      "|    clip_fraction        | 0.607     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.95      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0593   |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    std                  | 0.159     |\n",
      "|    value_loss           | 0.00114   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07460563 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.02       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0413    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06868534 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0323    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088656604 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.15        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 647       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0820597 |\n",
      "|    clip_fraction        | 0.611     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.19      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0555   |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.026    |\n",
      "|    std                  | 0.157     |\n",
      "|    value_loss           | 0.00117   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08753306 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.25       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.021     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 603         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074636735 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.31        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0723     |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0249     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.827     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 613       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0815771 |\n",
      "|    clip_fraction        | 0.613     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.4       |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0702   |\n",
      "|    n_updates            | 1540      |\n",
      "|    policy_gradient_loss | -0.0285   |\n",
      "|    std                  | 0.155     |\n",
      "|    value_loss           | 0.000986  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08163387 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.48       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08056565 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.55       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00996   |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08277128 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00828    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079535246 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.66        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000985    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09109728 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.76       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00734    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000856   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08027128 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.86       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06135562 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.95       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000866   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08589167 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000849   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 614       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0852352 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.1      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0283   |\n",
      "|    n_updates            | 1720      |\n",
      "|    policy_gradient_loss | -0.0228   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.000803  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08816552 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000872   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088889554 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0971     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000844    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09136314 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.003      |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000761   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07320543 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000722   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08476003 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0666    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000699   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094706535 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0487     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000685    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.842     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 638       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0761994 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.6      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0106   |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0197   |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 0.0007    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084403455 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000663    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098464236 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0339     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000749    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09352524 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0458     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000679   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08477177 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00201   |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07781105 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000648   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085517414 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00313    |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000637    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10047195 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0705    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 626       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0755472 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0437   |\n",
      "|    n_updates            | 2020      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.144     |\n",
      "|    value_loss           | 0.000582  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07490115 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0148    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09264617 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 623         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095015995 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.033       |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.00059     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 614        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09376697 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000541   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084395126 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000536    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08392839 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000549   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08967898 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00854    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000555   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086017475 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0624     |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000668    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 632       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1076259 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.5      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.028    |\n",
      "|    n_updates            | 2200      |\n",
      "|    policy_gradient_loss | -0.0115   |\n",
      "|    std                  | 0.141     |\n",
      "|    value_loss           | 0.000571  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09326954 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0697     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 625       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1096976 |\n",
      "|    clip_fraction        | 0.625     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.5      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.044     |\n",
      "|    n_updates            | 2240      |\n",
      "|    policy_gradient_loss | -0.0172   |\n",
      "|    std                  | 0.141     |\n",
      "|    value_loss           | 0.000534  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09319294 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0424     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 613        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08184843 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0351     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 618         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081594884 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0337     |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000608    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09697087 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00794    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08153903 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00611    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08861699 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000707   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.851    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 637      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.083017 |\n",
      "|    clip_fraction        | 0.611    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 11.8     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0578  |\n",
      "|    n_updates            | 2380     |\n",
      "|    policy_gradient_loss | -0.0128  |\n",
      "|    std                  | 0.139    |\n",
      "|    value_loss           | 0.000665 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10332062 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.00819   |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 643       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1082741 |\n",
      "|    clip_fraction        | 0.638     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0104    |\n",
      "|    n_updates            | 2420      |\n",
      "|    policy_gradient_loss | -0.0171   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000697  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09678111 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0554    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000752   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09556264 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09358374 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0472     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000686   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09814495 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.042     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.00945   |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09569852 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000686   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09069164 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0662     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.00825   |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09323658 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000586   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 611        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12179069 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0363     |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10573572 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12205781 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00467   |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00066    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 608        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11970589 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00334   |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000686   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10336381 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00061    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10851858 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.019      |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.00822   |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10770528 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.177      |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.00777   |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12958468 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000564   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11802169 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0507    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09672004 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.00859   |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000575   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13491133 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0152     |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000535   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 621        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11444421 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0367     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.00934   |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000557   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11471905 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000528   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.852     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 633       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1342574 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.000493  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11790337 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000574   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10336287 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00114    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0093    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12890795 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.00975   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000597   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120342396 |\n",
      "|    clip_fraction        | 0.654       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00599     |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000558    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118329965 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000557    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 605        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13750626 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0467    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000505   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11849091 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12549157 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0357     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.00623   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00053    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12861481 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0299     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13101217 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0667     |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.00962   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000462   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105370544 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0273      |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.00556    |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000503    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10231884 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.00879   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000521   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1310637 |\n",
      "|    clip_fraction        | 0.639     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0239   |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.0124   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.00048   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 619         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106395446 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000487    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12179128 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000444   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11090778 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0099    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000462   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09702445 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.00668   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000478   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11865421 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000428   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1128463 |\n",
      "|    clip_fraction        | 0.643     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.1      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0176   |\n",
      "|    n_updates            | 3220      |\n",
      "|    policy_gradient_loss | -0.0102   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000467  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12770386 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00402    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00044    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096821465 |\n",
      "|    clip_fraction        | 0.651       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00488    |\n",
      "|    n_updates            | 3260        |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.00044     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 647       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1146827 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.000738 |\n",
      "|    n_updates            | 3280      |\n",
      "|    policy_gradient_loss | -0.00932  |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000432  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 639       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1033119 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0029    |\n",
      "|    n_updates            | 3300      |\n",
      "|    policy_gradient_loss | -0.00558  |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000428  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100724556 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000391    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11545483 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00827   |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.00903   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000402   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112709686 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.076      |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00617    |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000451    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11332409 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0778    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000522   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 618         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122580245 |\n",
      "|    clip_fraction        | 0.662       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00676    |\n",
      "|    n_updates            | 3400        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000469    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 628       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1117394 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.5      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0344   |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | -0.00636  |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000496  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12391406 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0631     |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.00547   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000462   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10036941 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.00828   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000486   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104280986 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0546     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.00047     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113873884 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0287      |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000467    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 639       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1292196 |\n",
      "|    clip_fraction        | 0.648     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0364   |\n",
      "|    n_updates            | 3520      |\n",
      "|    policy_gradient_loss | -0.00225  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000456  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11371249 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0523     |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00555   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000484   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11720307 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0222    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00045    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12271025 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0549    |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.00781   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000477   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11243813 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00707   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000508   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11721233 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000503   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12357783 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0744     |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.00438   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000495   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10822634 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0353     |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00685   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000519   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13872279 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0471    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.00662   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108508445 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.014       |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.00729    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000531    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12744382 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0176    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00536   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000494   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12034707 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00746   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00047    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112409875 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000438    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 652        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15310106 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0631     |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000455   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14893389 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00842   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000447   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13109982 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00497   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000419   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14296728 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00435   |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00844   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000446   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11283584 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00258    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.00231   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000457   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 622         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122940026 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000442    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 634       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1222175 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.9      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0758    |\n",
      "|    n_updates            | 3900      |\n",
      "|    policy_gradient_loss | -0.00621  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000433  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09043318 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0593     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00442   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000406   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12580407 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00802   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 625       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1208544 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14        |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0463    |\n",
      "|    n_updates            | 3960      |\n",
      "|    policy_gradient_loss | -0.00582  |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.00045   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13168803 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00647   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000475   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11803713 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00854    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00476   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000447   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11659737 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00595   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000382   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13053599 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0633    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00576   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000426   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12435345 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0392     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00708   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000393   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13614742 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00954   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000417   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114025995 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000404    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12518075 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00794   |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00525   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13417892 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0082    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000399   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15093522 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0142     |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | 0.00226    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000404   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13641858 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.00876   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000397   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11630086 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.00593   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000358   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 634       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1390398 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.012    |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | -0.00737  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000376  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12385061 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00223   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000359   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17060924 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00152   |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00448   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000359   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15125349 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00102   |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | 0.000395   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11165275 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0531     |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00306   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000409   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13033035 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0583     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00559   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00039    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13523321 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00623   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 654       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1335993 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0281   |\n",
      "|    n_updates            | 4360      |\n",
      "|    policy_gradient_loss | -0.00608  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00037   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11507274 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00348   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000396   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13501821 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0255     |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00661   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000378   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14143452 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.126      |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00694   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000394   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14541669 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00625   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000414   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13543525 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00279   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000393   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13957016 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0052    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000416   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14842923 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0473    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00316   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00042    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12920287 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 9.87e-05   |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00519   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14297113 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00318   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000417   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12819827 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00948    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.000759  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000421   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15423985 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00378   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14859101 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00641   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000423   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16143362 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0779     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | 0.00125    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000397   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15108988 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0259     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | 0.0105     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000408   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13949601 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00306   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000404   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 633       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506513 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0673   |\n",
      "|    n_updates            | 4680      |\n",
      "|    policy_gradient_loss | 0.00744   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000407  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15094101 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | 0.00827    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000445   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13060589 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0565     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00241   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00039    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12358053 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | 0.00211    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000411   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12467895 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.031      |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00275   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000424   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13794306 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.155      |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00259   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00044    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15745611 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00282   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000437   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 634       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1462355 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0546   |\n",
      "|    n_updates            | 4820      |\n",
      "|    policy_gradient_loss | -0.00195  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000466  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15405937 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.135      |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | 0.00168    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00044    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15056068 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | 0.00737    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000459   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 639       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522774 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0417   |\n",
      "|    n_updates            | 4880      |\n",
      "|    policy_gradient_loss | -0.00417  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000464  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15380117 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0655    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000477   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 653        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15287784 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0618    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | 0.00196    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14445381 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0739    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | 0.000183   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000533   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11546872 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0331    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00384   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000502   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15192822 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0572     |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.0111     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 621        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15132825 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | 0.00452    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00046    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15989709 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0395     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | 0.00233    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000468   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15309413 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00936   |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | 0.00468    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00054    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15064873 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | 0.00683    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 631      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.150609 |\n",
      "|    clip_fraction        | 0.646    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.9     |\n",
      "|    explained_variance   | 0.993    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00291  |\n",
      "|    n_updates            | 5080     |\n",
      "|    policy_gradient_loss | 0.0188   |\n",
      "|    std                  | 0.122    |\n",
      "|    value_loss           | 0.000481 |\n",
      "--------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16042723 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00425   |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.0123     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1547633 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0502   |\n",
      "|    n_updates            | 5120      |\n",
      "|    policy_gradient_loss | 0.00414   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000485  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15436396 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00192   |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | 0.00424    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000491   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 632       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1498185 |\n",
      "|    clip_fraction        | 0.682     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.033    |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | -0.00587  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000517  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15538923 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0555    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | 0.0103     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13775116 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00881   |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.000878  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000524   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 635       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1505038 |\n",
      "|    clip_fraction        | 0.678     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00634  |\n",
      "|    n_updates            | 5220      |\n",
      "|    policy_gradient_loss | 0.00459   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000524  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15302445 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.048     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | 0.00602    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000516   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16319914 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | 0.00246    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000475   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16051136 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | 0.00191    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16353062 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0821    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | 0.0115     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000579   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15123889 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | 0.00381    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000575   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13368227 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00119   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000587   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 628       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1560814 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00637   |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | 0.00786   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000567  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14892879 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0878     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00157   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000574   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15254405 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0362    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00186    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000496   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 614       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1533527 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00805   |\n",
      "|    n_updates            | 5420      |\n",
      "|    policy_gradient_loss | -0.00334  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000503  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15162161 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.017      |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | 0.00752    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15120937 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0165     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00105   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15417168 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | 0.00547    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 642       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1357133 |\n",
      "|    clip_fraction        | 0.687     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0347   |\n",
      "|    n_updates            | 5500      |\n",
      "|    policy_gradient_loss | -0.00539  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00055   |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15259424 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0528     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.0044     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 609       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1511564 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0417   |\n",
      "|    n_updates            | 5540      |\n",
      "|    policy_gradient_loss | 0.00648   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000521  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15277581 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0781    |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00792    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 621       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1552084 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0646   |\n",
      "|    n_updates            | 5580      |\n",
      "|    policy_gradient_loss | 0.00168   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00057   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 623       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1515427 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.027    |\n",
      "|    n_updates            | 5600      |\n",
      "|    policy_gradient_loss | 0.00503   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00055   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 657        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14440508 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.0034    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15077838 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.00978    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14101042 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0147    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00335   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "Early stopping at step 5 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 14 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16236475 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0324     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.0223     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15028748 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | 0.000799   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16120608 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.081      |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00139   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000587   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15641794 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | 0.00626    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000565   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15015364 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0319     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | 0.00109    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000551   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 645       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1518605 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0162   |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | 0.00136   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000586  |\n",
      "---------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15362576 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | 0.0124     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14497566 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.00064    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000551   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15426351 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.064      |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | 0.00245    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f2a98051e10> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2a90070400>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15118256 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0639    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | 0.00292    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 58 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083026394 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.437      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.013       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.705      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 602        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05865692 |\n",
      "|    clip_fraction        | 0.476      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.93       |\n",
      "|    explained_variance   | 0.864      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0488    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00415    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 590        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04156413 |\n",
      "|    clip_fraction        | 0.453      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.909      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0608    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00344    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.726       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 615         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039351095 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0756     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040435933 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00507    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.713      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 610        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05139962 |\n",
      "|    clip_fraction        | 0.488      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.99       |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0487    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00254    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.716       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 615         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042998277 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.723     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 625       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0371707 |\n",
      "|    clip_fraction        | 0.49      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6         |\n",
      "|    explained_variance   | 0.947     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0516   |\n",
      "|    n_updates            | 160       |\n",
      "|    policy_gradient_loss | -0.0464   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00226   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 608        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03935047 |\n",
      "|    clip_fraction        | 0.506      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6          |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00216    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0482    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00233    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053000517 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00203     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055325985 |\n",
      "|    clip_fraction        | 0.495       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0754     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00201     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049667917 |\n",
      "|    clip_fraction        | 0.509       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.07        |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0749     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041265447 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00186     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05140389 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.17       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00191    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055394094 |\n",
      "|    clip_fraction        | 0.503       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0738     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047296397 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.26        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0684     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045662146 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0019      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057168685 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.35        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00178     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057931244 |\n",
      "|    clip_fraction        | 0.516       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00945     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.753      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05942586 |\n",
      "|    clip_fraction        | 0.545      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.753      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06454532 |\n",
      "|    clip_fraction        | 0.535      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.52       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0227    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05721081 |\n",
      "|    clip_fraction        | 0.544      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.59       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059702974 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 614        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06403508 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.76       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0451    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00182    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050410666 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.79        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05800891 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.83       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00177    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054704428 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.86        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0579     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0458     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06252526 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.88       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00176    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.763       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059007205 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.765      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 657        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06351886 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.94       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0614    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05438148 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.98       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.764     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0641596 |\n",
      "|    clip_fraction        | 0.552     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.04      |\n",
      "|    explained_variance   | 0.963     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0278   |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0408   |\n",
      "|    std                  | 0.173     |\n",
      "|    value_loss           | 0.00176   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.765      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06183095 |\n",
      "|    clip_fraction        | 0.553      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.07       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00922   |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.767       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058879077 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.083      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.04       |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00172     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05614431 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.16       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0411    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076685965 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.18        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00172     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 616        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06084165 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.25       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.773      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07637133 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.32       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0505    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060570143 |\n",
      "|    clip_fraction        | 0.564       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.38        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00844    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00174     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06758753 |\n",
      "|    clip_fraction        | 0.577      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.44       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0385    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.781    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 620      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0756   |\n",
      "|    clip_fraction        | 0.582    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.5      |\n",
      "|    explained_variance   | 0.966    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0898  |\n",
      "|    n_updates            | 820      |\n",
      "|    policy_gradient_loss | -0.0395  |\n",
      "|    std                  | 0.169    |\n",
      "|    value_loss           | 0.0017   |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07283773 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0155    |\n",
      "|    n_updates            | 840        |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07820149 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.62       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0608     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07130173 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.65       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0509    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08222717 |\n",
      "|    clip_fraction        | 0.577      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.76       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0492    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07553959 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.792       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077529974 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.88        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09857403 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.96       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.015      |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00162    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076866195 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8           |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00156     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07445126 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.02       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0546    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07033043 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.06       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0313    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07482388 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09266315 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.16       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.802      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07664464 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.21       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.056     |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06965304 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.033     |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.806      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08106138 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.36       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0626    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080895714 |\n",
      "|    clip_fraction        | 0.6         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.39        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0151      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07451097 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.45       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0334    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09319444 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.51       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0294     |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07549219 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.53       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00154    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07913365 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0819    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0318    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 645         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071497515 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.63        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0592     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.029      |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09361952 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.82     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 619      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.060542 |\n",
      "|    clip_fraction        | 0.585    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 8.72     |\n",
      "|    explained_variance   | 0.975    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.033   |\n",
      "|    n_updates            | 1280     |\n",
      "|    policy_gradient_loss | -0.0298  |\n",
      "|    std                  | 0.16     |\n",
      "|    value_loss           | 0.00131  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.821       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086491264 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.8         |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0197     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0317     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076823935 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.83        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08218521 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.88       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0876     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0293    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07758871 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.9        |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0467    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0277    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 638         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086902186 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.92        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 662        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06960669 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00823    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07463026 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0793    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09574257 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.09       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08288513 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.1        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0642    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000933   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08901705 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.15       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0911    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08101597 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.26       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0105     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079717055 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.34        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06552235 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.43       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0492    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000958   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06504452 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0686    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074981734 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.63        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000642    |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000968    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08042936 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.7        |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.116      |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 610         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079465516 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.78        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0499     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000948    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07811693 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.81       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 630         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077120066 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.8         |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.09       |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069855064 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.83        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000966    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07128805 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.9        |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000953   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08729279 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.99       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000938   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08870632 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0435     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000974   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 605         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071150295 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00159    |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000956    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079766646 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000968    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06789605 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00094    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07051171 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0463    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000916   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093257435 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000864    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 624       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0787533 |\n",
      "|    clip_fraction        | 0.607     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.4      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0569   |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0206   |\n",
      "|    std                  | 0.149     |\n",
      "|    value_loss           | 0.00081   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08464443 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000782   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0804158 |\n",
      "|    clip_fraction        | 0.617     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.5      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0779   |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0187   |\n",
      "|    std                  | 0.148     |\n",
      "|    value_loss           | 0.000882  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 612        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07584229 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0442    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000858   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089232825 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0431      |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000895    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 624         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081728496 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 1960        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000802    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08229107 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0518     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00077    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10347052 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06616684 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0506    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078893684 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000713    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08892734 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0652    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08536809 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00073    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07254579 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06574517 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0394     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07565128 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 642         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073806204 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.000724    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08437679 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000462  |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09739839 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07906012 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09360893 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 621       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0799834 |\n",
      "|    clip_fraction        | 0.604     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.4      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0452   |\n",
      "|    n_updates            | 2260      |\n",
      "|    policy_gradient_loss | -0.0134   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000664  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09167876 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0322     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000632   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101545155 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 2300        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000713    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09345818 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0481    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065618835 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00087    |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000666    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 621         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087739006 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.109       |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000628    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09578566 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0444     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08266491 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000589   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08863386 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000615   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09219651 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0171     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084143855 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00463     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000548    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06468792 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0505     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0091    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000558   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10218425 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0432    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000612   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08701455 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000615   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07269269 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 625         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088172995 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.026       |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000594    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 628         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103900455 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000599    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09691231 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00377    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097289525 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000573    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09870332 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 641         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074928954 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000536    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087957084 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000558    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07055547 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 616         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092883706 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0336     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000508    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10417364 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0479     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000527   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102390006 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000541    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08990644 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00193    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000483   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07875459 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000506   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09692462 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097734004 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000528    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09905626 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09776864 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08517865 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0516     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.00961   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000575   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08259727 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10205243 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.001     |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.00915   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00053    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 628       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0895951 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13        |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0535   |\n",
      "|    n_updates            | 2960      |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.000548  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09576801 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0463    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 614        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10742917 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094492 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099735364 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.105       |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000523    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09467739 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00055    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06867583 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0994     |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000501   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07316485 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0671     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000545   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09694205 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0575     |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000491   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08609341 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.03       |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000509   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09974991 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.00998   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000531   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104912624 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0834     |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000529    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10622282 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00162    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.00905   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000499   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 660       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1121343 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0404   |\n",
      "|    n_updates            | 3220      |\n",
      "|    policy_gradient_loss | -0.0137   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000532  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08341819 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000554   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11803727 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00611    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000515   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09419618 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0286     |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000542   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11133188 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0158     |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.00959   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000495   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10414926 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00511    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000532   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102251366 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00562    |\n",
      "|    n_updates            | 3340        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000543    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 652        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09203006 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10139595 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0345     |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.00966   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09916923 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0125    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 644       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1107312 |\n",
      "|    clip_fraction        | 0.638     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0525   |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | -0.0056   |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000559  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10979791 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000574   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13392797 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00394   |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000568   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 632         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122594774 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0525     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.0006      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 644       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0926234 |\n",
      "|    clip_fraction        | 0.642     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0328   |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -0.00981  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000568  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10698672 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.00616   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000602   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13537857 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00057    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14301193 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000593   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11556436 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000607   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10953562 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0692     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00908   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00053    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11882675 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0552     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 617        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10573212 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000562   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10617238 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00504   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10760479 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0479    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000554   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117676236 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0232      |\n",
      "|    n_updates            | 3700        |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000543    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11664206 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00423   |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.00763   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11416793 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000485   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11917615 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000456   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10314258 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00862   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000482   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11611259 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0365    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.00468   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000474   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13153782 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0649    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00972   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000466   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15028721 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0201     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00569   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00048    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 631         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101479694 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0452     |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000568    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11513648 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.00668   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14908262 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00833   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000578   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13268673 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0311     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00908   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11026432 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13572164 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000573   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13927951 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0264     |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15407328 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00198   |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00621   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000579   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15111849 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000644   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14214218 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0169    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00474   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112491965 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0402      |\n",
      "|    n_updates            | 4060        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000643    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11862049 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10447323 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00408   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000639   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1306328 |\n",
      "|    clip_fraction        | 0.655     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0457    |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -0.00992  |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000751  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13326414 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00534   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 635       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1514943 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0186   |\n",
      "|    n_updates            | 4160      |\n",
      "|    policy_gradient_loss | -0.00711  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000799  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13741225 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.00848   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13244557 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00622   |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000794   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14109382 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0895     |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00756   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000824   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12550488 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.065     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00835   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15687492 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0515     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0054    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000787   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11746955 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00078    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14259364 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000805   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 647       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1334002 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0163    |\n",
      "|    n_updates            | 4320      |\n",
      "|    policy_gradient_loss | -0.00906  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00078   |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15821958 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 656        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15942992 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.0057    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000746   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15143722 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0396    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00705   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14279003 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00922   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13015302 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.007     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14321432 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0252     |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00922   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 45 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13160518 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0664    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00553   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1616801 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0389   |\n",
      "|    n_updates            | 4480      |\n",
      "|    policy_gradient_loss | -0.00547  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000735  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15618674 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.000684  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000765   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15654361 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0423     |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00632   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000702   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13495171 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12855992 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0256    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.00806   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15334952 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0744     |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00637   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000757   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13524827 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12179413 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.00441   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000752   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 619       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1340472 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00183   |\n",
      "|    n_updates            | 4640      |\n",
      "|    policy_gradient_loss | -0.00901  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000766  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 46 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12237103 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00844   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000748   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14903924 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00197    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00482   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 644        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16168082 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0722     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00629   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000706   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15638676 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0454     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00543   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12225344 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00509    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00594   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000637   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14653131 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 619        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13628271 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0064    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00332   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 639        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12449801 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0569     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00876   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15170944 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00627   |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | 1.94e-05   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15192997 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | 0.000492   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 623        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15193228 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0713    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15492727 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00153   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000641   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 623      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.149174 |\n",
      "|    clip_fraction        | 0.659    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.8     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0256  |\n",
      "|    n_updates            | 4900     |\n",
      "|    policy_gradient_loss | -0.00713 |\n",
      "|    std                  | 0.123    |\n",
      "|    value_loss           | 0.000737 |\n",
      "--------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16361797 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.00031   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11682415 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0937     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00578   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000716   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 648       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1505948 |\n",
      "|    clip_fraction        | 0.641     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.144     |\n",
      "|    n_updates            | 4960      |\n",
      "|    policy_gradient_loss | 0.00564   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00072   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14541589 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00897   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14988233 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0658    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00926   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000809   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 614       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1508203 |\n",
      "|    clip_fraction        | 0.65      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0678    |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | 4.76e-05  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000778  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 632       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1566848 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0358   |\n",
      "|    n_updates            | 5040      |\n",
      "|    policy_gradient_loss | -0.0087   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000722  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 631        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15648976 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | 0.00113    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14423962 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0318     |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00412   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 633       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1585367 |\n",
      "|    clip_fraction        | 0.649     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0319   |\n",
      "|    n_updates            | 5100      |\n",
      "|    policy_gradient_loss | 0.00163   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000738  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15720874 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0715    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.003     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 624        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14401798 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00555   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00078    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13609783 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.041     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00076    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13774067 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00706   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13862053 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00284    |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.00874   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 633       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1508539 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0137    |\n",
      "|    n_updates            | 5220      |\n",
      "|    policy_gradient_loss | -0.00942  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000738  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14605176 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0366    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.00421   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 613        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15112086 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0336     |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 625       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1532361 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0431   |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | 0.00539   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000673  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15263219 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0471     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00798   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 649       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1232473 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.041    |\n",
      "|    n_updates            | 5320      |\n",
      "|    policy_gradient_loss | -0.0106   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000686  |\n",
      "---------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15748982 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | 0.00304    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000687   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 619       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1513039 |\n",
      "|    clip_fraction        | 0.656     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0184   |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | -0.000498 |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000712  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 626        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12946256 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00562   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00066    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15148926 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.00558   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000722   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14329399 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.00748   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522784 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.052     |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -0.00879  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000758  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 621        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14076759 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0288    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15455215 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0801    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | 0.000313   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000699   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15229957 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000955   |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00666   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15174122 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15214781 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0144    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.00187   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000715   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 632       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1537356 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0596    |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | 0.00347   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000764  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13900925 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00755   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 625        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13643166 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0239     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000773   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 627       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1428286 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.062    |\n",
      "|    n_updates            | 5620      |\n",
      "|    policy_gradient_loss | -0.00204  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000819  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 661        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15158324 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0629     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.00134    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14990166 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00987   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16060594 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.00288    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15006022 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0485     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.000544  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16386434 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00327   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000784   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15083067 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0361    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00969   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 651       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1570319 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0595    |\n",
      "|    n_updates            | 5760      |\n",
      "|    policy_gradient_loss | -0.00399  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00073   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13292713 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00975   |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00846   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12145648 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00398    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.0084    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15300952 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0306     |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00209   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 618        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15070081 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00235   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.3, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.5],\n",
    "                                episode_limit_array=[150000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
