{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_singlegrid_half'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f9c87f1b2e8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9c89b2f5f8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.705    |\n",
      "| time/              |          |\n",
      "|    fps             | 367      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06951565 |\n",
      "|    clip_fraction        | 0.446      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.85       |\n",
      "|    explained_variance   | -0.667     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0254     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0371    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0163     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.718       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038291354 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.83        |\n",
      "|    explained_variance   | 0.876       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0012     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00427     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.715      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03987624 |\n",
      "|    clip_fraction        | 0.459      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.85       |\n",
      "|    explained_variance   | 0.912      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00945   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00349    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039883114 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.89        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040888347 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00678     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00279     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.733      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04812799 |\n",
      "|    clip_fraction        | 0.489      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.92       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0872    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00254    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.734      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04155857 |\n",
      "|    clip_fraction        | 0.475      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0488     |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00253    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05351439 |\n",
      "|    clip_fraction        | 0.493      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6          |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00249    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053068973 |\n",
      "|    clip_fraction        | 0.484       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0618     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00224     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04710008 |\n",
      "|    clip_fraction        | 0.505      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.05       |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0658    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00229    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.746       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047005914 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.09        |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0436     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00211     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050448876 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0724     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00216     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058924753 |\n",
      "|    clip_fraction        | 0.505       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0291     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00212     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048519522 |\n",
      "|    clip_fraction        | 0.521       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0733     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00205     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055722844 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0513     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.742     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0545552 |\n",
      "|    clip_fraction        | 0.51      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.28      |\n",
      "|    explained_variance   | 0.954     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0633   |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.0432   |\n",
      "|    std                  | 0.18      |\n",
      "|    value_loss           | 0.00197   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052461974 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00154     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00177     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053902708 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0619     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00186     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045360465 |\n",
      "|    clip_fraction        | 0.541       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00933     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.752      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05217589 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.35       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.75       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05835058 |\n",
      "|    clip_fraction        | 0.527      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.32       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0588    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00184    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05374332 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.32       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.757      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05686102 |\n",
      "|    clip_fraction        | 0.517      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.36       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05804508 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.41       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.763      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05034511 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.43       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06806062 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00123   |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00172    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050565958 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.52        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00866    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00169     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.769     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 399       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0634646 |\n",
      "|    clip_fraction        | 0.547     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.59      |\n",
      "|    explained_variance   | 0.961     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0354   |\n",
      "|    n_updates            | 560       |\n",
      "|    policy_gradient_loss | -0.0452   |\n",
      "|    std                  | 0.177     |\n",
      "|    value_loss           | 0.00177   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057360776 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.62        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0601     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00178     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05088895 |\n",
      "|    clip_fraction        | 0.541      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00246   |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062485136 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0805     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052350808 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.72        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0284     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00168     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06947879 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.76       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0543    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00176    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06236727 |\n",
      "|    clip_fraction        | 0.559      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.81       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0661    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00184    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07022885 |\n",
      "|    clip_fraction        | 0.567      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.88       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056798678 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0387     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00174     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.787      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06287132 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.01       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.042     |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.787      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06727062 |\n",
      "|    clip_fraction        | 0.56       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.06       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0154    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.789     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0679609 |\n",
      "|    clip_fraction        | 0.567     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.12      |\n",
      "|    explained_variance   | 0.967     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0511   |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.04     |\n",
      "|    std                  | 0.173     |\n",
      "|    value_loss           | 0.00159   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06644224 |\n",
      "|    clip_fraction        | 0.572      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.18       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.789       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063024774 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.23        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.0405     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.788     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0635656 |\n",
      "|    clip_fraction        | 0.569     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.31      |\n",
      "|    explained_variance   | 0.968     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0713   |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.0397   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00154   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057181112 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.38        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07468913 |\n",
      "|    clip_fraction        | 0.582      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.43       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08249484 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.45       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0779    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00162    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06126952 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.43       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07442649 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.42       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07469214 |\n",
      "|    clip_fraction        | 0.566      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.4        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07693562 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.47       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00878    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07559548 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.802      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06935229 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0438    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07529944 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.7        |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.806      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06997828 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.75       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0176    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080234475 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0362     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091119766 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.87        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0368     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07021944 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.94       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.812     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0748721 |\n",
      "|    clip_fraction        | 0.587     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.07      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0344   |\n",
      "|    n_updates            | 1140      |\n",
      "|    policy_gradient_loss | -0.0353   |\n",
      "|    std                  | 0.165     |\n",
      "|    value_loss           | 0.00145   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08827482 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.19       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.813       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088835284 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.26        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073953435 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.31        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0397     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.0016      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091002986 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0496     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079580344 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.5         |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00156     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081606105 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.63        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0751     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0333     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08007504 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.69       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.03      |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08844456 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.77       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00643    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.031     |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08291106 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0393     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07857082 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.88       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.029     |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07867686 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.92       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0656     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08471253 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.01       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00647   |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089872226 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.1         |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0939     |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077588215 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.16        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084970735 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.23        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0569     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10004294 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.31       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0894    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.826     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 396       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0723746 |\n",
      "|    clip_fraction        | 0.604     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.38      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0585   |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0289   |\n",
      "|    std                  | 0.156     |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09341308 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.46       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.054     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08322309 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09790496 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0809    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0339    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08925069 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.63       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068075255 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.74        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0732     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08820995 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.82       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.082     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0273    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099382386 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.87        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0937     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0272     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08237245 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.96       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0839     |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07014485 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.037     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08709171 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0825    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10562412 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0649    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08007923 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0306    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09751121 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0267    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08806189 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0402    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0302    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088201836 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0533     |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08219439 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0741    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08338688 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09530179 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0904    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.839     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0852531 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.6      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00196  |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0226   |\n",
      "|    std                  | 0.148     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08534091 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08610346 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00332    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10212852 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0253     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07203456 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09215264 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09695175 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09615513 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0659    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09310113 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00642   |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07241226 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00188   |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10790865 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0712    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10989685 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0155    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102905646 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0669     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10543178 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08186723 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10929046 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10354779 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0528    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000996   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09248132 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00114   |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000943   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11187329 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0226     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109685205 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00529    |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09867485 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000967   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10207876 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00867   |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000934   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.842     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 392       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1027445 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0318    |\n",
      "|    n_updates            | 2300      |\n",
      "|    policy_gradient_loss | -0.0231   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000897  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12310119 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000948   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104890004 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.101      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000983    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110172465 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0748     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092378475 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00693     |\n",
      "|    n_updates            | 2380        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000959    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107390955 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0532     |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000913    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10925269 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0717    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000839   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10934049 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10252776 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0317     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097716585 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00702    |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0231     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000876    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124251746 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0693     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.024      |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000892    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10750599 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11390736 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00095    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11243918 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000146   |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000943   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12054002 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0578    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 375       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1198956 |\n",
      "|    clip_fraction        | 0.65      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13        |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0164    |\n",
      "|    n_updates            | 2600      |\n",
      "|    policy_gradient_loss | -0.0201   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12009616 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121524096 |\n",
      "|    clip_fraction        | 0.663       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0421     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000897    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15168169 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0399     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11684008 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000864   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11375149 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000905   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 400         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118080914 |\n",
      "|    clip_fraction        | 0.639       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000886    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12199205 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.112      |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13174553 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000958   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13086489 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00088    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123733744 |\n",
      "|    clip_fraction        | 0.662       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.056      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000978    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11463897 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15063164 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0849     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13584165 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000925   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12853873 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000927   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11183367 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000909   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13747533 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0626    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00088    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14183009 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15615773 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0844     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11513617 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0437    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000889   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12841018 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00682   |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13607606 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0211    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1349043 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00631   |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | -0.0156   |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.000924  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13137522 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000877   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11089449 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1293235 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0652   |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.0155   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000828  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13854404 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00227    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.844    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 382      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.117658 |\n",
      "|    clip_fraction        | 0.663    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.2     |\n",
      "|    explained_variance   | 0.986    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0462   |\n",
      "|    n_updates            | 3140     |\n",
      "|    policy_gradient_loss | -0.0133  |\n",
      "|    std                  | 0.125    |\n",
      "|    value_loss           | 0.000876 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12477052 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00695   |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000833   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13188696 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.054      |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12836382 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000946   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13592868 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121843934 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00321    |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000859    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12552337 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00333   |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13369049 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000851   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120871544 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00315    |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000846    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14001891 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0102     |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15587193 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 399       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1310539 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0128   |\n",
      "|    n_updates            | 3360      |\n",
      "|    policy_gradient_loss | -0.0153   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000758  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15204717 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0444    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00081    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13079667 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0059    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15080824 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00708    |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000836   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13990827 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14256468 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 392       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1297671 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.6      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00333  |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -0.0166   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000841  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15359855 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.051      |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00972   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000833   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15998058 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0295     |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.00795   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00079    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15125284 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00883   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.846    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 386      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.159094 |\n",
      "|    clip_fraction        | 0.66     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.8     |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.023   |\n",
      "|    n_updates            | 3560     |\n",
      "|    policy_gradient_loss | -0.012   |\n",
      "|    std                  | 0.122    |\n",
      "|    value_loss           | 0.000832 |\n",
      "--------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15650246 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0308     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.00844   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15433529 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00327   |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | 0.00204    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000786   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15234798 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.00228   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000743   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12901701 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000828   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15422595 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.00788   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 404        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14487234 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0491    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000923   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15753333 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1549842 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0663   |\n",
      "|    n_updates            | 3720      |\n",
      "|    policy_gradient_loss | -0.00851  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000967  |\n",
      "---------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15002498 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00126   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15135604 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0264    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1578283 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0428   |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | -0.0105   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000879  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15052894 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0711    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0022    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000903   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15907781 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15731427 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00536   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000935   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15324652 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00849   |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | 0.0116     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000964   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15707187 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | 0.00699    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15453252 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0708    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.000269  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000994   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15219456 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00834   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 382      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.162972 |\n",
      "|    clip_fraction        | 0.666    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.3     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.161    |\n",
      "|    n_updates            | 3940     |\n",
      "|    policy_gradient_loss | 0.00481  |\n",
      "|    std                  | 0.119    |\n",
      "|    value_loss           | 0.000988 |\n",
      "--------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15441446 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0271     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.00845   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000994   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15463719 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000402  |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00554   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000973   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 384       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1503775 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0211   |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | -0.00259  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000876  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15000488 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0761     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00483   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15747313 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.118      |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00695   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000887   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13761811 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0816    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000874   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1579813 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0232    |\n",
      "|    n_updates            | 4080      |\n",
      "|    policy_gradient_loss | -0.0149   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000832  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15508434 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00996   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000822   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1528699 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.025    |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -0.00714  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.0009    |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15147404 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00551   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14115389 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15452668 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15150484 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000886   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1547507 |\n",
      "|    clip_fraction        | 0.651     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.113     |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | 0.0125    |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000799  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13100207 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00244    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15039936 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0394     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | 0.00304    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000766   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15046623 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15444882 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00832   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15115522 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00725   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14614637 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0635    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00992   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000902   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15537457 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.000653  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000833   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15677209 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00397    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00879   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000903   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15183312 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0138     |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00453   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15823463 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.00236   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15598889 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0938     |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | 0.00565    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15954262 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0304     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.008     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15458784 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0432     |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | 0.00167    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000782   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15136549 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0664     |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00756   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000808   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 384       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1523338 |\n",
      "|    clip_fraction        | 0.68      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.045    |\n",
      "|    n_updates            | 4520      |\n",
      "|    policy_gradient_loss | -0.00173  |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000808  |\n",
      "---------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15007368 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0843     |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | 0.00981    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15175654 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.000581  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000788   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15534306 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0012    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15297334 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00899   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 386       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1517237 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0171   |\n",
      "|    n_updates            | 4620      |\n",
      "|    policy_gradient_loss | 0.00849   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000716  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15117848 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | 0.00311    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000687   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15183067 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0921     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | 0.00524    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00076    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15792358 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.116      |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00996   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15806827 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.000245  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00072    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15175816 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0796     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00482   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000653   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15017678 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00477   |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000722   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15392563 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00902    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | 0.00511    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 402       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1523706 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.121     |\n",
      "|    n_updates            | 4780      |\n",
      "|    policy_gradient_loss | 0.00538   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000709  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15213947 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00217    |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00409   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16485476 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0537    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | 0.00784    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15576959 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00848   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1643705 |\n",
      "|    clip_fraction        | 0.689     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0141    |\n",
      "|    n_updates            | 4860      |\n",
      "|    policy_gradient_loss | -0.00723  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000725  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15488283 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0814    |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00305   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16119623 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0133     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | 0.00263    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15057778 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0411     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | 0.00987    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15709136 |\n",
      "|    clip_fraction        | 0.69       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15575013 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0809     |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | 0.00496    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15542054 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00592   |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00387    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.849    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 387      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.153815 |\n",
      "|    clip_fraction        | 0.655    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.03    |\n",
      "|    n_updates            | 5000     |\n",
      "|    policy_gradient_loss | 0.00843  |\n",
      "|    std                  | 0.116    |\n",
      "|    value_loss           | 0.000715 |\n",
      "--------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16607329 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0382     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00104   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15119514 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0312     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | 0.016      |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 398       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1639098 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0641    |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -0.00378  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000672  |\n",
      "---------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.85     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 390      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.158951 |\n",
      "|    clip_fraction        | 0.659    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.8     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0314  |\n",
      "|    n_updates            | 5080     |\n",
      "|    policy_gradient_loss | 0.00936  |\n",
      "|    std                  | 0.116    |\n",
      "|    value_loss           | 0.000731 |\n",
      "--------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15475783 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.0097     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 396       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1551507 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00425  |\n",
      "|    n_updates            | 5120      |\n",
      "|    policy_gradient_loss | 0.0101    |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000659  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1588045 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.037     |\n",
      "|    n_updates            | 5140      |\n",
      "|    policy_gradient_loss | 0.00756   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000649  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 395       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1566476 |\n",
      "|    clip_fraction        | 0.676     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0374   |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | -0.0037   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000659  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16442451 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0313     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1502701 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0273   |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | 0.00102   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000616  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15086845 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.021     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00287   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15304005 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.068     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.00167   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 396       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1553307 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.136     |\n",
      "|    n_updates            | 5260      |\n",
      "|    policy_gradient_loss | 0.00193   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000612  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15479457 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0427    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00991   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1545641 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0164   |\n",
      "|    n_updates            | 5300      |\n",
      "|    policy_gradient_loss | 0.00294   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000616  |\n",
      "---------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15343824 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0368    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | 0.00281    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000664   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15623638 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0856     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | 0.0048     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000621   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15240262 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0191     |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.00317   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000653   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15092273 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0495     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | 0.0059     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15982611 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00357    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00628    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000657   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 404        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17010848 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0424    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.0106     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15281124 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0104     |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00135   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16325217 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | 0.000744   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15042989 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0694    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | 0.0144     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000748   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15657803 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0273    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | 0.00446    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 396       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506886 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0702   |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | 0.00366   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000709  |\n",
      "---------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16470459 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | 0.0102     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000749   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15821476 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0614     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00792    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 389       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1606712 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.12      |\n",
      "|    n_updates            | 5580      |\n",
      "|    policy_gradient_loss | 0.00643   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000753  |\n",
      "---------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1644274 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0526    |\n",
      "|    n_updates            | 5600      |\n",
      "|    policy_gradient_loss | 0.013     |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000729  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15671961 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0847     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00626   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16054404 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | 0.0082     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15531752 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000586   |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00726   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00065    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15847299 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.0018     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15807804 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0115     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | 0.00716    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15524524 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0463     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00739    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15695262 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0742    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00167   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15958194 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | 0.00173    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14330569 |\n",
      "|    clip_fraction        | 0.693      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0332     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00957   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000606   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15741257 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.000229  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 403        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15115513 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0271     |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00361   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15629569 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000583   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f9c89b0dda0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9c14354940>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15289764 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.00798   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000563   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054874636 |\n",
      "|    clip_fraction        | 0.413       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | -0.118      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0125      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.692       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048727877 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.014      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00424     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 377         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039280735 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.087      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00357     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.69        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037552178 |\n",
      "|    clip_fraction        | 0.453       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00335     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.703      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04280736 |\n",
      "|    clip_fraction        | 0.469      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0457    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00295    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.722      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05223161 |\n",
      "|    clip_fraction        | 0.483      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.02       |\n",
      "|    explained_variance   | 0.938      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0472    |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0472    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00259    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04258121 |\n",
      "|    clip_fraction        | 0.484      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00242    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.734      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05016265 |\n",
      "|    clip_fraction        | 0.492      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0025     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.741      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04841368 |\n",
      "|    clip_fraction        | 0.486      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.05       |\n",
      "|    explained_variance   | 0.94       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00241    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03319653 |\n",
      "|    clip_fraction        | 0.502      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.06       |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0678    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00218    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05940623 |\n",
      "|    clip_fraction        | 0.5        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.09       |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0427    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00202    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044543542 |\n",
      "|    clip_fraction        | 0.49        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.1         |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0889     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00207     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056382276 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.11        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0845     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.0022      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.744      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05546862 |\n",
      "|    clip_fraction        | 0.522      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.13       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0201    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00205    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044742752 |\n",
      "|    clip_fraction        | 0.509       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0596     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.746       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051667236 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.18        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0889     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.75      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 404       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0557412 |\n",
      "|    clip_fraction        | 0.519     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.24      |\n",
      "|    explained_variance   | 0.955     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0524   |\n",
      "|    n_updates            | 340       |\n",
      "|    policy_gradient_loss | -0.0438   |\n",
      "|    std                  | 0.18      |\n",
      "|    value_loss           | 0.00195   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.751      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06381259 |\n",
      "|    clip_fraction        | 0.522      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.28       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0531    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050019495 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.34        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0506     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00188     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053618826 |\n",
      "|    clip_fraction        | 0.53        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.39        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00633     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00194     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059289724 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.45        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00428    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.0018      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055775814 |\n",
      "|    clip_fraction        | 0.532       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0777     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.0019      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057103656 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.57        |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0638     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00197     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.753      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05869348 |\n",
      "|    clip_fraction        | 0.528      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0451    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00178    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05596555 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0444     |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050429564 |\n",
      "|    clip_fraction        | 0.545       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00174     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.763      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05938524 |\n",
      "|    clip_fraction        | 0.543      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.71       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00373    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00188    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.765       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056757785 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.78        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05474347 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.81       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00182    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.758      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07025282 |\n",
      "|    clip_fraction        | 0.548      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0887    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050425977 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.87        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0146      |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00171     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054327942 |\n",
      "|    clip_fraction        | 0.558       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061625995 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0449     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00175     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.762       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065135926 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.93        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0353      |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00164     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07421149 |\n",
      "|    clip_fraction        | 0.567      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.97       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00168    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.763       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065166965 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.04        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.763     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 392       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0657358 |\n",
      "|    clip_fraction        | 0.563     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.08      |\n",
      "|    explained_variance   | 0.963     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00824  |\n",
      "|    n_updates            | 740       |\n",
      "|    policy_gradient_loss | -0.0424   |\n",
      "|    std                  | 0.173     |\n",
      "|    value_loss           | 0.00173   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06337946 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.13       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.765       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066351816 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.16        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0544     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0386     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00168     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07318449 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.22       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06636414 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0787    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.0017     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.77      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 378       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0662156 |\n",
      "|    clip_fraction        | 0.575     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.32      |\n",
      "|    explained_variance   | 0.963     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0736   |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00175   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065000325 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.37        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0873     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045458805 |\n",
      "|    clip_fraction        | 0.562       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.43        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0601     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00168     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063304745 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.45        |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074241795 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.46        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059946902 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.5         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.059      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088641696 |\n",
      "|    clip_fraction        | 0.588       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.57        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0608     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07383388 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0618    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059370387 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.61        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0371     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073843874 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.7         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0412     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06931661 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.75       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0709    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07736858 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.78       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07215893 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.83       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00479   |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06458119 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.9        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00394    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06922829 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8          |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00504    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074570894 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.08        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0809     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.0015      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.802       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074800774 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.17        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0361     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00149     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.802       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078938805 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.22        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.0382     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 397       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0733516 |\n",
      "|    clip_fraction        | 0.6       |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.28      |\n",
      "|    explained_variance   | 0.972     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0306   |\n",
      "|    n_updates            | 1200      |\n",
      "|    policy_gradient_loss | -0.0367   |\n",
      "|    std                  | 0.164     |\n",
      "|    value_loss           | 0.00143   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07368705 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.32       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07677201 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.38       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0349    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0357    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08897878 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.43       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0338    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074870646 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.46        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.103      |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0354     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00146     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075679064 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.52        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08733928 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0698    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0323    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08665381 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.61       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0494    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05823748 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.032     |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08650184 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.67       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08902993 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.74       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0349    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072854236 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.78        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.106       |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08299084 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.86       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0301    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08653037 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.92       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0912     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09325014 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07568854 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.03       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08521654 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.09       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0707     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089868166 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.15        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.000953    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.823     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0860865 |\n",
      "|    clip_fraction        | 0.602     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.17      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0183    |\n",
      "|    n_updates            | 1560      |\n",
      "|    policy_gradient_loss | -0.0256   |\n",
      "|    std                  | 0.157     |\n",
      "|    value_loss           | 0.000972  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 402        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08727435 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000954   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07558352 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000936   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0740581 |\n",
      "|    clip_fraction        | 0.607     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.26      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0599   |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -0.0217   |\n",
      "|    std                  | 0.157     |\n",
      "|    value_loss           | 0.000922  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06829699 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.35       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000851   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09095876 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.46       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088701665 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.55        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0903     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0252     |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.000888    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07088785 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.61       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.0009     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088780366 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.68        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0625     |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000886    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09948598 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0237    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086456016 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000787    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09737526 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.87       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000769   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07831943 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.93       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00551   |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09953274 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000655   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080286294 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000699    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.838     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 389       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1045651 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.000727 |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0208   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.000698  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.839     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0871138 |\n",
      "|    clip_fraction        | 0.61      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.3      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0487   |\n",
      "|    n_updates            | 1880      |\n",
      "|    policy_gradient_loss | -0.0205   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.000746  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10432935 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00188    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07972409 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089740865 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0475     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000646    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09039365 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07897877 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000572   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.842     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0883652 |\n",
      "|    clip_fraction        | 0.622     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0427   |\n",
      "|    n_updates            | 2000      |\n",
      "|    policy_gradient_loss | -0.0181   |\n",
      "|    std                  | 0.147     |\n",
      "|    value_loss           | 0.000565  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074550904 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.059      |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000584    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100515105 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000618    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083230995 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000616    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0863475 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00338   |\n",
      "|    n_updates            | 2080      |\n",
      "|    policy_gradient_loss | -0.0156   |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.000616  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10857068 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0509     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000611   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097266495 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000569    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08614465 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0214    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00056    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09488921 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0857     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090894476 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00734     |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000642    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10102743 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0771    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000639   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08478248 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000598   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120152116 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000618    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09624626 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000541   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09359448 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11949153 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00799   |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000479   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11055811 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000436   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096798874 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000667    |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000519    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09909996 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00049    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10446634 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.033     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000479   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11145978 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0274    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09865922 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0625    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000477   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0979507 |\n",
      "|    clip_fraction        | 0.628     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12        |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0408   |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -0.0141   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000536  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1206357 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0564   |\n",
      "|    n_updates            | 2460      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000515  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 392       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0924622 |\n",
      "|    clip_fraction        | 0.64      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.2      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0369   |\n",
      "|    n_updates            | 2480      |\n",
      "|    policy_gradient_loss | -0.0149   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000528  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09312988 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00392    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000499   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103907265 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00936    |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000558    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09116713 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000318   |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.00924   |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11924921 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00146   |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000568   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 392       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1136894 |\n",
      "|    clip_fraction        | 0.632     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.4      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.067    |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | -0.0126   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.000543  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102574065 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000518    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101305105 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000498    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105575085 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000485    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11462365 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0546    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00046    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09256725 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000548   |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.00737   |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000476   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11620925 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000444   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11448791 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000452   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10419639 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.058     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000408   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09334197 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00125    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000401   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11654136 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00041    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09389814 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0229     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.00947   |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000463   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112201475 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0942      |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000454    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10971888 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000456   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09907266 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00049    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10955851 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0597    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.00978   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000502   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12223556 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0211    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.00667   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13037154 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12613449 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.00963   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08699252 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0612     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.00649   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13928589 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0138    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12571253 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000539   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13087906 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.00757   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000537   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12763895 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000512   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11676355 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0275     |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.00759   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000528   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11926607 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.017      |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11422415 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.00749   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000528   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12836294 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.00882   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00054    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12747829 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000558   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11159472 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00991    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000581   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11743828 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0341     |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.00832   |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000556   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13856693 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000824  |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000522   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09578397 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000559   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13449049 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00466   |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.00678   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000547   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12891582 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0652    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000548   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14956726 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.00852   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12009919 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0581    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.00661   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000529   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13923119 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.852     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1534882 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0677    |\n",
      "|    n_updates            | 3340      |\n",
      "|    policy_gradient_loss | -0.00586  |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000543  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124265775 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00698     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000514    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1171857 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.5      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0702   |\n",
      "|    n_updates            | 3380      |\n",
      "|    policy_gradient_loss | -0.00834  |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000554  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11117379 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000546   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12712225 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12655535 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0376    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.00575   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000526   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14094192 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.00909   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000505   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11068465 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0151     |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.00472   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000539   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1394136 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.6      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0545   |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -0.0132   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000525  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14740388 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0446     |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.00845   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12600411 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0242     |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00715   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000583   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12767771 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00759    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.00523   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000606   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1560808 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0196   |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | -0.00462  |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000635  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12133982 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.00465   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000604   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12447568 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0469    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.009     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000582   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10934675 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0837     |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.00605   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000538   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15324628 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | 0.00077    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000514   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10776268 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00473   |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000503   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13871956 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00185   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000511   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1251594 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0409   |\n",
      "|    n_updates            | 3720      |\n",
      "|    policy_gradient_loss | -0.00736  |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000472  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14348392 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0704    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000437   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12732345 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.00567   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000412   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15281415 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.00668   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000447   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 384       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1403003 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.9      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00686  |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -0.00736  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000421  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15686916 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0571     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | 0.00208    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000399   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13203137 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0602    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000383   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 390       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1311965 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14        |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00502  |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | -0.00577  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000391  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117184736 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0189     |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000401    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15077645 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00518   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00043    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14279261 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00328   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000452   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13767228 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00648   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000395   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1345567 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14        |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0218   |\n",
      "|    n_updates            | 3960      |\n",
      "|    policy_gradient_loss | -0.00142  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000437  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12723523 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0863     |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00402   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000429   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13304953 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00192   |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000393   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13347974 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.11       |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00524   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000385   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 386       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1378861 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0613    |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | -0.00329  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000404  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1539985 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0645    |\n",
      "|    n_updates            | 4060      |\n",
      "|    policy_gradient_loss | -0.00653  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000353  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14047158 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0502    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.000452  |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000366   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12406249 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000788  |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00317   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000373   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11495049 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0625    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00145   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000375   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12360158 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.00493   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15172999 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00518   |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | 6.59e-05   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.0004     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12842469 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.155      |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0014    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000433   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1457092 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0283   |\n",
      "|    n_updates            | 4200      |\n",
      "|    policy_gradient_loss | -0.000388 |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000407  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15019424 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.076      |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000409   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15484051 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0477    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00575   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000404   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12984948 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00985   |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00449   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15046933 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | 0.00396    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000392   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15072659 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0138    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | 0.00334    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000377   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15050992 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0237    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | 0.000223   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00038    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15196845 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | 0.00189    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00038    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15637894 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0695     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | 0.00122    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000394   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13178106 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00295   |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00126   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000457   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16123363 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | 0.0021     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000457   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15580794 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0278     |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | 0.00181    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000436   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 389       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1575242 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0467   |\n",
      "|    n_updates            | 4440      |\n",
      "|    policy_gradient_loss | 0.00871   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000435  |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15727144 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00218   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000437   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1690654 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0207   |\n",
      "|    n_updates            | 4480      |\n",
      "|    policy_gradient_loss | 0.00656   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000451  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13206095 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000455   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14230368 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0472    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00606   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000409   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15478039 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.141      |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | 0.000216   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15357962 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.00757   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000377   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12955578 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0283    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00306   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000325   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12774496 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00376   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00036    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13845894 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00272   |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.00398   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000362   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16380903 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0612     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00528   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000335   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13799556 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00133    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0013    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15866041 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.007     |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00698   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000399   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14558358 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | 0.00334    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000363   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.856    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 385      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.143466 |\n",
      "|    clip_fraction        | 0.68     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.1     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0383  |\n",
      "|    n_updates            | 4720     |\n",
      "|    policy_gradient_loss | -0.00478 |\n",
      "|    std                  | 0.128    |\n",
      "|    value_loss           | 0.000395 |\n",
      "--------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15032545 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -4.22e-05  |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000396   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13184829 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0517    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00328   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15076733 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0363     |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | 0.00149    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000448   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13317876 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.057     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | 0.000922   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000454   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15136628 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0882    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | 0.00846    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00043    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14267342 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.00667   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.0004     |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15395305 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.103      |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | 0.00473    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000412   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14372481 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00436    |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00214   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000418   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1401985 |\n",
      "|    clip_fraction        | 0.684     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0308   |\n",
      "|    n_updates            | 4900      |\n",
      "|    policy_gradient_loss | -0.00524  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000372  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.856    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 384      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.115978 |\n",
      "|    clip_fraction        | 0.672    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.2     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0604  |\n",
      "|    n_updates            | 4920     |\n",
      "|    policy_gradient_loss | -0.00134 |\n",
      "|    std                  | 0.127    |\n",
      "|    value_loss           | 0.000392 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12937014 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0273     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | 0.000266   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000409   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15770888 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.096      |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | 0.0041     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000372   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15285829 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00823    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14616446 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0304     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00381   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000347   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13177383 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00474   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00034    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14913413 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00363   |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.00261   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00035    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 394       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1530413 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0017   |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -0.000177 |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000357  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16317168 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0201    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00395   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000418   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13582799 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.993      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.00184   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000408   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12104823 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.108      |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.000331  |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000368   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15000917 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00745   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000339   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15064636 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0668    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00588   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16043302 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0262     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | 0.00305    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000374   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 385       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1433512 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0396   |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | -0.00572  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000361  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13729218 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00753   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000366   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11388011 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.00628   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000369   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14707513 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00305   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.123301685 |\n",
      "|    clip_fraction        | 0.676       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0383      |\n",
      "|    n_updates            | 5280        |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000445    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 375       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506103 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.993     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0693    |\n",
      "|    n_updates            | 5300      |\n",
      "|    policy_gradient_loss | -0.00528  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000415  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14018685 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00828   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000388   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11688237 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00424   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000365   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15902641 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00947   |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.00633   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000363   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 381       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1311202 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.994     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0408   |\n",
      "|    n_updates            | 5380      |\n",
      "|    policy_gradient_loss | -0.00278  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000382  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13855013 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00659   |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00039    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13836184 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0131    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.00247   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000384   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13918844 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.121      |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00993   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000376   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 391      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.135225 |\n",
      "|    clip_fraction        | 0.681    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.2     |\n",
      "|    explained_variance   | 0.994    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0649  |\n",
      "|    n_updates            | 5460     |\n",
      "|    policy_gradient_loss | -0.013   |\n",
      "|    std                  | 0.127    |\n",
      "|    value_loss           | 0.000341 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14141448 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00906    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00914   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000346   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11095498 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0658    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00692   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000351   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15450288 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0349     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00742   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000328   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12902018 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.071      |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0038    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000325   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108369365 |\n",
      "|    clip_fraction        | 0.685       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 5560        |\n",
      "|    policy_gradient_loss | -0.00588    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000358    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12113271 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00729   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13946006 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0268     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.00813   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000355   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15025504 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0983     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00719   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00033    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13974203 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00263   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000336   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12383616 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00952   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15391447 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00449   |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.00129    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000312   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16007653 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0481    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00109   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000357   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15071061 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.088     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00759    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000333   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14429137 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00468   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000373   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15506239 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0818     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | 0.0088     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000405   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15626822 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00588    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.0018    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000393   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13945243 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.036     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000387   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15181132 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.994      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.00854    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000362   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14519408 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.995      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0234     |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00489   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000335   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f9c142fb518> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9c142a2240>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.696     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 379       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1523191 |\n",
      "|    clip_fraction        | 0.676     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.995     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0317   |\n",
      "|    n_updates            | 5860      |\n",
      "|    policy_gradient_loss | -7.74e-05 |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000325  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.696      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08111097 |\n",
      "|    clip_fraction        | 0.448      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.9        |\n",
      "|    explained_variance   | -0.437     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0322    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.013      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.703      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05601839 |\n",
      "|    clip_fraction        | 0.475      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.93       |\n",
      "|    explained_variance   | 0.868      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0193    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0487    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00412    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.717       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040070303 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.913       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00339     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041205455 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0681     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00304     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.714       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 377         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035660684 |\n",
      "|    clip_fraction        | 0.464       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0051     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00266     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.717    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 379      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.054179 |\n",
      "|    clip_fraction        | 0.5      |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 5.98     |\n",
      "|    explained_variance   | 0.938    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0625  |\n",
      "|    n_updates            | 120      |\n",
      "|    policy_gradient_loss | -0.0502  |\n",
      "|    std                  | 0.182    |\n",
      "|    value_loss           | 0.00251  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034445092 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.071      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00237     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038094055 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00219     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039371938 |\n",
      "|    clip_fraction        | 0.507       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0354      |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053081404 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00199     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.739       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057444118 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.02        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0771     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.045      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05307149 |\n",
      "|    clip_fraction        | 0.505      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0557    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00194    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03832407 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.12       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00186    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05775733 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.15       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0341    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00194    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060178757 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.22        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0523     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00194     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052642696 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.26        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0779     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.751       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056363545 |\n",
      "|    clip_fraction        | 0.53        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00192     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057596646 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.33        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0433     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0018      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05995184 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.4        |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0189    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050746728 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.45        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0904     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00168     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.758      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06419682 |\n",
      "|    clip_fraction        | 0.539      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.49       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00177    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054216944 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.54        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0724     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00188     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059265234 |\n",
      "|    clip_fraction        | 0.539       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.61        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0789     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063112244 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00176     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060968168 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.65        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0467     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.758     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 384       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0691389 |\n",
      "|    clip_fraction        | 0.555     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.7       |\n",
      "|    explained_variance   | 0.961     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0541   |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | -0.046    |\n",
      "|    std                  | 0.176     |\n",
      "|    value_loss           | 0.00181   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043633837 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.71        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0474     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0018      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.768      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05463151 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.72       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0851    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 391         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050303686 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.73        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0674     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0391     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.769       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070671454 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.78        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0844     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.769     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 379       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0700372 |\n",
      "|    clip_fraction        | 0.569     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.85      |\n",
      "|    explained_variance   | 0.962     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0455   |\n",
      "|    n_updates            | 620       |\n",
      "|    policy_gradient_loss | -0.044    |\n",
      "|    std                  | 0.175     |\n",
      "|    value_loss           | 0.00183   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06009183 |\n",
      "|    clip_fraction        | 0.566      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.92       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.062     |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.773      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06748147 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.778      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07134856 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00894   |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0415    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06180941 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.12       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00187    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068529226 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.18        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0559     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00174     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071354195 |\n",
      "|    clip_fraction        | 0.584       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.28        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0582     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00182     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.777    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 384      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.072666 |\n",
      "|    clip_fraction        | 0.57     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.33     |\n",
      "|    explained_variance   | 0.962    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.03    |\n",
      "|    n_updates            | 760      |\n",
      "|    policy_gradient_loss | -0.039   |\n",
      "|    std                  | 0.171    |\n",
      "|    value_loss           | 0.00182  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.781     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0698457 |\n",
      "|    clip_fraction        | 0.567     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.42      |\n",
      "|    explained_variance   | 0.96      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.076    |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.0391   |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00191   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064310744 |\n",
      "|    clip_fraction        | 0.573       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.47        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0742     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07061782 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.52       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.79        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065100364 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.58        |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0605     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0385     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00169     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07950334 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0125    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0389    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.791     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0658805 |\n",
      "|    clip_fraction        | 0.59      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.75      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0533   |\n",
      "|    n_updates            | 880       |\n",
      "|    policy_gradient_loss | -0.0383   |\n",
      "|    std                  | 0.168     |\n",
      "|    value_loss           | 0.00155   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.792       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080295905 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.85        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.795      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07181338 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.93       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07576482 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.02       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073531345 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.12        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00159     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.802       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074265875 |\n",
      "|    clip_fraction        | 0.588       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.16        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07820667 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.2        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0308     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07860587 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.3        |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077274874 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.37        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0754     |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00159     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086114906 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.4         |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0811     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06505003 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0427    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06786247 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.52       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0771    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08501889 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.6        |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0672    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076809175 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.65        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0564      |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.0016      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06929532 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.7        |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.813       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069968864 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0867     |\n",
      "|    n_updates            | 1180        |\n",
      "|    policy_gradient_loss | -0.033      |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07911225 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.78       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08597145 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.88       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079976775 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.96        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0372     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0328     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09658255 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08934367 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.11       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0712    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09651625 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.2        |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09104507 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.28       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0183    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0264    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07135209 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.36       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08389364 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.47       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08466618 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0783    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08414054 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098860145 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.7         |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0574     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.032      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00136     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09254594 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.75       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07686881 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.81       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0313    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09177869 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.88       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09227638 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.93       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00223   |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 371         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094435945 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10          |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0108     |\n",
      "|    n_updates            | 1520        |\n",
      "|    policy_gradient_loss | -0.0296     |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080376066 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0715     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.835    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 377      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.088921 |\n",
      "|    clip_fraction        | 0.623    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 10.2     |\n",
      "|    explained_variance   | 0.978    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0443  |\n",
      "|    n_updates            | 1560     |\n",
      "|    policy_gradient_loss | -0.0267  |\n",
      "|    std                  | 0.15     |\n",
      "|    value_loss           | 0.00123  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100000575 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09769119 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0309    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087899685 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00699     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10313835 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100387335 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09635852 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00469   |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08606496 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0807    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11388008 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10390379 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07863213 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06506254 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0708    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 359         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089567855 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096872285 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0414      |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107675456 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08415253 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.042     |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10882233 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000987   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10007107 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07911068 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000957   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104580484 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10322299 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106392264 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00457    |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000997    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10519154 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0761    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00099    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 374         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101874255 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0454     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00096     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 366         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105171695 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0528     |\n",
      "|    n_updates            | 2040        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000947    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09368216 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0273     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000911   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112354025 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000894    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 369       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0897028 |\n",
      "|    clip_fraction        | 0.625     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12        |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0056    |\n",
      "|    n_updates            | 2100      |\n",
      "|    policy_gradient_loss | -0.0175   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.00101   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13166252 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.844    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 364      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.090867 |\n",
      "|    clip_fraction        | 0.628    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 12.1     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0442  |\n",
      "|    n_updates            | 2140     |\n",
      "|    policy_gradient_loss | -0.0202  |\n",
      "|    std                  | 0.138    |\n",
      "|    value_loss           | 0.00101  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0989051 |\n",
      "|    clip_fraction        | 0.637     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.2      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0461   |\n",
      "|    n_updates            | 2160      |\n",
      "|    policy_gradient_loss | -0.0163   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.00103   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100979425 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00867    |\n",
      "|    n_updates            | 2180        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11121025 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0176    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10324053 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0707    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10266994 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10758567 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0682     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11470036 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.072     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10798435 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12045226 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0638    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000947   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106874146 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0469     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112819806 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0152     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.001       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10839126 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0509    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 355       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1106868 |\n",
      "|    clip_fraction        | 0.641     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0195   |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.0166   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09733267 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000912   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10725608 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.068     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10902353 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.017      |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000926   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111579515 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00627    |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000841    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09681451 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00079    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 361         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105062746 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.000881    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11242926 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000864   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12370501 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00755    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09102084 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0712    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 333        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11630152 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0762     |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 328        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12633638 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0322     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000856   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11502107 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00708    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000795   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13347211 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0327     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 334        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13073292 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0103     |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000809   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104783095 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000847    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10767941 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000832   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 328        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11571596 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 338       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1089226 |\n",
      "|    clip_fraction        | 0.644     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0119   |\n",
      "|    n_updates            | 2760      |\n",
      "|    policy_gradient_loss | -0.0129   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.000894  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12261816 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12538496 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11780081 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.027      |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000869   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 361        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10372442 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0266    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113748595 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12958029 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0634    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12162377 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14993611 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000951   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113383874 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000976    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11557704 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12551376 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0221    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13655856 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 356       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1392044 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.9      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.018    |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | -0.0153   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.00103   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 329        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14560401 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000923   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11625972 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0365    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11177747 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0725    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10437195 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0321     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13383566 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0506     |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000925   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15562972 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0496     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13126707 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13756791 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00127   |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000965   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102363065 |\n",
      "|    clip_fraction        | 0.652       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 3200        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000939    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12394141 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000956   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11907051 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0984     |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11469688 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00368   |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.00849   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000904   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115740344 |\n",
      "|    clip_fraction        | 0.66        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000883    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13319008 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000998   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12000134 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.00837   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14145882 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12219544 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000974   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12168082 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0263     |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15340382 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0205     |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 361         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093126535 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0492     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000974    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13261199 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000928   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10271956 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00854   |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000906   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122289635 |\n",
      "|    clip_fraction        | 0.663       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0686      |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000975    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12958202 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000906   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14371777 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0288    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13996872 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00808    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12775142 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124493405 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0507     |\n",
      "|    n_updates            | 3580        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000963    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12588076 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.134      |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 353       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1236639 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0726   |\n",
      "|    n_updates            | 3620      |\n",
      "|    policy_gradient_loss | -0.011    |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000865  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12079521 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0797    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000907   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12983955 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00089    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14652185 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000934   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12037692 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0248     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00716   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13017316 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0613    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 368       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1267333 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0198    |\n",
      "|    n_updates            | 3740      |\n",
      "|    policy_gradient_loss | -0.0113   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000897  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12974688 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0259     |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 363       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1354719 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0102    |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | -0.00978  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000831  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 361        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15891692 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 358       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1354005 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.1      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0136   |\n",
      "|    n_updates            | 3820      |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000821  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12692964 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0124     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000838   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12226193 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.00648   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000858   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15327203 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11820688 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00759   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15500237 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000828   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12418375 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0103     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 354       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1180707 |\n",
      "|    clip_fraction        | 0.676     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0105    |\n",
      "|    n_updates            | 3960      |\n",
      "|    policy_gradient_loss | -0.015    |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000792  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15954407 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0782    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00958   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000784   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.847    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 357      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.131708 |\n",
      "|    clip_fraction        | 0.67     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.1     |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0117  |\n",
      "|    n_updates            | 4000     |\n",
      "|    policy_gradient_loss | -0.0106  |\n",
      "|    std                  | 0.121    |\n",
      "|    value_loss           | 0.000791 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11771409 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0152     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000851   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14807422 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14750043 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000795   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15293701 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00327    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00143   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15863454 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | 0.000355   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000802   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15461376 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.00714   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000792   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14690533 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.102      |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000755   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14547613 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0167     |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000714   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16006741 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000715   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15402284 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00248    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.00359   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13587584 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.0088    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000724   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13637109 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0078    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000679   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15967524 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0399     |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00108   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000706   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13227291 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00405    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000682   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15065877 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0734     |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 361        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14202991 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00521   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000689   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14877331 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00306    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000767   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 362       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1338773 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.1       |\n",
      "|    n_updates            | 4360      |\n",
      "|    policy_gradient_loss | -0.00968  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000739  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14029023 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.00709   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000725   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12985626 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00636    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00072    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15178083 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.026      |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | 0.000956   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000852   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15479839 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00342   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000831   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14294828 |\n",
      "|    clip_fraction        | 0.69       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15007982 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00245   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000714   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15593448 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00932    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00297   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000702   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522265 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.061     |\n",
      "|    n_updates            | 4520      |\n",
      "|    policy_gradient_loss | 0.00659   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000811  |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15388137 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.00803   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000849   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15676841 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0773     |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.00124   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000821   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14290786 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0472     |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15320691 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0334     |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15344222 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.00845   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000885   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15640943 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.069     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -5.59e-05  |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000905   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15392774 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00623   |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00379   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15236941 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00355   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000902   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15468495 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.023      |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00773   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000953   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15063104 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000904  |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00212   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 15 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15497543 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0472    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | 0.0183     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15008803 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0433    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | 0.00506    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16039027 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0287     |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00593   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000993   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14153497 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0331     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000908   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15578322 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.141      |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | 0.00437    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15599392 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.168      |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | 0.00128    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16323607 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00153   |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | 0.00299    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000945   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15691455 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0108     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00141   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000963   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12923412 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0379     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00767   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15166853 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15827785 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0243    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0039    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16643663 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00105   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15595224 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00894    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00294    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 368       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522449 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.9      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00839   |\n",
      "|    n_updates            | 5000      |\n",
      "|    policy_gradient_loss | -0.00279  |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00105   |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 370       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1551956 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.04     |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | 0.00452   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.000985  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15099588 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0187     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.00728   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000982   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15659074 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0414    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16475698 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00415   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000987   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15152934 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0446    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | 0.00131    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15265629 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0652    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00507   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 365       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1546581 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0726    |\n",
      "|    n_updates            | 5140      |\n",
      "|    policy_gradient_loss | 0.0109    |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00105   |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15143286 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0227     |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 354       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1571079 |\n",
      "|    clip_fraction        | 0.689     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0235   |\n",
      "|    n_updates            | 5180      |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00109   |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15744689 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.00321   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15272143 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.033      |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0067    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00099    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15250857 |\n",
      "|    clip_fraction        | 0.695      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15006326 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0511     |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00119   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15070316 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0151     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00685   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15002358 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00434   |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.000766  |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000839   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15752855 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00082    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15526557 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | 0.00299    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16446783 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000296  |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.00572   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15274222 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0572     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | 0.000762   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000858   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15278837 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.00263   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000836   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17126414 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00714    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.000127   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00082    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15792914 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | 0.00597    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 16 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15300834 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | 0.0103     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16963005 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0402     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | 0.00759    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16883238 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.03      |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | 0.0075     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000852   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15620568 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0266    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | 0.00186    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000833   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15049633 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.00673   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15317735 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0601     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00046    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15256603 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00247   |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 18 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15768875 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0349    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | 0.00417    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000782   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 362       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1524228 |\n",
      "|    clip_fraction        | 0.686     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0471   |\n",
      "|    n_updates            | 5620      |\n",
      "|    policy_gradient_loss | -0.00704  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000787  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 366        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15073287 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0311     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00304   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000681   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 367       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1396363 |\n",
      "|    clip_fraction        | 0.682     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0186    |\n",
      "|    n_updates            | 5660      |\n",
      "|    policy_gradient_loss | -0.00959  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000785  |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 370       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1556799 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0194    |\n",
      "|    n_updates            | 5680      |\n",
      "|    policy_gradient_loss | 0.00634   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000798  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11962044 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00769   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15264009 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0627    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00836    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000751   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12663727 |\n",
      "|    clip_fraction        | 0.696      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000767   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15334229 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0611    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.000112  |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000724   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 355       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1524941 |\n",
      "|    clip_fraction        | 0.696     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.024     |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | -0.0124   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.00079   |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15646884 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0384     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.00605   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000737   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15740833 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.144      |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.00343    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15035401 |\n",
      "|    clip_fraction        | 0.695      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.3, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.5],\n",
    "                                episode_limit_array=[150000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
