{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_multigrid_fixed'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f645b008dd8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f645b008a90>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 137      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.679     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 746       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0707188 |\n",
      "|    clip_fraction        | 0.445     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.85      |\n",
      "|    explained_variance   | -0.64     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0213   |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0366   |\n",
      "|    std                  | 0.183     |\n",
      "|    value_loss           | 0.0156    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.7        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 730        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03175534 |\n",
      "|    clip_fraction        | 0.444      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.883      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00377    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.695       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 759         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035791736 |\n",
      "|    clip_fraction        | 0.456       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.87        |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0215      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00301     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.696       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031586558 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00767    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.704       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 727         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038644783 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0755     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00233     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 733         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042874157 |\n",
      "|    clip_fraction        | 0.494       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0524     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00205     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.715       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 762         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042074747 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00189     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.712       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046469264 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.06        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0448     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.717    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 736      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.049085 |\n",
      "|    clip_fraction        | 0.493    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.11     |\n",
      "|    explained_variance   | 0.961    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0788  |\n",
      "|    n_updates            | 180      |\n",
      "|    policy_gradient_loss | -0.0467  |\n",
      "|    std                  | 0.181    |\n",
      "|    value_loss           | 0.00163  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04640566 |\n",
      "|    clip_fraction        | 0.504      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.15       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.718      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04995772 |\n",
      "|    clip_fraction        | 0.487      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055400204 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00158     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.728      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 719        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05184882 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.27       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 729        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05387343 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.31       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0763    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00167    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05276501 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.36       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00155    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 728         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052379556 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0798     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 762        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05409802 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.46       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.746       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054557253 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.53        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0715     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00157     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04908336 |\n",
      "|    clip_fraction        | 0.52       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.58       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0428    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 749         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050528366 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 737        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06235838 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0886    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 716        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06051864 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00149    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 727        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05880314 |\n",
      "|    clip_fraction        | 0.536      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.6        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0926    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 813        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04857253 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.0015     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07528596 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 747         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061827272 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.64        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0699     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0429     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.773      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06836298 |\n",
      "|    clip_fraction        | 0.545      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.65       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05624243 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 712         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056877136 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.74        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061973475 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 720        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07079506 |\n",
      "|    clip_fraction        | 0.573      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.84       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0771    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.78     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 723      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 3        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.064237 |\n",
      "|    clip_fraction        | 0.559    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 6.92     |\n",
      "|    explained_variance   | 0.971    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0718  |\n",
      "|    n_updates            | 640      |\n",
      "|    policy_gradient_loss | -0.0414  |\n",
      "|    std                  | 0.174    |\n",
      "|    value_loss           | 0.00137  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 741         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065170564 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 753         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060446948 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.98        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0392     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 750        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06369358 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.05       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.014     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 746        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06640868 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.14       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.783       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 737         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065608464 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.22        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 717        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07560213 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.27       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0384    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069437705 |\n",
      "|    clip_fraction        | 0.578       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.35        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00325    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.037      |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07151332 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.41       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06294056 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.45       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071153745 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.5         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0601      |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0322     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07178465 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.59       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06554037 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 732        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07847029 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.78       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0585    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 743        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06510444 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.86       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07782781 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.91       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0381    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061063003 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.99        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0656     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.798      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 760        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07259835 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.09       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 764        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07358284 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.25       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.801       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 752         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064116515 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0407      |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07665865 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.46       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08085267 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07606259 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0576    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 777         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072046354 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.74        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00567     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08402011 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0942    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0346    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 718         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068703726 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.94        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 765        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08006798 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 769        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07962311 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.13       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000755   |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000951   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 734         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064977705 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.21        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0275     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000974    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 747        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07093988 |\n",
      "|    clip_fraction        | 0.592      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.28       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0852    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0271    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084386334 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 1240        |\n",
      "|    policy_gradient_loss | -0.0284     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000869    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07447239 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.44       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00909    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000836   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 766        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08865658 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.5        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 728        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281404 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.56       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 768         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083601676 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.67        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00136    |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.026      |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000843    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 765         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072260655 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.72        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0823     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0261     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.000846    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 737        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09999265 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.83       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000853   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 767        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.92       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.818     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 740       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0800878 |\n",
      "|    clip_fraction        | 0.615     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.99      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0538   |\n",
      "|    n_updates            | 1400      |\n",
      "|    policy_gradient_loss | -0.0258   |\n",
      "|    std                  | 0.152     |\n",
      "|    value_loss           | 0.000854  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 760         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086556196 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.000774    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 751        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08638317 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0269    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000837   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 775         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091239884 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0285     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000851    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.82      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 755       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0784907 |\n",
      "|    clip_fraction        | 0.604     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0808   |\n",
      "|    n_updates            | 1480      |\n",
      "|    policy_gradient_loss | -0.0264   |\n",
      "|    std                  | 0.15      |\n",
      "|    value_loss           | 0.00081   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 771         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081656635 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000784    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 733        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07281746 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.092     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07756476 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000834   |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07710326 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000222   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.822       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 746         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082571015 |\n",
      "|    clip_fraction        | 0.6         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0713     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000768    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 722        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09877135 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0825    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000732   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 754        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06918831 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000692   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 769         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090791225 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0536     |\n",
      "|    n_updates            | 1640        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000722    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08319668 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0869     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.824     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 743       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0894752 |\n",
      "|    clip_fraction        | 0.601     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00128  |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.0211   |\n",
      "|    std                  | 0.146     |\n",
      "|    value_loss           | 0.000713  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07546101 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000146   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000757   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 739        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10213168 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0851    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072928384 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.00085     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.824       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 764         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105452314 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0623     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.000761    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09592082 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000863   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 740        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10572268 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 743       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0814404 |\n",
      "|    clip_fraction        | 0.617     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0501   |\n",
      "|    n_updates            | 1820      |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000744  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 829        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08519796 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095315434 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0511     |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000696    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 761        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09649859 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0323     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000707   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104618594 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000784    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.826     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 749       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0859244 |\n",
      "|    clip_fraction        | 0.618     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.5      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0623   |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -0.0175   |\n",
      "|    std                  | 0.142     |\n",
      "|    value_loss           | 0.000707  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 748        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08383379 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f645b20a390> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f645ad0c470>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 139        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10461812 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.91e-05   |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 594        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14662427 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0083    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 602         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097623326 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 620        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10048504 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 615        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08831065 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00291   |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 622        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09289644 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09936123 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10795504 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 629       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1038513 |\n",
      "|    clip_fraction        | 0.623     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0318   |\n",
      "|    n_updates            | 2120      |\n",
      "|    policy_gradient_loss | -0.022    |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08388333 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07826731 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0496     |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 631       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0935414 |\n",
      "|    clip_fraction        | 0.611     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.3      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0474   |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0161   |\n",
      "|    std                  | 0.137     |\n",
      "|    value_loss           | 0.00115   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 627         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118246175 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 636         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112765096 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00337    |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 629         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108631566 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0815      |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10520711 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09064202 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0174     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10932287 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0473     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 643         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104588404 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.154       |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086992905 |\n",
      "|    clip_fraction        | 0.641       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 627        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08725697 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0552     |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09570773 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.122      |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 629       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0938755 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.4      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0319   |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10484123 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10422556 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.048     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 628        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08623483 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 652        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11267831 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 653        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09587612 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0841     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088183604 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13134083 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 661        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10233867 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10884801 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 645       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0935068 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0397   |\n",
      "|    n_updates            | 2600      |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10261114 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0231     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10610481 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0179     |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 633        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10823421 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 638       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1163579 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0598   |\n",
      "|    n_updates            | 2680      |\n",
      "|    policy_gradient_loss | -0.0161   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.00109   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10590993 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 653        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11012225 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0694    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000938   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08464231 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104668796 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00394    |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.001       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13335435 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10571144 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0405    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0921456 |\n",
      "|    clip_fraction        | 0.642     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13        |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0452   |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | -0.0139   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.000966  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 660        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11020992 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0521     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 652        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08718574 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0227    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000944   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 658        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10341996 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0035    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000934   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10591855 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0341    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000954   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 657       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0980331 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0273   |\n",
      "|    n_updates            | 2920      |\n",
      "|    policy_gradient_loss | -0.0162   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09627794 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.0009     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11980264 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0706    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 643        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11533673 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0297     |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000981   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12014923 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 656       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1284352 |\n",
      "|    clip_fraction        | 0.648     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0465    |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | -0.0157   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.00101   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12727761 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10846832 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0498     |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000998   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09705423 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00723    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000933   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 643       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1330241 |\n",
      "|    clip_fraction        | 0.642     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.5      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0198   |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.014    |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.00097   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 637       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1020725 |\n",
      "|    clip_fraction        | 0.648     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.6      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0516   |\n",
      "|    n_updates            | 3120      |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000968  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15009919 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00097    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12947652 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000961   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.846    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 621      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.110256 |\n",
      "|    clip_fraction        | 0.65     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.7     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0257  |\n",
      "|    n_updates            | 3180     |\n",
      "|    policy_gradient_loss | -0.013   |\n",
      "|    std                  | 0.129    |\n",
      "|    value_loss           | 0.00103  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 635        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13899429 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0341    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 660         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106777504 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0502      |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 646         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102111004 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000997    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12836626 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000991   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 630        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11960423 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 636        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10433404 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000976   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13092475 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000996   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 637        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12624526 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0174     |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000958   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 637         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117426395 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0036      |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000954    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 652        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13382336 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0632    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 654        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11964104 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00117   |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12058596 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0403    |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000996   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 646        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12315029 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0293     |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13074622 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0231    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15293327 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000967   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 640        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14660728 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0543     |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 658        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13776189 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0662     |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000991   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117608525 |\n",
      "|    clip_fraction        | 0.667       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00124    |\n",
      "|    n_updates            | 3540        |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000917    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 656        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13030349 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0221    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15613349 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0314     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | 0.0044     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12238892 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0786     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 649        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10547161 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0631     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 629        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13269517 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0495     |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13261971 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00331   |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14495246 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 641        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13316412 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0521    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000928   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15379074 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0408    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000924   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 634        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14830798 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0297    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000964   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 656         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119710445 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 3760        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000854    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 638        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 4          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13492091 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0683     |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000904   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1235498 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.3      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0332   |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000916  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 645        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10938829 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0214    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.00997   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12059051 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.00817   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 640       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1507475 |\n",
      "|    clip_fraction        | 0.651     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0546   |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | -0.00625  |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000856  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 642        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13225384 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0263     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 656       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1495228 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0646    |\n",
      "|    n_updates            | 3900      |\n",
      "|    policy_gradient_loss | -0.0143   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000856  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f645ad0c470> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f645b222320>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 115        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12636353 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0402    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 326        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15366831 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00224   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 332        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12112875 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.00953   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12586471 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11646231 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00365   |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 349       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1100363 |\n",
      "|    clip_fraction        | 0.647     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.029    |\n",
      "|    n_updates            | 4020      |\n",
      "|    policy_gradient_loss | -0.0144   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.00113   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13530307 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0741    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11315735 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12423446 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0341    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124132276 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000804    |\n",
      "|    n_updates            | 4100        |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10794236 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11607885 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0898    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14079556 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00468   |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13241829 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14087394 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000819   |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10894084 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15877259 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0859     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13206375 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00393    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15187402 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0571    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00443   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12547465 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12565029 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.085      |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12679109 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0544    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00979   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12532935 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13134141 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15025549 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0169    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.00726   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12484322 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13968925 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00265    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11946342 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00788   |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12750237 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0394    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14448038 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0714    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119224384 |\n",
      "|    clip_fraction        | 0.666       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0508     |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13523355 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 348       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1419702 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0547   |\n",
      "|    n_updates            | 4560      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00126   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12804309 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12945382 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13980058 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16670883 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13429046 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00107   |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1302258 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.6      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0665   |\n",
      "|    n_updates            | 4680      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14144866 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00932    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13393903 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0781    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1335862 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0611   |\n",
      "|    n_updates            | 4740      |\n",
      "|    policy_gradient_loss | -0.0148   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13327646 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.854    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 349      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.145189 |\n",
      "|    clip_fraction        | 0.664    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.6     |\n",
      "|    explained_variance   | 0.98     |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0343  |\n",
      "|    n_updates            | 4780     |\n",
      "|    policy_gradient_loss | -0.0168  |\n",
      "|    std                  | 0.124    |\n",
      "|    value_loss           | 0.00129  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13775818 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00256   |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13792983 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15040594 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 346       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1541157 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.6      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00627   |\n",
      "|    n_updates            | 4860      |\n",
      "|    policy_gradient_loss | -0.013    |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00125   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15392235 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0032     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15306637 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15503797 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15624878 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00663   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15457636 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00177    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12183952 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15348923 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0125     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00546   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15546685 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0103     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00788   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15216534 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.000174  |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15213437 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15375651 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0503     |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15579231 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0283     |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13714702 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.054     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13861221 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0681    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1435667 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.6      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0207   |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | -0.0175   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15564469 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0793     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.000636  |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15126094 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15886354 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0798     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00934   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16018602 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11679772 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15639569 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.0035    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15122248 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16093248 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0516    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00984   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13834706 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1529886 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.6      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.096     |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | -0.00577  |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00128   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1290603 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0775   |\n",
      "|    n_updates            | 5380      |\n",
      "|    policy_gradient_loss | -0.013    |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00131   |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1611251 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.035    |\n",
      "|    n_updates            | 5400      |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00131   |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1500019 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0073    |\n",
      "|    n_updates            | 5420      |\n",
      "|    policy_gradient_loss | -0.0109   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1498824 |\n",
      "|    clip_fraction        | 0.683     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0363    |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -0.0178   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15751976 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13620627 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13010702 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00504    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12749545 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14788704 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15402642 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.000403  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15648493 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00654   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12625147 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0333     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15379758 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | 0.000635   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1520441 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0541   |\n",
      "|    n_updates            | 5640      |\n",
      "|    policy_gradient_loss | -0.0115   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00124   |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1528594 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.014    |\n",
      "|    n_updates            | 5660      |\n",
      "|    policy_gradient_loss | -0.0105   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00113   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15158339 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00122   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15403096 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000723  |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00622   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15321195 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000463  |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15362208 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15003888 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00898   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15883772 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | 0.00131    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15969777 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0561     |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15431833 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0878    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.854    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 350      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.155241 |\n",
      "|    clip_fraction        | 0.666    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.8     |\n",
      "|    explained_variance   | 0.979    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.03    |\n",
      "|    n_updates            | 5840     |\n",
      "|    policy_gradient_loss | -0.00598 |\n",
      "|    std                  | 0.122    |\n",
      "|    value_loss           | 0.00134  |\n",
      "--------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15334427 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 150528\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='3a0d1165-d8cb-4c24-b2c1-163027a9fe19'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[50000, 50000, 50000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
