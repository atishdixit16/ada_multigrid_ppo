{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_multigrid_fixed'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfc05c1fd0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfc21d4630>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 447      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071611784 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.86        |\n",
      "|    explained_variance   | -0.64       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03454525 |\n",
      "|    clip_fraction        | 0.447      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00374    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.701      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03573232 |\n",
      "|    clip_fraction        | 0.458      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00124    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00301    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030601244 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00539    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.713       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040795945 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0882     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00229     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047549043 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045685787 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.729      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04374043 |\n",
      "|    clip_fraction        | 0.481      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0557    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05085764 |\n",
      "|    clip_fraction        | 0.485      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041601393 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.09        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042629927 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.11        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048662223 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06281271 |\n",
      "|    clip_fraction        | 0.494      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.2        |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052917052 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0925     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.736       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061812293 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.736      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05109284 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.3        |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048280817 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048708476 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.066      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052609287 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.4         |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 457         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054532938 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059489477 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00108     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 470       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0585882 |\n",
      "|    clip_fraction        | 0.552     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.44      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0294   |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.0482   |\n",
      "|    std                  | 0.178     |\n",
      "|    value_loss           | 0.00137   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05957352 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052519597 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058969725 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.61        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000243   |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.772      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06373503 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06501575 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.68       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051936734 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0719     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 458        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07438328 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.79       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06692326 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062172912 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06330013 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.98       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0391    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059190728 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06229939 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.08       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0794    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.784      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 452        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06865124 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05645574 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.789       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061112154 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.31        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0685     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07695669 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.36       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.792     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 450       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0773824 |\n",
      "|    clip_fraction        | 0.581     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.41      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0663   |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00117   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068699375 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.48        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07169143 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064957775 |\n",
      "|    clip_fraction        | 0.582       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.63        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.802      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07421793 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.72       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.805    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 475      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 5        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.066572 |\n",
      "|    clip_fraction        | 0.578    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.78     |\n",
      "|    explained_variance   | 0.979    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0416  |\n",
      "|    n_updates            | 880      |\n",
      "|    policy_gradient_loss | -0.0332  |\n",
      "|    std                  | 0.168    |\n",
      "|    value_loss           | 0.00111  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05657038 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061324097 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.87        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0639     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058894586 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.94        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.81      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 461       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0796905 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.01      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0314    |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0328   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.0011    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07091652 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07064302 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.23       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0015    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08055522 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.31       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0719471 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.4       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0498   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0322   |\n",
      "|    std                  | 0.164     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05825283 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0432    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 471       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0747148 |\n",
      "|    clip_fraction        | 0.6       |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.6       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0635   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.000956  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06105361 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.7        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080220796 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.84        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000874    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07097624 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0304    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000877   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079861596 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.07        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.000821    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05854218 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.17       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000831   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07405409 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062092163 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000828    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08321391 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.47       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0973    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.821     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0797853 |\n",
      "|    clip_fraction        | 0.597     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.54      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0424   |\n",
      "|    n_updates            | 1260      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    std                  | 0.156     |\n",
      "|    value_loss           | 0.00075   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 489        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06940863 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06982125 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07479205 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.77       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000728   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078808546 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.88        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000739    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09472382 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07766579 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07774629 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09312196 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084135614 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0731     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000654    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09430981 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06655766 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.18       |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07956515 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0185     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09485544 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0838    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079657935 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073997006 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000635    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08437343 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0393    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06543742 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082806155 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000566    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07732544 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545779 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0909723 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.017    |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.000665  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06803899 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09224341 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.084     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09557831 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0776     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08764621 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06973883 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0317    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09591163 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07945833 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000595   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 448        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08959212 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0307     |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000625   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 450        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08115378 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0843     |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09341939 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0452     |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085557364 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0782      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000725    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087084375 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0723     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000744    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08045194 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfc21d4630> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfc05b0390>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09031649 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00342    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15264371 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08027135 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09316577 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0836     |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07990596 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.009     |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00142    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09555108 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00649    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00151    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09412138 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09954493 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10799937 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0156     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09348722 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00568   |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093847595 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11670251 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10138961 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00686    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09295184 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09524922 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0672    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09482247 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09702243 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0806    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09778843 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0265    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08023657 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00324    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07968742 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08931113 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10789784 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090670824 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0704     |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10263586 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10151513 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0693    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09576894 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0626    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09877676 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10231356 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10146849 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09531021 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0388    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11176219 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.057     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09459754 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.039     |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102157906 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0627     |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108910665 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0728     |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10154281 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0698    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10261388 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00337    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096683726 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0462     |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09970151 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109385155 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 2720        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111328304 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0607      |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12010454 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114785984 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11395824 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0121    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 375       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1061248 |\n",
      "|    clip_fraction        | 0.633     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0707   |\n",
      "|    n_updates            | 2820      |\n",
      "|    policy_gradient_loss | -0.0182   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.00111   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111224726 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11608813 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10617395 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0645     |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11674879 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0306    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12523279 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09695885 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00671    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00099    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11501205 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000948   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11302753 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0425    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000976   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12726893 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00386    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10354056 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0625    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10441647 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0647    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 394       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1146765 |\n",
      "|    clip_fraction        | 0.637     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.5      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0672   |\n",
      "|    n_updates            | 3060      |\n",
      "|    policy_gradient_loss | -0.0173   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000973  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108279034 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00864     |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000863    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113998316 |\n",
      "|    clip_fraction        | 0.65        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00582     |\n",
      "|    n_updates            | 3100        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000859    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096076466 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0619     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000849    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12860474 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000828   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10348947 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000919   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107597426 |\n",
      "|    clip_fraction        | 0.65        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000813    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12565127 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 363        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11995103 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00757    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000804   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116100594 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 3240        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000787    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10047493 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10899235 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0569    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000828   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 380         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116959475 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000769    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10997667 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000768   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12182065 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0285     |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000796   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12538178 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.023      |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11236258 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12772503 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000769   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 375         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102483824 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11726795 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0715    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000751   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12558676 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0248    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12360243 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11133816 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15907247 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0818    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000797   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13383742 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0195     |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000793   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101454474 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000806    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14896229 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0482     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000783   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11388712 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0501    |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000761   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12526342 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15001471 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00834   |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13479874 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0693    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13021031 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15021273 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0363     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00618   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15281372 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13123322 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0473    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12122387 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000717   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107919976 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0775     |\n",
      "|    n_updates            | 3780        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000718    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16091679 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0926     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1198532 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0783    |\n",
      "|    n_updates            | 3820      |\n",
      "|    policy_gradient_loss | -0.0143   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000655  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12616368 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000698   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1306216 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00556   |\n",
      "|    n_updates            | 3860      |\n",
      "|    policy_gradient_loss | -0.0126   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000686  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13211091 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0184     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000669   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12871817 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0316    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.00703   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfc21d4518> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfac0969b0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11214262 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0221    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00062    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15190625 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00499   |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | 0.00119    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000906   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11814187 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11904931 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0706    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000887   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 218        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14414677 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0846    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000852   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13283738 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14418514 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0735    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000889   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11773815 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.13       |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000849   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13471368 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0373     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12581494 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000919   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13351683 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.066     |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000916   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12946227 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00835    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11934006 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00435   |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10254822 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.174      |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117128335 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.125       |\n",
      "|    n_updates            | 4200        |\n",
      "|    policy_gradient_loss | -0.014      |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000857    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 228       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1270005 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0286    |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000831  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15774678 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.034      |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000904   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12877779 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000825   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13055703 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000835   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15603109 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0413    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00818   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000827   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12342949 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000883   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11129769 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0475    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.00954   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14726502 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000801   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13151827 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000907   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 228       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1268824 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0348   |\n",
      "|    n_updates            | 4400      |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000904  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11164941 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000921   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15183799 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000857   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14353494 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000874   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12967713 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000827   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12319187 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0445    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14593908 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14247331 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0346    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14198577 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000912   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.855    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 230      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.123561 |\n",
      "|    clip_fraction        | 0.669    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.2     |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0218  |\n",
      "|    n_updates            | 4580     |\n",
      "|    policy_gradient_loss | -0.0142  |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000829 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12390731 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00079    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119230315 |\n",
      "|    clip_fraction        | 0.673       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0477      |\n",
      "|    n_updates            | 4620        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.000824    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 217       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1396167 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0747   |\n",
      "|    n_updates            | 4640      |\n",
      "|    policy_gradient_loss | -0.0161   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.00086   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13315396 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124547586 |\n",
      "|    clip_fraction        | 0.678       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0655     |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.000822    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16995111 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0577     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15129542 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0761     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000765   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13948998 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0789     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00084    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 221       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1421153 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0417   |\n",
      "|    n_updates            | 4760      |\n",
      "|    policy_gradient_loss | -0.0127   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000841  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13645346 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0816    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00092    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 225       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1436603 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0169   |\n",
      "|    n_updates            | 4800      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000894  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14843044 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000802   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13455796 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0212    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12538573 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0632    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000811   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16166265 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0338     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00398   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000835   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13924758 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0483    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15532355 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15345034 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15616077 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000832   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15141763 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | 0.00148    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 229       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1308533 |\n",
      "|    clip_fraction        | 0.678     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0506   |\n",
      "|    n_updates            | 5000      |\n",
      "|    policy_gradient_loss | -0.0109   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000842  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13792193 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0805    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00679   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000875   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13650218 |\n",
      "|    clip_fraction        | 0.694      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0317    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.00981   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15196347 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.0074    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000936   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 222      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.151505 |\n",
      "|    clip_fraction        | 0.676    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.6     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0669   |\n",
      "|    n_updates            | 5080     |\n",
      "|    policy_gradient_loss | -0.0115  |\n",
      "|    std                  | 0.118    |\n",
      "|    value_loss           | 0.000991 |\n",
      "--------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15046556 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.000175  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15041375 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00631    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | 0.00366    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000992   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15754955 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | 0.000455   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14415732 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12245812 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0536    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00878   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000984   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15429273 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0053    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14429377 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0433     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00897   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16007158 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0549    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000895   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15754895 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0517    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00286   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000869   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15040624 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0593    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00107   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00092    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 229       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1515896 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00749   |\n",
      "|    n_updates            | 5300      |\n",
      "|    policy_gradient_loss | 0.00866   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000954  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15005437 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0612    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00934   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15344107 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00162   |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.000717  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14555785 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0705    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15879214 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.021     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.000468  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15056717 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0835    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15143776 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0111     |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.00696   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 231       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1552498 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0286   |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -7.65e-05 |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000945  |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.858    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 225      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.160218 |\n",
      "|    clip_fraction        | 0.664    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0226   |\n",
      "|    n_updates            | 5460     |\n",
      "|    policy_gradient_loss | 0.0047   |\n",
      "|    std                  | 0.118    |\n",
      "|    value_loss           | 0.00106  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14987001 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00967   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14414176 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00267   |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16088477 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15400092 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | 0.0115     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 225       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1469607 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0203    |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.00109   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13963301 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0424    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15958962 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.108      |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.858    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 219      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.155345 |\n",
      "|    clip_fraction        | 0.674    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.982    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0221   |\n",
      "|    n_updates            | 5620     |\n",
      "|    policy_gradient_loss | -0.00479 |\n",
      "|    std                  | 0.117    |\n",
      "|    value_loss           | 0.00114  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1392388 |\n",
      "|    clip_fraction        | 0.684     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0371   |\n",
      "|    n_updates            | 5640      |\n",
      "|    policy_gradient_loss | -0.0137   |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15071973 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00375   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15001795 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00602   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15460396 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00994   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15274504 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00426    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00923   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15486197 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00405   |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15266275 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00626   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16129467 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0247     |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00762   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15214638 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0736    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | 0.00317    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15048438 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0737    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00901   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13091871 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15401001 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0645    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 150528\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfac096b00> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfc05b9908>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.671      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15899126 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0455    |\n",
      "|    n_updates            | 5880       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051376868 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.0716     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.701       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048658594 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 455         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033352904 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.044      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040718857 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0528     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.702      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 451        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03411284 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.071     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00229    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.711       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037490245 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.99        |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00201     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041925915 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.726       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043623336 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04546948 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.09       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0383     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030160943 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00162     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05025809 |\n",
      "|    clip_fraction        | 0.492      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.19       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00154    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.735       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054097105 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.22        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04217591 |\n",
      "|    clip_fraction        | 0.499      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.23       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0801    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05191766 |\n",
      "|    clip_fraction        | 0.498      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048750035 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.2         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.731     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 462       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0551486 |\n",
      "|    clip_fraction        | 0.51      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.24      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0647   |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.0444   |\n",
      "|    std                  | 0.18      |\n",
      "|    value_loss           | 0.0014    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05785197 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.29       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.725      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04822092 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.33       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04862503 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057611883 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06000798 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.49       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0761    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.741       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 451         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054572217 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.55        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06736399 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056733884 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.67        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05385695 |\n",
      "|    clip_fraction        | 0.535      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.744     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 462       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0693997 |\n",
      "|    clip_fraction        | 0.538     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.74      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.046    |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    std                  | 0.176     |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 454        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05284319 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.81       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04929874 |\n",
      "|    clip_fraction        | 0.553      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062127255 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066382244 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061261117 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.756     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 465       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0666964 |\n",
      "|    clip_fraction        | 0.57      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.97      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0621   |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0435   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.755     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 466       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0651351 |\n",
      "|    clip_fraction        | 0.567     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.01      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0809   |\n",
      "|    n_updates            | 660       |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 447         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061286975 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0698     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06732507 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.06       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056050707 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.1         |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0856     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00117     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06630887 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 453        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07023197 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.22       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0823    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05852921 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07357506 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.38       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07846562 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.44       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0383    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.773     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 460       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0608586 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.51      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067037866 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.55        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 450        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07450347 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.58       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076563425 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.62        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.785       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075655535 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.69        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06556271 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.79       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.791     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0922491 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.88      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 456        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07615379 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.98       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072590634 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081946425 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.1         |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07336445 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.2        |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 491        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08942877 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.808     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 470       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0684288 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.37      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0287   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0337   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098124005 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083682366 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.58        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000931    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06746669 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0417     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000966   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.819       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076223746 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.77        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000954    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08127213 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.89       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07096313 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9          |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078128785 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.11        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.000906    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07393008 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0154    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072332405 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.34        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000809    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07787631 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00189   |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07440407 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07948859 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07599194 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.69       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09329804 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.81       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 458        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07452371 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.91       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08580129 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063462935 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000764    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090517186 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000729    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062073715 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000666    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09799924 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078958824 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07313873 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06864514 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0566    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079242826 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000562    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087600395 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00061     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09132691 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0557     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08345769 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000602   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08290584 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08842277 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00493   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09605677 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000648   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08552261 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00556   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0883674 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.3      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.000851 |\n",
      "|    n_updates            | 1720      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000606  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10462477 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00396    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086456716 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000611    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09217997 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11325834 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00588    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.834       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078990184 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00487    |\n",
      "|    n_updates            | 1820        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00066     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08605505 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000544   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09499385 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08094972 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000552   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09100573 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00163    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000553   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.835     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 462       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1063107 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0178    |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000532  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09971459 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000523   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfc05a7e48> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfac06e748>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10190296 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.992      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0762    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.00856   |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000501   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14964512 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0582    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10465088 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0376    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00091    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07577591 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0574    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000925   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08533673 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000869   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10213103 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0275    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000881   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10846629 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0253    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000869   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09519581 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0674    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106239974 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0265     |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000848    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 398       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1010682 |\n",
      "|    clip_fraction        | 0.629     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.3      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00162  |\n",
      "|    n_updates            | 2140      |\n",
      "|    policy_gradient_loss | -0.0162   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.000782  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 400       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0809952 |\n",
      "|    clip_fraction        | 0.637     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0854   |\n",
      "|    n_updates            | 2160      |\n",
      "|    policy_gradient_loss | -0.0176   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00081   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10530305 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.041      |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 403         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094331846 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0881      |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000858    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095678404 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000705    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 405        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10500835 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000787   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 403         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102365956 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.077      |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000758    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10602144 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0212    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000784   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08829789 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000686   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08708376 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089688204 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000763    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11058035 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00076    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09746095 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000786   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10454895 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0272    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000754   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10017347 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07224054 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.026      |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 389       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1090858 |\n",
      "|    clip_fraction        | 0.646     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0379   |\n",
      "|    n_updates            | 2460      |\n",
      "|    policy_gradient_loss | -0.016    |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000797  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10541105 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0246     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000695   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 401         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104246594 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.00995    |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.00074     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 404        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10063972 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081107184 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0287      |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000736    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09999621 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00078    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 395         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100400046 |\n",
      "|    clip_fraction        | 0.653       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.054       |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.00079     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10320632 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000762   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090271994 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 2620        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000771    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093370005 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.4        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    std                  | 0.13        |\n",
      "|    value_loss           | 0.000748    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.849     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 390       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1043375 |\n",
      "|    clip_fraction        | 0.634     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0353   |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | -0.0145   |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000786  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10589743 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000745   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11506724 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0198    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000717   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10247202 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0311    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000767   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11249642 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -7.95e-06  |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113078274 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0545     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000686    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114917256 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000741    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09485103 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0128     |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000657   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10944716 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0414    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00066    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10758009 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0584     |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11561253 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0501     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09721558 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00376   |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000677   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08961997 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0569     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00068    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09846366 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0729    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00059    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101870276 |\n",
      "|    clip_fraction        | 0.636       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000722   |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000607    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10612144 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000683   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10802164 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.072     |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13163665 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0639     |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000697   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09429176 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0518    |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000674   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 383       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1012331 |\n",
      "|    clip_fraction        | 0.643     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00386  |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | -0.0149   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000715  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093551025 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.8        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0636      |\n",
      "|    n_updates            | 3060        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000732    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10156916 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0772     |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10921197 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0445     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13611336 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0405    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10475625 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0885    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000642   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09107836 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000658   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11334691 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0336    |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000675   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12721309 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0438    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00068    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11725642 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12448232 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10408113 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000736   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11864237 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0304     |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10871384 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00076    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11605495 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13130346 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112662174 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0443     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000663    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 374       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1267657 |\n",
      "|    clip_fraction        | 0.651     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.058    |\n",
      "|    n_updates            | 3380      |\n",
      "|    policy_gradient_loss | -0.0106   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000681  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12810943 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13443069 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0741     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00811   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000724   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09263601 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0131     |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000711   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12603469 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00167    |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12319516 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124639705 |\n",
      "|    clip_fraction        | 0.654       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.039       |\n",
      "|    n_updates            | 3500        |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.00074     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115335084 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 3520        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 375        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13674538 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00528   |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 378       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1292787 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0248   |\n",
      "|    n_updates            | 3560      |\n",
      "|    policy_gradient_loss | -0.0119   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000766  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 375       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1249565 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0129    |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | -0.00852  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000767  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12523952 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13106212 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0442    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1308392 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0168   |\n",
      "|    n_updates            | 3640      |\n",
      "|    policy_gradient_loss | -0.0123   |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000782  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120918944 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.00773    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000694    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13019006 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11955078 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000743   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13652548 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.142      |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10949371 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0075    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12957999 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00185    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11906145 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0938     |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11141586 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0365    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000687   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12030192 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0696     |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000725   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12221543 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00067    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 356         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.109785035 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0442     |\n",
      "|    n_updates            | 3860        |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000675    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107817195 |\n",
      "|    clip_fraction        | 0.659       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00387    |\n",
      "|    n_updates            | 3880        |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000614    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15092055 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00489    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfac093908> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcf8c191358>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110686935 |\n",
      "|    clip_fraction        | 0.669       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0269      |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.00977    |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000681    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16723025 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0943     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.00113   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116417214 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000968    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15177064 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00513   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 215       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1234679 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0217    |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | -0.0145   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000977  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13531148 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0314     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12854648 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.104      |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.00972   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11900753 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0383     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000909   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10151799 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13322578 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.056     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000959   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11422093 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000808   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12830243 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0301    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000972   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13208023 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15469961 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.00865   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13077843 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13796075 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00827   |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13708356 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0217     |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15822741 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00942   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115541816 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 4280        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13257603 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.014      |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1262997 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 4320      |\n",
      "|    policy_gradient_loss | -0.0147   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000972  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 218        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13739021 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00275   |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000972   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13502654 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12153679 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00791   |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12864698 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0724    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11219571 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0349     |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0169    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11680534 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14499928 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13407403 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0505    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14120264 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11610551 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0717    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13288279 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11814336 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0711    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13773894 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11974044 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0297     |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11944504 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00247    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000942   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12778275 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0695    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11506214 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0535     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00989   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000889   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13152361 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0219     |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00893   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000921   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17217124 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0674     |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00094    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1392103 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0451   |\n",
      "|    n_updates            | 4720      |\n",
      "|    policy_gradient_loss | -0.0163   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000982  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12537597 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14768483 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13687173 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0672    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000949   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117753804 |\n",
      "|    clip_fraction        | 0.664       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.12        |\n",
      "|    value_loss           | 0.000974    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11224131 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.013      |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00907   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14232469 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000963   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15343364 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0585    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00914   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000942   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15117581 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.033      |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13697866 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0803    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00997   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000903   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 217        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12807353 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1316826 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0117   |\n",
      "|    n_updates            | 4940      |\n",
      "|    policy_gradient_loss | -0.0114   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000962  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13449991 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.859    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 224      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.132533 |\n",
      "|    clip_fraction        | 0.661    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.2     |\n",
      "|    explained_variance   | 0.985    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00862  |\n",
      "|    n_updates            | 4980     |\n",
      "|    policy_gradient_loss | -0.0128  |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000959 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14330289 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00904   |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 218        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12140007 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.041     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14502235 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00939   |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000943   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 218       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1590371 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0288   |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -0.0192   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.00114   |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15456924 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00532   |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | 0.000645   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13840069 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0481     |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15509793 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0591     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00369   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14070661 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13522033 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00354   |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000935   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12839973 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0484     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000965   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14187829 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0538    |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00097    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15001488 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0565    |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000998   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12515225 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 215       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1330924 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0729   |\n",
      "|    n_updates            | 5260      |\n",
      "|    policy_gradient_loss | -0.0116   |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 219        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15446883 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00629   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000936   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14212234 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0456     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15276201 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13414259 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 215       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1604127 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0455   |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | -0.00923  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.00109   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12372732 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 209       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1514751 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.033    |\n",
      "|    n_updates            | 5400      |\n",
      "|    policy_gradient_loss | -0.0104   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 219       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1504586 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0277    |\n",
      "|    n_updates            | 5420      |\n",
      "|    policy_gradient_loss | -0.0121   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12880155 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0818    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15063325 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15197095 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0422     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00747   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15089166 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0758    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.000338  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 205        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13025698 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.214      |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000949   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 211        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14926587 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0634    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000949   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 211       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1461315 |\n",
      "|    clip_fraction        | 0.689     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0154    |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | -0.014    |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15230113 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0237     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00229   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 212        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13838604 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.011     |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000993   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15102759 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00551   |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.0078    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000998   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14832994 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0108     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 200        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15267077 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | 0.00907    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15194206 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.006     |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.008     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 207       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1598432 |\n",
      "|    clip_fraction        | 0.68      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0763    |\n",
      "|    n_updates            | 5700      |\n",
      "|    policy_gradient_loss | -0.000248 |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.00101   |\n",
      "---------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 208        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15266211 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00472    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00253    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000963   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 202        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15363006 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0973     |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00872   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000976   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15333012 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00336   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 207        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15133443 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.029      |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00962   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 206        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15648107 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00458   |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.0034    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000974   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 199        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12776385 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00904   |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 192        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15694752 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00894   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15435526 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0663    |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.00779   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 150528\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='f463dba9-8fb6-4598-ba3d-5e22e9b23fd9'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfac096438> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcf8c1aaeb8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14545372 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0136     |\n",
      "|    n_updates            | 5880       |\n",
      "|    policy_gradient_loss | -0.00963   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000978   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.674      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06958654 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.89       |\n",
      "|    explained_variance   | -0.376     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0125     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 457         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047490202 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0861     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.711      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03339555 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.94       |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00295    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044822954 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037756953 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.718       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040763944 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043451317 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00177     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.719       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041048933 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048675895 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.727      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04694677 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046830624 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.734     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0459747 |\n",
      "|    clip_fraction        | 0.503     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.98      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0434   |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00152   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036458228 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04491026 |\n",
      "|    clip_fraction        | 0.519      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.11       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046822768 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0657     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.741      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04647106 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056340653 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0644     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053496473 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060202397 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0763     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05329554 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.35       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045318723 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0645     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06090635 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.44       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056513917 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.5         |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.762     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 483       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0578928 |\n",
      "|    clip_fraction        | 0.548     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.57      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.177     |\n",
      "|    value_loss           | 0.0013    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05858321 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056070495 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.65        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055896807 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0704     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06903891 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.74       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 463         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057420634 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.77        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05457897 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.83       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054109592 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058915943 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.02        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00935     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06029724 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.07       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07435803 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.17       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06099558 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05521869 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0398    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.783     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 465       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0634351 |\n",
      "|    clip_fraction        | 0.578     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.36      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0338   |\n",
      "|    n_updates            | 740       |\n",
      "|    policy_gradient_loss | -0.0407   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07324021 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.46       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.787       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 448         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770976 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.6         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06914873 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.64       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 449        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06435315 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.68       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072419755 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0428     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06710909 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.85       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068342045 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.91        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07601108 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.05       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 456        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06550066 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0342     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093846254 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0915     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 466         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070667684 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08304366 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.48       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.799     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0701161 |\n",
      "|    clip_fraction        | 0.599     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.59      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0212   |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.0361   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07460266 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.65       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 491       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0819386 |\n",
      "|    clip_fraction        | 0.595     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.68      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.161     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07067679 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.73       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.804       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077644095 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.81        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.805       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086551845 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.89        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000756   |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08583589 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0376    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07476324 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.01       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00233    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09169489 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.08       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07583239 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08412103 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.29       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081914276 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0575     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09545218 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.45       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08993331 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07361041 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08226754 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0878    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075846694 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000996    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10608163 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.97       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00673   |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08148558 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093813166 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000882    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09656456 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770022 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0517     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000919    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 478         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089062504 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000908    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07812403 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0799     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000899   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07990335 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000896   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 485        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08633542 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09878737 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0727    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08626648 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08954557 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00716   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09657636 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08019618 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0501    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07889671 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00167    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08969821 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00949   |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09659344 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0113     |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08663081 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10572666 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.024     |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097353496 |\n",
      "|    clip_fraction        | 0.605       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000679    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081000626 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000712    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.823      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10313401 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0191     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10554282 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 456        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07711329 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08266686 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0487    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000715   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07703554 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0502    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09163483 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08696411 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.825     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 458       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1004765 |\n",
      "|    clip_fraction        | 0.632     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.012    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0177   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.000621  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095734775 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0521     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000596    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09113147 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcf8c1aaeb8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcfac093c50>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08722533 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13660038 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0522    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09013145 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.048      |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10877577 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0806    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10563181 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09658824 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00551   |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.839     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 382       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1026001 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.3      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0656   |\n",
      "|    n_updates            | 2080      |\n",
      "|    policy_gradient_loss | -0.0218   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10498543 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00618   |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 381         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106690384 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13635202 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0389     |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09719013 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00898   |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000985   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.841     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0981983 |\n",
      "|    clip_fraction        | 0.641     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.5      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0313    |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0246   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.000989  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09398838 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09769764 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0803    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12572101 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107915446 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 2260        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11858334 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0157    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10165963 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0244    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 359       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1154719 |\n",
      "|    clip_fraction        | 0.639     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0436    |\n",
      "|    n_updates            | 2320      |\n",
      "|    policy_gradient_loss | -0.0189   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.000986  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 395       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0978028 |\n",
      "|    clip_fraction        | 0.624     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 2.79e-06  |\n",
      "|    n_updates            | 2340      |\n",
      "|    policy_gradient_loss | -0.018    |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.843     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 378       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1119915 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00905  |\n",
      "|    n_updates            | 2360      |\n",
      "|    policy_gradient_loss | -0.0174   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.000983  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11514078 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0209    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10381689 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 392        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12557283 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10975508 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0701    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08674215 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0738    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10678872 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0414     |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10365398 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11470918 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0252    |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11297648 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000918  |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000978   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10213725 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10865815 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.072      |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10255019 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0475    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000957   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10819974 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0707    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.845    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 381      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.119711 |\n",
      "|    clip_fraction        | 0.644    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.5     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.00897  |\n",
      "|    n_updates            | 2640     |\n",
      "|    policy_gradient_loss | -0.0184  |\n",
      "|    std                  | 0.13     |\n",
      "|    value_loss           | 0.00095  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11606549 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00718   |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10284786 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12531434 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0175    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11173954 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113908246 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0681     |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000905    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105483726 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0511     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000947    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 362        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08401755 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0154     |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13152468 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000963   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 372        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10636363 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000902   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13159722 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000895   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11290089 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00284    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000882   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11493079 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0415    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110112034 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000898    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13207798 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0083    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000886   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115605794 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.018       |\n",
      "|    n_updates            | 2940        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000905    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12547612 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0489    |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000939   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11482283 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000987   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 382         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113231204 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 3000        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000955    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122121215 |\n",
      "|    clip_fraction        | 0.653       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000862    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094405785 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0121     |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000868    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 376        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11115736 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00407   |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11461739 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0425    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000874   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 390        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11462585 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0455     |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000839   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10819533 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0726    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11501472 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0304     |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000853   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12282236 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000807   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11213283 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0594     |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000834   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10510655 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.017     |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12227635 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000792   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11350089 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00071    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11597709 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0739     |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000706   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114800036 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.124       |\n",
      "|    value_loss           | 0.000695    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10643534 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000703   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 376       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1262879 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.5      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00586   |\n",
      "|    n_updates            | 3320      |\n",
      "|    policy_gradient_loss | -0.0199   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00074   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 384        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11625998 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0602    |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000766   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10542722 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000725   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14548731 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000872   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10138321 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0703    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119324945 |\n",
      "|    clip_fraction        | 0.666       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.064      |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000799    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13926896 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.105      |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000808   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15208891 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0549     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000753   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.119263604 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0438     |\n",
      "|    n_updates            | 3480        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000719    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.847     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 394       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1361979 |\n",
      "|    clip_fraction        | 0.657     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0473   |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -0.013    |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000759  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11988511 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0899    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000792   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12180952 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00079    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12020467 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13988538 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000788   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15133615 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0296     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124188304 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.8        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 3620        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.00082     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13043928 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15442625 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0567    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000791   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.848    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 368      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 6        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.134027 |\n",
      "|    clip_fraction        | 0.66     |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15       |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0129  |\n",
      "|    n_updates            | 3680     |\n",
      "|    policy_gradient_loss | -0.016   |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000797 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12287513 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0428     |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000833   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13741332 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 379       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1568524 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.125     |\n",
      "|    n_updates            | 3740      |\n",
      "|    policy_gradient_loss | -0.00933  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000779  |\n",
      "---------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15521313 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0136    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15053186 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0134    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 378        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14211218 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0507     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.0008     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12387222 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0721    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000795   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1174107 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0238   |\n",
      "|    n_updates            | 3840      |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000789  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14077055 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0659    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12373278 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0505    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 389        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15527721 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0203     |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000711   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7fcfac093c50> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fcf8c0d5630>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 218       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1401315 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0312    |\n",
      "|    n_updates            | 3920      |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000738  |\n",
      "---------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 229       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1561644 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.181     |\n",
      "|    n_updates            | 3940      |\n",
      "|    policy_gradient_loss | 0.00934   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12267108 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15138787 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0047    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.00711   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 219       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1407114 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0218    |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | -0.0168   |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.00113   |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15907563 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 222       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1510134 |\n",
      "|    clip_fraction        | 0.653     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00381   |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | -0.000872 |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.000997  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13249001 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0537     |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000982   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 228       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1479481 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.7      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0539   |\n",
      "|    n_updates            | 4080      |\n",
      "|    policy_gradient_loss | -0.015    |\n",
      "|    std                  | 0.117     |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13155715 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0649    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15751211 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13397291 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0105    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124039195 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.8        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0642     |\n",
      "|    n_updates            | 4160        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.117       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1341218 |\n",
      "|    clip_fraction        | 0.683     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00303  |\n",
      "|    n_updates            | 4180      |\n",
      "|    policy_gradient_loss | -0.014    |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13982998 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15657276 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0611    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13630143 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00901   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13617161 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12419258 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14545849 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0233     |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13588509 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0658     |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00848   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 222       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1280182 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.038    |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | -0.013    |\n",
      "|    std                  | 0.116     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15048411 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0778    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.0166    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13129595 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0505     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11477604 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0314    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11756915 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15245377 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0488    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00932   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15353844 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0752    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0098    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15117314 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15267007 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0838    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.00479   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15684263 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0746    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00777   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 225         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116908476 |\n",
      "|    clip_fraction        | 0.674       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16          |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0543      |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    std                  | 0.115       |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13571846 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0881    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14357252 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0639    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13834547 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0293    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15436938 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | 0.0033     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15977602 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0436    |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16020194 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0967     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00241   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15191773 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00576   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15743384 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0529    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00132   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15096202 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0688     |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14716983 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0244     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16533081 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0423    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15412262 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0249     |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15975454 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0321    |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00675   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15618873 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00696   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14403602 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00712    |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15956578 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00185    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00132    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12973897 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.061     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15345193 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00575    |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14331138 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14311998 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0598    |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15498415 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.0091    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 230       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1503696 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0132    |\n",
      "|    n_updates            | 4980      |\n",
      "|    policy_gradient_loss | -0.00374  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15614083 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0872    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15214686 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0058     |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | -0.00853   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15254687 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00377   |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.00012   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1526427 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.1      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0777    |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -0.00734  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.00107   |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15513137 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0219    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00645   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 224       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1560069 |\n",
      "|    clip_fraction        | 0.681     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00852   |\n",
      "|    n_updates            | 5100      |\n",
      "|    policy_gradient_loss | -0.00886  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 223       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1490519 |\n",
      "|    clip_fraction        | 0.681     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0418    |\n",
      "|    n_updates            | 5120      |\n",
      "|    policy_gradient_loss | -0.00932  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.00117   |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15255716 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00323   |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00987   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14774522 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00975   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118559875 |\n",
      "|    clip_fraction        | 0.684       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 16.3        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0534     |\n",
      "|    n_updates            | 5180        |\n",
      "|    policy_gradient_loss | -0.00685    |\n",
      "|    std                  | 0.114       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 218       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1529445 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0157    |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | -0.000344 |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13897702 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00218   |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 224       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1522611 |\n",
      "|    clip_fraction        | 0.665     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.3      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00313  |\n",
      "|    n_updates            | 5240      |\n",
      "|    policy_gradient_loss | -0.000652 |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.0011    |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15182011 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00248   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16336624 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | 0.000295   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14202869 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0201     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13709538 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0731    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00711   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000912   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15331781 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00927    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00544   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00092    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14492278 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0841    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.0071    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00088    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15086776 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0826    |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | 0.00257    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15732071 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0543    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | 0.00647    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 221       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1523702 |\n",
      "|    clip_fraction        | 0.666     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16.2      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0768    |\n",
      "|    n_updates            | 5420      |\n",
      "|    policy_gradient_loss | -0.00254  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15783808 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00271   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000959   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15318337 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | 0.00398    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000932   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14765222 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.042     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000922   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15012237 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15121508 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0425     |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00728   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000958   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15240867 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0282    |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | 0.00545    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15011111 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0553     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | 0.00799    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.858    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 222      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 11       |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.152863 |\n",
      "|    clip_fraction        | 0.671    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 16       |\n",
      "|    explained_variance   | 0.983    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0536  |\n",
      "|    n_updates            | 5580     |\n",
      "|    policy_gradient_loss | -0.00491 |\n",
      "|    std                  | 0.115    |\n",
      "|    value_loss           | 0.00105  |\n",
      "--------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15869811 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000947   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15207048 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0316    |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | 0.00493    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15972492 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.012      |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00099   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000967   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16021672 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0232    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | 0.00193    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15981121 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.115      |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00821   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15304808 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0776    |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | 0.00912    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15266256 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.048     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00789    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15633863 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.022      |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00727   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000942   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15456876 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0765    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14351562 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0411    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00682   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000948   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15726264 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.00187   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00092    |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 221        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15050021 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0228     |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.0024    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15255669 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.019     |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.000898  |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000909   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15222861 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000931   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 150528\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div id='2b1a7184-74b4-41f3-9d8f-a0e12e2a8f01'></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[50000, 50000, 50000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
