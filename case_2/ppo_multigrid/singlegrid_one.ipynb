{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_singlegrid_one'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/coarse_grid_functions.py:51: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mFirst-class function type feature is experimental\u001b[0m\u001b[0m\n",
      "  for j in range(len(p_1)-1):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f841c50d630> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f841c504c18>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.712    |\n",
      "| time/              |          |\n",
      "|    fps             | 118      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 54 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.693      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07591894 |\n",
      "|    clip_fraction        | 0.452      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.86       |\n",
      "|    explained_variance   | -0.695     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0446     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0162     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.711       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043508492 |\n",
      "|    clip_fraction        | 0.458       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.85        |\n",
      "|    explained_variance   | 0.86        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0425      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00478     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.724      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03461929 |\n",
      "|    clip_fraction        | 0.458      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00489   |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0429    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00407    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.729      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03971128 |\n",
      "|    clip_fraction        | 0.477      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.92       |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0669    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00354    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.717      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03715331 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.91       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000871  |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00327    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042050738 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0895     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0031      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.729      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04244227 |\n",
      "|    clip_fraction        | 0.486      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.94       |\n",
      "|    explained_variance   | 0.926      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00303    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055143226 |\n",
      "|    clip_fraction        | 0.478       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0485     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00291     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050451618 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.934       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0394     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00275     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.738      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05339067 |\n",
      "|    clip_fraction        | 0.488      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00281    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049152415 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.01        |\n",
      "|    explained_variance   | 0.938       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0318     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00257     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.72       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04080268 |\n",
      "|    clip_fraction        | 0.496      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.08       |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00279    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.736      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04675705 |\n",
      "|    clip_fraction        | 0.497      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.13       |\n",
      "|    explained_variance   | 0.937      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0451    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.0026     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050816465 |\n",
      "|    clip_fraction        | 0.512       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.16        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0721     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00251     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.739       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045789957 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.2         |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0906     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00239     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 356         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048387975 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.19        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0531     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00249     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04893533 |\n",
      "|    clip_fraction        | 0.518      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.946      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0673    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0457    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00234    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050010838 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00246     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.748       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044042837 |\n",
      "|    clip_fraction        | 0.529       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.23        |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00842    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00244     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052312605 |\n",
      "|    clip_fraction        | 0.532       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00247     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051159132 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.29        |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0941     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00239     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059032865 |\n",
      "|    clip_fraction        | 0.54        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.3         |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00238     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058802057 |\n",
      "|    clip_fraction        | 0.528       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0435     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0456     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00245     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.757       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059138935 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00685     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00225     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050450385 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.41        |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0624     |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00226     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.763       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062044125 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.43        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00229     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.765       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061131757 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.44        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0733     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00221     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.769      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06601739 |\n",
      "|    clip_fraction        | 0.543      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.51       |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0778    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0453    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00228    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.773       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052495886 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.51        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00221     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.772      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06738924 |\n",
      "|    clip_fraction        | 0.557      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.49       |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0842    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00223    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.772     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 353       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0677283 |\n",
      "|    clip_fraction        | 0.547     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.52      |\n",
      "|    explained_variance   | 0.949     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0235   |\n",
      "|    n_updates            | 620       |\n",
      "|    policy_gradient_loss | -0.0436   |\n",
      "|    std                  | 0.177     |\n",
      "|    value_loss           | 0.00232   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06631874 |\n",
      "|    clip_fraction        | 0.56       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.57       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00209    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053066384 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0273     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0437     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07132383 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0719    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00221    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.778      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06342655 |\n",
      "|    clip_fraction        | 0.552      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.68       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00209    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.783      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06825962 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.74       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0652    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00223    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06417152 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.77       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000135   |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00199    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05606574 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0757    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00217    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06424752 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.9        |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0319    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00217    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.774       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083348885 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.95        |\n",
      "|    explained_variance   | 0.951       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00232     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06875864 |\n",
      "|    clip_fraction        | 0.566      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.99       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0148    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00204    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076585546 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.06        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0411     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00225     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065188326 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.12        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0512     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0396     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.0021      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07109944 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.17       |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0262    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00213    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07376496 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.24       |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0957    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00211    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.792       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067728594 |\n",
      "|    clip_fraction        | 0.572       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.25        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0509     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.172       |\n",
      "|    value_loss           | 0.00206     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08169958 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.3        |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0392    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00215    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08204214 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.35       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0521    |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0385    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00206    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.793       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071524516 |\n",
      "|    clip_fraction        | 0.581       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.41        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0578     |\n",
      "|    n_updates            | 980         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00211     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.793     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 354       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0788313 |\n",
      "|    clip_fraction        | 0.569     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.49      |\n",
      "|    explained_variance   | 0.956     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0834   |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.0341   |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00218   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.794      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08285993 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.56       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.0021     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07668886 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.6        |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00222    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09060184 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.66       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0361    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00218    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08221191 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.71       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0366    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00218    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073996685 |\n",
      "|    clip_fraction        | 0.579       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0597     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00203     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07766193 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.85       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0336    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00218    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08456362 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.93       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0867    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00209    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09147726 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.01       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00201    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07026043 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.04       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00186    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07947793 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.089     |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00204    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07144249 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0656    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0351    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00192    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09282883 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.24       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00189    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08389802 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.31       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0502    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0343    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00195    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071854755 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0175     |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00196     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077472925 |\n",
      "|    clip_fraction        | 0.59        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.49        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0638     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08761371 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.58       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00175    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08535735 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.63       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00256   |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0317    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00182    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077306986 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.69        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00179     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08964409 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.79       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0441    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.035     |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.0018     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09166606 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.85       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0745    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00192    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09093394 |\n",
      "|    clip_fraction        | 0.59       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.89       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00185    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07608007 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.87       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.0019     |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08389551 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.91       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00188    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06701435 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.98       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00885   |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0336    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00176    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09224546 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.03       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0326    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09018172 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.1        |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.072     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08532579 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.15       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0377    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00174    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085424006 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.19        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0383     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0314     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10572994 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00173    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07817481 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.35       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07148434 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.35       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08180907 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.38       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00156    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07082423 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.41       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00358   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.823     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 353       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1069165 |\n",
      "|    clip_fraction        | 0.616     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.46      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0375   |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.0307   |\n",
      "|    std                  | 0.155     |\n",
      "|    value_loss           | 0.00151   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07922618 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.53       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0414    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07852005 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.61       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.13       |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0261    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09190252 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.68       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08761953 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.72       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0593    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08291142 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.76       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00141    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077194795 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.79        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.069       |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0248     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.00138     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0774046 |\n",
      "|    clip_fraction        | 0.619     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.81      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0893   |\n",
      "|    n_updates            | 1820      |\n",
      "|    policy_gradient_loss | -0.0269   |\n",
      "|    std                  | 0.153     |\n",
      "|    value_loss           | 0.0014    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08534046 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.92       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07843461 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.99       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09703143 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.026      |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00144    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.835       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090832435 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0288      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06496908 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0183     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081381276 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0901     |\n",
      "|    n_updates            | 1940        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07645217 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0059    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0241    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08092503 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0638     |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08427192 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00136    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07778015 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0431    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07101218 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00637   |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08595468 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086646356 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.6        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0134      |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07699802 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085655436 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000771   |\n",
      "|    n_updates            | 2120        |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07070238 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.089     |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0232    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087414585 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0555     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00117     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.842     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 352       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1014336 |\n",
      "|    clip_fraction        | 0.614     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.9      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0242   |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0213   |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.00124   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09048705 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0111     |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.842      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08864813 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0179    |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11594389 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0081     |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08685829 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00321    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0205    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08173389 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10160959 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08998717 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0636    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08824028 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0427    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12140832 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0605    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09359322 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12004472 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0232    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090858996 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.072      |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0227     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1019427 |\n",
      "|    clip_fraction        | 0.646     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.7      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.024    |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -0.0205   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.00124   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10640842 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0234    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09557442 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 2480       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105387494 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.066      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.025      |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1037467 |\n",
      "|    clip_fraction        | 0.626     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.044    |\n",
      "|    n_updates            | 2520      |\n",
      "|    policy_gradient_loss | -0.0207   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08841453 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09276129 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0753    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08404767 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09020039 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10002065 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084083304 |\n",
      "|    clip_fraction        | 0.642       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.068      |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09839443 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10230453 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00292   |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10457053 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00766   |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08686514 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0241    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08795446 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00105    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08763125 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098574445 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0234     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088256635 |\n",
      "|    clip_fraction        | 0.66        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 2800        |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11563166 |\n",
      "|    clip_fraction        | 0.642      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101696946 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0582     |\n",
      "|    n_updates            | 2840        |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00118     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091151334 |\n",
      "|    clip_fraction        | 0.646       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 2860        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106779434 |\n",
      "|    clip_fraction        | 0.648       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0653     |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 360         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112970926 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 2900        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11698022 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00475   |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0195    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10868408 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09668608 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00286   |\n",
      "|    n_updates            | 2960       |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115639135 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0446      |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11425626 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0679    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 354       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1040131 |\n",
      "|    clip_fraction        | 0.649     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0258   |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | -0.0188   |\n",
      "|    std                  | 0.133     |\n",
      "|    value_loss           | 0.00115   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11651558 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11615505 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103735425 |\n",
      "|    clip_fraction        | 0.658       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 3080        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 353       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1157767 |\n",
      "|    clip_fraction        | 0.662     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13        |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0917   |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.0245   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.00109   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 350         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113527134 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0838     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10930643 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0292    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.85      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1292437 |\n",
      "|    clip_fraction        | 0.658     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0133   |\n",
      "|    n_updates            | 3160      |\n",
      "|    policy_gradient_loss | -0.0206   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.0011    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11619274 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10704748 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.85     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 352      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.116199 |\n",
      "|    clip_fraction        | 0.662    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.2     |\n",
      "|    explained_variance   | 0.983    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0276  |\n",
      "|    n_updates            | 3220     |\n",
      "|    policy_gradient_loss | -0.0211  |\n",
      "|    std                  | 0.131    |\n",
      "|    value_loss           | 0.00106  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12434069 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11891009 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12551261 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 3280       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12067664 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000433  |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12508711 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.852     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1026608 |\n",
      "|    clip_fraction        | 0.66      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0544   |\n",
      "|    n_updates            | 3340      |\n",
      "|    policy_gradient_loss | -0.0198   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.00111   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11225841 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000361  |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11520903 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12021339 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0614    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112944804 |\n",
      "|    clip_fraction        | 0.655       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00302    |\n",
      "|    n_updates            | 3420        |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13722248 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114513196 |\n",
      "|    clip_fraction        | 0.673       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0555     |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0244     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13283114 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13305339 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12111814 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0638    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000969   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14176074 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0882    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12129457 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0633     |\n",
      "|    n_updates            | 3560       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14299265 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12257172 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0113     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11779318 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0537    |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11954782 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11733385 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0068    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12982996 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12113035 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00446   |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14148308 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0886    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11658363 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0339     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12544128 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0595    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1287295 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.4      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0151   |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | -0.0142   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.00101   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14183363 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0688     |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000876   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13013306 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00727   |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116511345 |\n",
      "|    clip_fraction        | 0.667       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.5        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00985     |\n",
      "|    n_updates            | 3840        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000908    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 18 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16941532 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0521    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000912   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13518994 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12678851 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0633    |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13591102 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00214    |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000956   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10831863 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0805     |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000956   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118826166 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0404     |\n",
      "|    n_updates            | 3960        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12858844 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15024534 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13008203 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0289     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14198084 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13429466 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00732    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12563682 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0212     |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000874   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11428791 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0557     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12589431 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.027     |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00097    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13506682 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0327    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000964   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14057693 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0101    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11688831 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14175951 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0411    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000982   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12479274 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0472    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000994   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13689978 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.0091    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15863714 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00471    |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.00708   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14309958 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0061     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15083733 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00969    |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00421   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15084422 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00179   |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00583   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14420123 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0736    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15806577 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0333    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14425589 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0169    |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13667437 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00498    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.854    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 347      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.153318 |\n",
      "|    clip_fraction        | 0.664    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.2     |\n",
      "|    explained_variance   | 0.984    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.066   |\n",
      "|    n_updates            | 4420     |\n",
      "|    policy_gradient_loss | -0.00186 |\n",
      "|    std                  | 0.119    |\n",
      "|    value_loss           | 0.000945 |\n",
      "--------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15108371 |\n",
      "|    clip_fraction        | 0.682      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00896   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14205666 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0698    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15559912 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0104    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00169   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13247211 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0647    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000938   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13941512 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0585    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15203024 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00551    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15505134 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15485659 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00213    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00509   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.00092    |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1587943 |\n",
      "|    clip_fraction        | 0.684     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0528    |\n",
      "|    n_updates            | 4600      |\n",
      "|    policy_gradient_loss | -0.00431  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000942  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12815256 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0125    |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15886241 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0178     |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00175   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000925   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13661158 |\n",
      "|    clip_fraction        | 0.698      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0315     |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000931   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1568355 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00361   |\n",
      "|    n_updates            | 4680      |\n",
      "|    policy_gradient_loss | -0.00102  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000931  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15038104 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | 0.0033     |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15606831 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0879    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00613   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15236118 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0121    |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00541   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 346       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1543819 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0336    |\n",
      "|    n_updates            | 4760      |\n",
      "|    policy_gradient_loss | -0.00857  |\n",
      "|    std                  | 0.118     |\n",
      "|    value_loss           | 0.000968  |\n",
      "---------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15067753 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0521    |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.00556   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15172105 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.00669   |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15366077 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.000545  |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000876   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14284089 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.042      |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.118      |\n",
      "|    value_loss           | 0.000856   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117866255 |\n",
      "|    clip_fraction        | 0.683       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.4        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 4860        |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    std                  | 0.117       |\n",
      "|    value_loss           | 0.000956    |\n",
      "-----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15219116 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0716     |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00272   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15839548 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00478   |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00473   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000997   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15199058 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00742   |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0085    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000875   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12677649 |\n",
      "|    clip_fraction        | 0.69       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0164     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00973   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000961   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15800758 |\n",
      "|    clip_fraction        | 0.687      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000961   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13369918 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00894   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000858   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15089294 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0866     |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.00968   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000857   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 20 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 356        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16835752 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00216    |\n",
      "|    n_updates            | 5020       |\n",
      "|    policy_gradient_loss | 0.00797    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000862   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13109739 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000866   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15744035 |\n",
      "|    clip_fraction        | 0.691      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00743   |\n",
      "|    std                  | 0.117      |\n",
      "|    value_loss           | 0.000921   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 354        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15480088 |\n",
      "|    clip_fraction        | 0.695      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00956    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00692   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.856    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 347      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.140012 |\n",
      "|    clip_fraction        | 0.686    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.7     |\n",
      "|    explained_variance   | 0.986    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0489  |\n",
      "|    n_updates            | 5100     |\n",
      "|    policy_gradient_loss | -0.00914 |\n",
      "|    std                  | 0.116    |\n",
      "|    value_loss           | 0.000866 |\n",
      "--------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15185276 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0056    |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | 0.00408    |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15119457 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0518     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.00821   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000893   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16857089 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00398   |\n",
      "|    std                  | 0.116      |\n",
      "|    value_loss           | 0.000914   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 354       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1558213 |\n",
      "|    clip_fraction        | 0.694     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00414  |\n",
      "|    n_updates            | 5180      |\n",
      "|    policy_gradient_loss | -0.0109   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000849  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15749866 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.00949   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000851   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 353       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1387228 |\n",
      "|    clip_fraction        | 0.686     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.8      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.01      |\n",
      "|    n_updates            | 5220      |\n",
      "|    policy_gradient_loss | -0.00955  |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000875  |\n",
      "---------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 361        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15205753 |\n",
      "|    clip_fraction        | 0.689      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | 0.000859   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000903   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.856    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 346      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.160299 |\n",
      "|    clip_fraction        | 0.675    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.8     |\n",
      "|    explained_variance   | 0.985    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0887   |\n",
      "|    n_updates            | 5260     |\n",
      "|    policy_gradient_loss | 0.00189  |\n",
      "|    std                  | 0.115    |\n",
      "|    value_loss           | 0.000934 |\n",
      "--------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 351       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1505478 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.9      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0521    |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | 0.00759   |\n",
      "|    std                  | 0.115     |\n",
      "|    value_loss           | 0.000955  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15396756 |\n",
      "|    clip_fraction        | 0.693      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0928     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | 0.000829   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15011604 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00584   |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.000664  |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000977   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15114152 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0216    |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00269   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15018567 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0195     |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | 0.00654    |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000889   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16578391 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0218     |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00862   |\n",
      "|    std                  | 0.115      |\n",
      "|    value_loss           | 0.000935   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15176943 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.00406   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000943   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 21 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15640345 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0382    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | 0.00959    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 348       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1545304 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0173   |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | 0.000868  |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000926  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 353        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15433545 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0255     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.00168   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000882   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15068972 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00806    |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | 0.000147   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000868   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15404096 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0458     |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00392   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000916   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15492296 |\n",
      "|    clip_fraction        | 0.697      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0667    |\n",
      "|    n_updates            | 5520       |\n",
      "|    policy_gradient_loss | -0.00976   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000974   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14604077 |\n",
      "|    clip_fraction        | 0.698      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0092    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000909   |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15320559 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000689   |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.000491  |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15476206 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00096    |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15449652 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0921    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.00359   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000853   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 11 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 349       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1641707 |\n",
      "|    clip_fraction        | 0.675     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 16        |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0374   |\n",
      "|    n_updates            | 5620      |\n",
      "|    policy_gradient_loss | 0.00261   |\n",
      "|    std                  | 0.114     |\n",
      "|    value_loss           | 0.000863  |\n",
      "---------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15219496 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0024     |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00151   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000917   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14695704 |\n",
      "|    clip_fraction        | 0.694      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0742    |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000921   |\n",
      "----------------------------------------\n",
      "Early stopping at step 9 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 22 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15687068 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000221   |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | 0.00139    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.00089    |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15549836 |\n",
      "|    clip_fraction        | 0.692      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0613     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.00753   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000967   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15067941 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | 0.00613    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000891   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15398566 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00865   |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00788   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n",
      "Early stopping at step 6 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 17 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15663417 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0853     |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | 0.00985    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000901   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 12 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16694845 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00626   |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15263012 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 5800       |\n",
      "|    policy_gradient_loss | -0.0048    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000931   |\n",
      "----------------------------------------\n",
      "Early stopping at step 8 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15509799 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.0089     |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000985   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16617677 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0454     |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | 0.00647    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000981   |\n",
      "----------------------------------------\n",
      "Early stopping at step 10 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 23 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f841c50cac8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f841c4f9278>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.694      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 120        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15500809 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 16.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00912   |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | 0.00187    |\n",
      "|    std                  | 0.114      |\n",
      "|    value_loss           | 0.000941   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 50 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.687       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 338         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056820698 |\n",
      "|    clip_fraction        | 0.414       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | -0.128      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0125      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.713      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04988291 |\n",
      "|    clip_fraction        | 0.469      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.92       |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00459    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.689       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043153916 |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0571     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00411     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.708      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03857675 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00382    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042642526 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00331     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.71      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 343       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0437498 |\n",
      "|    clip_fraction        | 0.488     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.1       |\n",
      "|    explained_variance   | 0.925     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0634   |\n",
      "|    n_updates            | 120       |\n",
      "|    policy_gradient_loss | -0.046    |\n",
      "|    std                  | 0.181     |\n",
      "|    value_loss           | 0.00306   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.715      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03730691 |\n",
      "|    clip_fraction        | 0.49       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.12       |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0465    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00292    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05311913 |\n",
      "|    clip_fraction        | 0.49       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.14       |\n",
      "|    explained_variance   | 0.928      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0768    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.003      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050556827 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.16        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0464     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0426     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00286     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.736      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03706815 |\n",
      "|    clip_fraction        | 0.506      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.18       |\n",
      "|    explained_variance   | 0.931      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0367    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00291    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052769113 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.2         |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00259     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.749      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04843483 |\n",
      "|    clip_fraction        | 0.5        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.23       |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0952    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00252    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.761      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04897382 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.27       |\n",
      "|    explained_variance   | 0.934      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00277    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.76       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05169188 |\n",
      "|    clip_fraction        | 0.511      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.28       |\n",
      "|    explained_variance   | 0.942      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0462    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00249    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053819466 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.31        |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0505     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.046      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0025      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.766       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060507365 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0024      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.767       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050777126 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0909     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05967202 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.41       |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0852    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00234    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05124115 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.46       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.0022     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.772      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06311015 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.46       |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.074     |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.047     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00235    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04155927 |\n",
      "|    clip_fraction        | 0.513      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0409    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00208    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05499617 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.51       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0663    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00224    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05062926 |\n",
      "|    clip_fraction        | 0.53       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.56       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0586    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00224    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.774       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.050460957 |\n",
      "|    clip_fraction        | 0.538       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.6         |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0104      |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00218     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.779       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064490475 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.63        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.071      |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0441     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00218     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054201733 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.66        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0458     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.784      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05720278 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.74       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00231    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057947993 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.82        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057423342 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.82        |\n",
      "|    explained_variance   | 0.952       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0354     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.779       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 349         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059548974 |\n",
      "|    clip_fraction        | 0.56        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.85        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.0021      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.779       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 350         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053631008 |\n",
      "|    clip_fraction        | 0.551       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0784     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05226881 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.97       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00205    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07403279 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.04       |\n",
      "|    explained_variance   | 0.953      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0529    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00214    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06444917 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.1        |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0801    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0449    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00204    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06992182 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0483    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00214    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074548095 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.24        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0831     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00214     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06028049 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0717    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00199    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.784       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068577915 |\n",
      "|    clip_fraction        | 0.582       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.33        |\n",
      "|    explained_variance   | 0.957       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00199     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.785       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064346835 |\n",
      "|    clip_fraction        | 0.571       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.4         |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0723     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00192     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06886627 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.47       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00195    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.79      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0658682 |\n",
      "|    clip_fraction        | 0.571     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.54      |\n",
      "|    explained_variance   | 0.961     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0731   |\n",
      "|    n_updates            | 820       |\n",
      "|    policy_gradient_loss | -0.0396   |\n",
      "|    std                  | 0.169     |\n",
      "|    value_loss           | 0.00186   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.792       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057406116 |\n",
      "|    clip_fraction        | 0.561       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.6         |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0317     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00187     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07113321 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.64       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0117     |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00176    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.794       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063262954 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.71        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0706     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00193     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061195992 |\n",
      "|    clip_fraction        | 0.577       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.77        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0146     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.797     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 348       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0713586 |\n",
      "|    clip_fraction        | 0.568     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.81      |\n",
      "|    explained_variance   | 0.962     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00321  |\n",
      "|    n_updates            | 920       |\n",
      "|    policy_gradient_loss | -0.0383   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.00188   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07612028 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.89       |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0748    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00186    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07467415 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.98       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00188    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.804      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07145046 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.09       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0612    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.0018     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066914454 |\n",
      "|    clip_fraction        | 0.591       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.15        |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.037      |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0374     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08159535 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.23       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07476628 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.3        |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00169    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.808       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076961614 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.35        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0721     |\n",
      "|    n_updates            | 1060        |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00165     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07787007 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.44       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07329474 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.54       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0226     |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00171    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080256805 |\n",
      "|    clip_fraction        | 0.584       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.62        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07452823 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.69       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0469    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00171    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077559635 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.78        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0396     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0299     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.0017      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0672629 |\n",
      "|    clip_fraction        | 0.587     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.83      |\n",
      "|    explained_variance   | 0.968     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0262    |\n",
      "|    n_updates            | 1180      |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    std                  | 0.16      |\n",
      "|    value_loss           | 0.00173   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08145234 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.86       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0301    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.817     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0759124 |\n",
      "|    clip_fraction        | 0.589     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.84      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0186   |\n",
      "|    n_updates            | 1220      |\n",
      "|    policy_gradient_loss | -0.0341   |\n",
      "|    std                  | 0.16      |\n",
      "|    value_loss           | 0.00165   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06548181 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.91       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0679    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06991298 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.96       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0311    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083945714 |\n",
      "|    clip_fraction        | 0.59        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.02        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0425     |\n",
      "|    n_updates            | 1280        |\n",
      "|    policy_gradient_loss | -0.0298     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087286994 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.08        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0966     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06621703 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.09       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0612    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00158    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 350        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06537771 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.13       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0115     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064931825 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.11        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00148     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071649045 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.1         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0436      |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00147     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083452865 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.16        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 1400        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 357        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06399197 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.2        |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0699    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0289    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00139    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 359        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06726808 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.22       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0688    |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080888286 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.27        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000112   |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06346123 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.32       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0659    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06344227 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.38       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0203    |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00145    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07177577 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.45       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06807419 |\n",
      "|    clip_fraction        | 0.587      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.49       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0617    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07178652 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0127    |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082494624 |\n",
      "|    clip_fraction        | 0.59        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.6         |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07152999 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.63       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.01      |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0255    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08176852 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.68       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00658   |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0272    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07647912 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.75       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0636    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074917175 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.87        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0667     |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    std                  | 0.152       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06370513 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.99       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07181821 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0768    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078074455 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0526     |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06598405 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0439     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.843      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07089133 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0276    |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0246    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 336         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084448814 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 1780        |\n",
      "|    policy_gradient_loss | -0.023      |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07799785 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0297    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 338       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0821486 |\n",
      "|    clip_fraction        | 0.601     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.3      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0374   |\n",
      "|    n_updates            | 1820      |\n",
      "|    policy_gradient_loss | -0.0229   |\n",
      "|    std                  | 0.149     |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089075424 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.4        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0802      |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06901343 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09365995 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0492    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08907257 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00842   |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 343       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0813032 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 10.6      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0262   |\n",
      "|    n_updates            | 1920      |\n",
      "|    policy_gradient_loss | -0.022    |\n",
      "|    std                  | 0.148     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08259942 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0616     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07382548 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0733    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000964   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06466104 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000948   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 337         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089987494 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.043      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000971    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07773404 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0202    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000897   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07677426 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00204    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000974   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07385652 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000949   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07785465 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.012      |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.00094    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.074320674 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00878     |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000981    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08662443 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0415     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075204656 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000982    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10549909 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0527    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08695098 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0112    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09388313 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0702    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.851     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 349       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0873914 |\n",
      "|    clip_fraction        | 0.605     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.3      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0673   |\n",
      "|    n_updates            | 2220      |\n",
      "|    policy_gradient_loss | -0.0212   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.00105   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 338         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087666236 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.3        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00698    |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000945    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09069305 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.018     |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000992   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08348833 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0605    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08397127 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0477     |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08178357 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08504151 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000999   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093833975 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0535     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09403175 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0272    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08799837 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000968   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08081571 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000683  |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000983   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08147572 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0704    |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083932444 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0126      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000939    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091226354 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.00101     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08649014 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0202     |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000921   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069666065 |\n",
      "|    clip_fraction        | 0.616       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000944    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10149584 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0606    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000908   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08691513 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0292     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 348         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082947336 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0265      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00086     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085126504 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00967    |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000846    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08534016 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0036    |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10219934 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.058      |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116338655 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00088     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10630474 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00883   |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09118609 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0304    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00085    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11224596 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0643    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 334        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08863873 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000788   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10018794 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0171    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000876   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11756952 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0554    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000878   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09984611 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000875   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09740542 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00819   |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000795   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08077827 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000825   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09588693 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0605    |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000878   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09863174 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000866   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09462963 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0158     |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00091    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088063896 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.083      |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000819    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10383288 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0294    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079937436 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00503     |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00091     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10478411 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0384    |\n",
      "|    n_updates            | 2980       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000946   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10635557 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1053674 |\n",
      "|    clip_fraction        | 0.642     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.9      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0394   |\n",
      "|    n_updates            | 3020      |\n",
      "|    policy_gradient_loss | -0.0166   |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.000876  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088481404 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 3040        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00083     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08288668 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00808   |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10829061 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09576656 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0316    |\n",
      "|    n_updates            | 3100       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09112185 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000849   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101095155 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0351     |\n",
      "|    n_updates            | 3140        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000862    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09823916 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000875   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12210067 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0812     |\n",
      "|    n_updates            | 3180       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000802   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11708941 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.117910445 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0614      |\n",
      "|    n_updates            | 3220        |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000919    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12904832 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0418     |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11362084 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.000963   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 346         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100870565 |\n",
      "|    clip_fraction        | 0.641       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0673     |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.000854    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11270599 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0156    |\n",
      "|    n_updates            | 3300       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000809   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 344      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.110247 |\n",
      "|    clip_fraction        | 0.645    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.4     |\n",
      "|    explained_variance   | 0.986    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0149   |\n",
      "|    n_updates            | 3320     |\n",
      "|    policy_gradient_loss | -0.0147  |\n",
      "|    std                  | 0.131    |\n",
      "|    value_loss           | 0.000895 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12102529 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 3340       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10216651 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 3360       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000847   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10549154 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 3380       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000865   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09595932 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00894   |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000857   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.857    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 350      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.094202 |\n",
      "|    clip_fraction        | 0.635    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.6     |\n",
      "|    explained_variance   | 0.987    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0787  |\n",
      "|    n_updates            | 3420     |\n",
      "|    policy_gradient_loss | -0.0125  |\n",
      "|    std                  | 0.13     |\n",
      "|    value_loss           | 0.000819 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13381913 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00852    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 344         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108951524 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.000121    |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000802    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11259935 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 3480       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000839   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11463608 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00892   |\n",
      "|    n_updates            | 3500       |\n",
      "|    policy_gradient_loss | -0.00996   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09610488 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0515    |\n",
      "|    n_updates            | 3520       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000804   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11293934 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000779   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121737674 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14          |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0369     |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.00959    |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000886    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09968971 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000814   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10742362 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11040534 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0175     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124133095 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 3640        |\n",
      "|    policy_gradient_loss | -0.00994    |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.00077     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113528386 |\n",
      "|    clip_fraction        | 0.649       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000779    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10962844 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0335     |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12521894 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0447    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000826   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14514801 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0646     |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12623255 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0502     |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000776   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11437805 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0408    |\n",
      "|    n_updates            | 3760       |\n",
      "|    policy_gradient_loss | -0.0147    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000712   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10153172 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0436    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 338         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104044095 |\n",
      "|    clip_fraction        | 0.65        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00816    |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000723    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12944297 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0609    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11480248 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0506    |\n",
      "|    n_updates            | 3840       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13089623 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0668     |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000743   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11178172 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0269     |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13258895 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 3900       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000725   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122937635 |\n",
      "|    clip_fraction        | 0.647       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00824    |\n",
      "|    n_updates            | 3920        |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 0.000742    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10726564 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00379   |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000748   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12790279 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0554    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000792   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12692314 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0716    |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.013     |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000809   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13385722 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0115     |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.00984   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15379927 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.00879   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000682   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 302        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11952762 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0244    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000801   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13334277 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000799   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.86     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 338      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.128163 |\n",
      "|    clip_fraction        | 0.667    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 14.5     |\n",
      "|    explained_variance   | 0.988    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0357  |\n",
      "|    n_updates            | 4080     |\n",
      "|    policy_gradient_loss | -0.0129  |\n",
      "|    std                  | 0.125    |\n",
      "|    value_loss           | 0.000747 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13258506 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0736    |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000821   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11727677 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 4120       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000834   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11850853 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0238    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.0091    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000838   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14054306 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0627    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15015016 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0204    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14122903 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0151    |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000764   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12531568 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0508    |\n",
      "|    n_updates            | 4220       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000733   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 347        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13260579 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000365  |\n",
      "|    n_updates            | 4240       |\n",
      "|    policy_gradient_loss | -0.00788   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000769   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121004745 |\n",
      "|    clip_fraction        | 0.667       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0445      |\n",
      "|    n_updates            | 4260        |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000752    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10673859 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.0145    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000772   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12492029 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.001      |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11502383 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00636   |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.00435   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000704   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 342       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1320234 |\n",
      "|    clip_fraction        | 0.669     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0407   |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | -0.0125   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.000726  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13465723 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000744   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11196232 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0137     |\n",
      "|    n_updates            | 4380       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 337       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1289566 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.033    |\n",
      "|    n_updates            | 4400      |\n",
      "|    policy_gradient_loss | -0.0107   |\n",
      "|    std                  | 0.125     |\n",
      "|    value_loss           | 0.00078   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 345       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1346959 |\n",
      "|    clip_fraction        | 0.673     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0301   |\n",
      "|    n_updates            | 4420      |\n",
      "|    policy_gradient_loss | -0.0134   |\n",
      "|    std                  | 0.124     |\n",
      "|    value_loss           | 0.000762  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12133461 |\n",
      "|    clip_fraction        | 0.659      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0363    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000766   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13081713 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0378    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.00963   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13176946 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0663    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12932412 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 4500       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00091    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15120892 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0757    |\n",
      "|    n_updates            | 4520       |\n",
      "|    policy_gradient_loss | -0.00931   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12813102 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0626    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0135    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000869   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13448317 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00999   |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11246212 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00197   |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00902   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000906   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15085874 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0301    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00861   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000881   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12993857 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.036     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000876   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12934497 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.051      |\n",
      "|    n_updates            | 4640       |\n",
      "|    policy_gradient_loss | -0.00977   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13659936 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.00822   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000755   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12597243 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000794   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 343       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1378471 |\n",
      "|    clip_fraction        | 0.67      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15        |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0302    |\n",
      "|    n_updates            | 4700      |\n",
      "|    policy_gradient_loss | -0.00932  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000796  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13337235 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000794   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13675012 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0656     |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000813   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.112769105 |\n",
      "|    clip_fraction        | 0.668       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 15.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 4760        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000796    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11982274 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00244   |\n",
      "|    n_updates            | 4780       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000897   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12485744 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0331     |\n",
      "|    n_updates            | 4800       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000884   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14261195 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0126    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00872   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000882   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12367842 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12314949 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00736    |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | -0.00923   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000898   |\n",
      "----------------------------------------\n",
      "Early stopping at step 7 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 19 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15599042 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0725    |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | 0.0113     |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000926   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13525754 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0235     |\n",
      "|    n_updates            | 4900       |\n",
      "|    policy_gradient_loss | -0.00951   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000882   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12732029 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0142    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13217235 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0558     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000812   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12345084 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00225   |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00826   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000851   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13425498 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00789   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000813   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14710417 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 5000       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000835   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.859     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 347       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1489085 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.2      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.03     |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | -0.00681  |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000938  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13100085 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0332     |\n",
      "|    n_updates            | 5040       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000845   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12586215 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12751874 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0655    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13719308 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16126302 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0358     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14135383 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00845   |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000872   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15055105 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000783   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12190799 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00836   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000849   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13611312 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0426     |\n",
      "|    n_updates            | 5200       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16207406 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 5220       |\n",
      "|    policy_gradient_loss | -0.00435   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000912   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 348        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12407017 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0739    |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000944   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15453346 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0781    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00981   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000875   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 349        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12677321 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0129    |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.01      |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000908   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 346        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15007655 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0395     |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00554   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12901779 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0235    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.17\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 340       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1666107 |\n",
      "|    clip_fraction        | 0.679     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.3      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 5340      |\n",
      "|    policy_gradient_loss | -0.0126   |\n",
      "|    std                  | 0.121     |\n",
      "|    value_loss           | 0.000917  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15092829 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0244    |\n",
      "|    n_updates            | 5360       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000889   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13229172 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00662   |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000843   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 345        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15246367 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0398    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000852   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13403217 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.0152    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000816   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15190056 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0537    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00344   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000891   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 333        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13499022 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0382     |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000843   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15116353 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.101      |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.00911   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.861     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 342       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1511693 |\n",
      "|    clip_fraction        | 0.682     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0135    |\n",
      "|    n_updates            | 5500      |\n",
      "|    policy_gradient_loss | -0.00775  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000883  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.861     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1585085 |\n",
      "|    clip_fraction        | 0.672     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.4      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.04     |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | -0.00289  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000823  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15354714 |\n",
      "|    clip_fraction        | 0.681      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 5540       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000848   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 342        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14208992 |\n",
      "|    clip_fraction        | 0.683      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00893   |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000864   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13855426 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0149     |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000847   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 351        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14547975 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0185    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.0113    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000866   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15014048 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.5       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.00316   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.00093    |\n",
      "----------------------------------------\n",
      "Early stopping at step 12 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 344       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1548197 |\n",
      "|    clip_fraction        | 0.664     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.5      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0216   |\n",
      "|    n_updates            | 5640      |\n",
      "|    policy_gradient_loss | -0.00165  |\n",
      "|    std                  | 0.12      |\n",
      "|    value_loss           | 0.000924  |\n",
      "---------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15128224 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0691     |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00887   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000932   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 340        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13037345 |\n",
      "|    clip_fraction        | 0.684      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0615    |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00702   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 339        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14423418 |\n",
      "|    clip_fraction        | 0.676      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000955   |\n",
      "----------------------------------------\n",
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.862    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 338      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 7        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.150893 |\n",
      "|    clip_fraction        | 0.673    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 15.6     |\n",
      "|    explained_variance   | 0.985    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | 0.0186   |\n",
      "|    n_updates            | 5720     |\n",
      "|    policy_gradient_loss | -0.00642 |\n",
      "|    std                  | 0.12     |\n",
      "|    value_loss           | 0.000906 |\n",
      "--------------------------------------\n",
      "Early stopping at step 16 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 337        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15410888 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 5740       |\n",
      "|    policy_gradient_loss | -0.00943   |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000911   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 341        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13614103 |\n",
      "|    clip_fraction        | 0.685      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00462   |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 344        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15918112 |\n",
      "|    clip_fraction        | 0.686      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0069    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.12       |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "Early stopping at step 13 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 343       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1512632 |\n",
      "|    clip_fraction        | 0.686     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 15.6      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0217    |\n",
      "|    n_updates            | 5800      |\n",
      "|    policy_gradient_loss | -0.00304  |\n",
      "|    std                  | 0.119     |\n",
      "|    value_loss           | 0.000907  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15218167 |\n",
      "|    clip_fraction        | 0.688      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0267    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | -0.00805   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000902   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15124974 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00458   |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.00813   |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000927   |\n",
      "----------------------------------------\n",
      "Early stopping at step 11 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f841c97e978> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f83fc09a860>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.695      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 105        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15267739 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 5860       |\n",
      "|    policy_gradient_loss | 0.00503    |\n",
      "|    std                  | 0.119      |\n",
      "|    value_loss           | 0.000915   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 55 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.692      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 330        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08081289 |\n",
      "|    clip_fraction        | 0.45       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.9        |\n",
      "|    explained_variance   | -0.45      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.0134     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 330         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052582044 |\n",
      "|    clip_fraction        | 0.463       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.866       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0465     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00441     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.718      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 343        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04229314 |\n",
      "|    clip_fraction        | 0.457      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.94       |\n",
      "|    explained_variance   | 0.901      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00384    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040602442 |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.911       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0771     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00359     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 343         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.049015045 |\n",
      "|    clip_fraction        | 0.475       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0478     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00319     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.725       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 324         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052672796 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.99        |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0784     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0471     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00309     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.71        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040674716 |\n",
      "|    clip_fraction        | 0.487       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0334     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0457     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.719       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 334         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048002683 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.081      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0472     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00281     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.721      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 327        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04954487 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6          |\n",
      "|    explained_variance   | 0.933      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0087    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0483    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00283    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 341         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046792895 |\n",
      "|    clip_fraction        | 0.487       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0466     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00255     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 314         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057635892 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00259     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.739       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 331         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048277717 |\n",
      "|    clip_fraction        | 0.52        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0447     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00256     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 314         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037094265 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00667    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00239     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048987456 |\n",
      "|    clip_fraction        | 0.509       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.03        |\n",
      "|    explained_variance   | 0.943       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00243     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.745      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 324        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04774984 |\n",
      "|    clip_fraction        | 0.501      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.1        |\n",
      "|    explained_variance   | 0.944      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00742    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00242    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.75      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 341       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0447653 |\n",
      "|    clip_fraction        | 0.504     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.11      |\n",
      "|    explained_variance   | 0.945     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0864   |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.0445   |\n",
      "|    std                  | 0.181     |\n",
      "|    value_loss           | 0.00235   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063750565 |\n",
      "|    clip_fraction        | 0.527       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.13        |\n",
      "|    explained_variance   | 0.944       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00234     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 331         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060843825 |\n",
      "|    clip_fraction        | 0.523       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0023      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 332         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060586363 |\n",
      "|    clip_fraction        | 0.525       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0484     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0434     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00222     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.759      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 328        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06212709 |\n",
      "|    clip_fraction        | 0.544      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.28       |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0697    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00213    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.763      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 320        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06412488 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.32       |\n",
      "|    explained_variance   | 0.949      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0565    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00223    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.765      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 335        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05832967 |\n",
      "|    clip_fraction        | 0.547      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.0022     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.765       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 332         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056791663 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.49        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00208     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 329        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06908785 |\n",
      "|    clip_fraction        | 0.554      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.57       |\n",
      "|    explained_variance   | 0.951      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.0482    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00222    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 319        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06466172 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.61       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.0021     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 331        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06963079 |\n",
      "|    clip_fraction        | 0.542      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0596    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00201    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.768       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055095006 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.61        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0865     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0449     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00208     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057958405 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.59        |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0994     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 328         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058523316 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.63        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0562     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00203     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 327         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053355463 |\n",
      "|    clip_fraction        | 0.569       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.7         |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0445     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00204     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 319        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05640583 |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.77       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00206    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05854539 |\n",
      "|    clip_fraction        | 0.555      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.85       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0755    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00202    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.777      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 314        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07043061 |\n",
      "|    clip_fraction        | 0.553      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.88       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.0021     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 311        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06756091 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.95       |\n",
      "|    explained_variance   | 0.954      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0841    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00207    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.786       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.065464154 |\n",
      "|    clip_fraction        | 0.565       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.99        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00996    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.002       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.788       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 304         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070631206 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7           |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0228      |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.042      |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00191     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 306        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06597082 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.06       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00213    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 306        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06421696 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.1        |\n",
      "|    explained_variance   | 0.952      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0309    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.0022     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 296        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06795393 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.955      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00215    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 302       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0615013 |\n",
      "|    clip_fraction        | 0.586     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.19      |\n",
      "|    explained_variance   | 0.957     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.043    |\n",
      "|    n_updates            | 800       |\n",
      "|    policy_gradient_loss | -0.0433   |\n",
      "|    std                  | 0.172     |\n",
      "|    value_loss           | 0.002     |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 317        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08471087 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00198    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 321       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0722088 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.39      |\n",
      "|    explained_variance   | 0.958     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0798   |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.0377   |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00201   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.801     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 311       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0620166 |\n",
      "|    clip_fraction        | 0.568     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.49      |\n",
      "|    explained_variance   | 0.961     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00666  |\n",
      "|    n_updates            | 860       |\n",
      "|    policy_gradient_loss | -0.035    |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00185   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071724474 |\n",
      "|    clip_fraction        | 0.588       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.54        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0489     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0393     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00188     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.8       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 309       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0702755 |\n",
      "|    clip_fraction        | 0.587     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.58      |\n",
      "|    explained_variance   | 0.965     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0429    |\n",
      "|    n_updates            | 900       |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    std                  | 0.169     |\n",
      "|    value_loss           | 0.00181   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.804       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069516696 |\n",
      "|    clip_fraction        | 0.584       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.66        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0369     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00195     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.806       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069064274 |\n",
      "|    clip_fraction        | 0.583       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.7         |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0324     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.0019      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.805       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 304         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079915926 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.78        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0325     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00182     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07171615 |\n",
      "|    clip_fraction        | 0.571      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.84       |\n",
      "|    explained_variance   | 0.964      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0415    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00183    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 305        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07611591 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.88       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0575     |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00186    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 300        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04817744 |\n",
      "|    clip_fraction        | 0.582      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.93       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0324    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0323    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00192    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06927724 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.97       |\n",
      "|    explained_variance   | 0.967      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0327    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00165    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 302        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07681189 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.02       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0806    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0337    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06974466 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.02       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0738    |\n",
      "|    n_updates            | 1080       |\n",
      "|    policy_gradient_loss | -0.0305    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060281835 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.07        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 294        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08251905 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.14       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00146    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 293         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056973457 |\n",
      "|    clip_fraction        | 0.6         |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.15        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00869     |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00142     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 297        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08480522 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.19       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0333    |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0324    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00148    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07047035 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.22       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.024      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0312    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00152    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072233304 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.29        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0811     |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0302     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00141     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 298        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07923396 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.37       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0088    |\n",
      "|    n_updates            | 1220       |\n",
      "|    policy_gradient_loss | -0.0301    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 289        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08722076 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.44       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07684628 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.55       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00134    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 292        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07314037 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.62       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0628    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0292    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.826       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076501876 |\n",
      "|    clip_fraction        | 0.588       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.68        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0647     |\n",
      "|    n_updates            | 1300        |\n",
      "|    policy_gradient_loss | -0.0289     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066545986 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.72        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0596     |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.161       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 294        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08454917 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.78       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0257     |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 289        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07237084 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.83       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0281     |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.16       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 287       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0644783 |\n",
      "|    clip_fraction        | 0.587     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.91      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.042     |\n",
      "|    n_updates            | 1380      |\n",
      "|    policy_gradient_loss | -0.0235   |\n",
      "|    std                  | 0.159     |\n",
      "|    value_loss           | 0.00119   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06486927 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.98       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.834     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 292       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0852467 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.05      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0105    |\n",
      "|    n_updates            | 1420      |\n",
      "|    policy_gradient_loss | -0.0258   |\n",
      "|    std                  | 0.158     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06947287 |\n",
      "|    clip_fraction        | 0.583      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.04       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00979   |\n",
      "|    n_updates            | 1440       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06490608 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.07       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0239    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 43 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 284        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06835892 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.14       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 288         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057315934 |\n",
      "|    clip_fraction        | 0.582       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.18        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0242     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 309        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07948409 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.23       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0274    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 308        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06769631 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.3        |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0314    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.023     |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052232813 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.38        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0664     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 295        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09284343 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.46       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0114     |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0251    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077872485 |\n",
      "|    clip_fraction        | 0.594       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.51        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 1600        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 288         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.069209255 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.56        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0598     |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0238     |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.00104     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 291        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08008179 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.58       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0445    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.842       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077077106 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.62        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0491     |\n",
      "|    n_updates            | 1660        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.843       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079447225 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.66        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0343     |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00107     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072250925 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.71        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0143     |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 282         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081119016 |\n",
      "|    clip_fraction        | 0.597       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.78        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0588     |\n",
      "|    n_updates            | 1720        |\n",
      "|    policy_gradient_loss | -0.0211     |\n",
      "|    std                  | 0.154       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07422452 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.87       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0666    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07861051 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.93       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0105     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08041853 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.97       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0619    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06997297 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0228    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08110167 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00647    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07357028 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0687    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00108    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06727458 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0504    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06534342 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0613    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000967   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 287        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07791539 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0197    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 289         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080983266 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0418     |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0225     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000985    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07517908 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0659     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000964   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 294        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08146752 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0296    |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000959   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.85        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072932996 |\n",
      "|    clip_fraction        | 0.599       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0711     |\n",
      "|    n_updates            | 1980        |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000948    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08854906 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.007     |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000903   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 284        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09672131 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0288    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000855   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 289        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08372738 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0164    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0173    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000888   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08912514 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0396     |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000913   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 289        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08252235 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0238     |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10649147 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0044     |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000934   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08131175 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0442    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000916   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10094074 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0408    |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000928   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089249186 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.9        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0304     |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000977    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 285       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0744892 |\n",
      "|    clip_fraction        | 0.609     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11        |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0587   |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0159   |\n",
      "|    std                  | 0.146     |\n",
      "|    value_loss           | 0.000944  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085768126 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11          |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 2200        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.000973    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07533164 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0393     |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00091    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 285        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08192541 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000892   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09990284 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0864    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00095    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07933052 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000882   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10721669 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0225    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000883   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 272         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104055665 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0672     |\n",
      "|    n_updates            | 2320        |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    std                  | 0.143       |\n",
      "|    value_loss           | 0.000863    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08537542 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.107167445 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.5        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0253      |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000813    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09028967 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0297    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000859   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10491691 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00667    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000823   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116702296 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.8        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000822    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11220571 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0366     |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.000891   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101992324 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0542      |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000852    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089695714 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00758     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0197     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000753    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106463455 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000852    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093955204 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000829    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 287        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11010931 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 287        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09504548 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0304     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00089    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1005378 |\n",
      "|    clip_fraction        | 0.629     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12        |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.031     |\n",
      "|    n_updates            | 2580      |\n",
      "|    policy_gradient_loss | -0.0173   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.000929  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 285        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10586391 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000962   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 278       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1166652 |\n",
      "|    clip_fraction        | 0.623     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.2      |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0631   |\n",
      "|    n_updates            | 2620      |\n",
      "|    policy_gradient_loss | -0.0151   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000929  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09762589 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11313965 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0237     |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0155    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000971   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12036047 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00729   |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000976   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11525609 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00096    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08740522 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000989   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116368435 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0382     |\n",
      "|    n_updates            | 2740        |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10114844 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000956   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11986194 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00738    |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00087    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1174659 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.4      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0403   |\n",
      "|    n_updates            | 2800      |\n",
      "|    policy_gradient_loss | -0.0129   |\n",
      "|    std                  | 0.137     |\n",
      "|    value_loss           | 0.000994  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098351166 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 2820        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000928    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13272211 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000937   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 73216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10189531 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 2860       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000938   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 73728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12318649 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 2880       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000924   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 74240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10753898 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0385    |\n",
      "|    n_updates            | 2900       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000887   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 74752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11941899 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 2920       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000871   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 75264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11259379 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0195    |\n",
      "|    n_updates            | 2940       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000878   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 75776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113000676 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 2960        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000887    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 76288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103861734 |\n",
      "|    clip_fraction        | 0.623       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0311     |\n",
      "|    n_updates            | 2980        |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000859    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 76800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11258955 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000678   |\n",
      "|    n_updates            | 3000       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000841   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 77312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09828746 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.068     |\n",
      "|    n_updates            | 3020       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 77824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11712805 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0707    |\n",
      "|    n_updates            | 3040       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000774   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 78336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11564164 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 3060       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00075    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 78848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11507325 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 3080       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 79360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 275       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1050721 |\n",
      "|    clip_fraction        | 0.627     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0892    |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -0.00999  |\n",
      "|    std                  | 0.134     |\n",
      "|    value_loss           | 0.00074   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 79872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10782772 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0206    |\n",
      "|    n_updates            | 3120       |\n",
      "|    policy_gradient_loss | -0.0092    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 80384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10520375 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0505    |\n",
      "|    n_updates            | 3140       |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 80896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10453836 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00668    |\n",
      "|    n_updates            | 3160       |\n",
      "|    policy_gradient_loss | -0.0143    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000796   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 81408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 275         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104234554 |\n",
      "|    clip_fraction        | 0.638       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 3180        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000746    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 81920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12118907 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0172    |\n",
      "|    n_updates            | 3200       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000765   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 82432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13237582 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0672    |\n",
      "|    n_updates            | 3220       |\n",
      "|    policy_gradient_loss | -0.0165    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 82944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11794025 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0622     |\n",
      "|    n_updates            | 3240       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000729   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 83456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.858      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11545734 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0568     |\n",
      "|    n_updates            | 3260       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000791   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 83968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103873335 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00507    |\n",
      "|    n_updates            | 3280        |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000768    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 84480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124960676 |\n",
      "|    clip_fraction        | 0.635       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0246      |\n",
      "|    n_updates            | 3300        |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000691    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 84992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.858       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 285         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.103067495 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 3320        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000736    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 85504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.858     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 291       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1143914 |\n",
      "|    clip_fraction        | 0.642     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.1      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0336   |\n",
      "|    n_updates            | 3340      |\n",
      "|    policy_gradient_loss | -0.0106   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.000698  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 86016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102438405 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0824     |\n",
      "|    n_updates            | 3360        |\n",
      "|    policy_gradient_loss | -0.00891    |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000698    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 86528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.859       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 291         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104479335 |\n",
      "|    clip_fraction        | 0.631       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 3380        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.000728    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 87040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 287        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12521532 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 3400       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00074    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 87552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08551003 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0465     |\n",
      "|    n_updates            | 3420       |\n",
      "|    policy_gradient_loss | -0.00969   |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 88064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.859      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12223542 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0454    |\n",
      "|    n_updates            | 3440       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000693   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 88576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12345972 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0212     |\n",
      "|    n_updates            | 3460       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.000668   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 89088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1361498 |\n",
      "|    clip_fraction        | 0.641     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0223    |\n",
      "|    n_updates            | 3480      |\n",
      "|    policy_gradient_loss | -0.0145   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000706  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 89600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1022047 |\n",
      "|    clip_fraction        | 0.632     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0484   |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -0.00723  |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000691  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 90112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.86      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1079758 |\n",
      "|    clip_fraction        | 0.631     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.3      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.105     |\n",
      "|    n_updates            | 3520      |\n",
      "|    policy_gradient_loss | -0.014    |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.000688  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 90624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11318767 |\n",
      "|    clip_fraction        | 0.644      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00204    |\n",
      "|    n_updates            | 3540       |\n",
      "|    policy_gradient_loss | -0.00946   |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 91136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.86     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 284      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 8        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.129492 |\n",
      "|    clip_fraction        | 0.651    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 13.4     |\n",
      "|    explained_variance   | 0.989    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0429  |\n",
      "|    n_updates            | 3560     |\n",
      "|    policy_gradient_loss | -0.0126  |\n",
      "|    std                  | 0.13     |\n",
      "|    value_loss           | 0.000706 |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 91648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12192066 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0646     |\n",
      "|    n_updates            | 3580       |\n",
      "|    policy_gradient_loss | -0.0163    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000771   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 92160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14911593 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0134     |\n",
      "|    n_updates            | 3600       |\n",
      "|    policy_gradient_loss | -0.0129    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000777   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 92672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10307292 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0126     |\n",
      "|    n_updates            | 3620       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000746   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 93184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12729076 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0315    |\n",
      "|    n_updates            | 3640       |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000745   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 93696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09009103 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 3660       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 94208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13709593 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 3680       |\n",
      "|    policy_gradient_loss | -0.0157    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000713   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 94720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11511054 |\n",
      "|    clip_fraction        | 0.65       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 3700       |\n",
      "|    policy_gradient_loss | -0.00994   |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000735   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 95232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10544331 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0162    |\n",
      "|    n_updates            | 3720       |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000643   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 95744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11300763 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 3740       |\n",
      "|    policy_gradient_loss | -0.00879   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000751   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 96256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1318947 |\n",
      "|    clip_fraction        | 0.655     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0559    |\n",
      "|    n_updates            | 3760      |\n",
      "|    policy_gradient_loss | -0.00922  |\n",
      "|    std                  | 0.129     |\n",
      "|    value_loss           | 0.000792  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 96768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14101836 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0401    |\n",
      "|    n_updates            | 3780       |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000738   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 97280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13702086 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0412    |\n",
      "|    n_updates            | 3800       |\n",
      "|    policy_gradient_loss | -0.0097    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000694   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 97792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12644932 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0415    |\n",
      "|    n_updates            | 3820       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000687   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 98304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 281       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1413872 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.8      |\n",
      "|    explained_variance   | 0.988     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0384   |\n",
      "|    n_updates            | 3840      |\n",
      "|    policy_gradient_loss | -0.00969  |\n",
      "|    std                  | 0.128     |\n",
      "|    value_loss           | 0.000747  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 98816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13698259 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0825    |\n",
      "|    n_updates            | 3860       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 99328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 278        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13161176 |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00747   |\n",
      "|    n_updates            | 3880       |\n",
      "|    policy_gradient_loss | -0.00901   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000765   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 99840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.861       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.120238975 |\n",
      "|    clip_fraction        | 0.656       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.9        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0644     |\n",
      "|    n_updates            | 3900        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000757    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 100352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13744004 |\n",
      "|    clip_fraction        | 0.653      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0264     |\n",
      "|    n_updates            | 3920       |\n",
      "|    policy_gradient_loss | -0.00974   |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000767   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 100864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 286        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11696909 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 3940       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000817   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 101376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12679921 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 3960       |\n",
      "|    policy_gradient_loss | -0.0117    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 101888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 285        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12711945 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 3980       |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.000818   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 102400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14058629 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0407    |\n",
      "|    n_updates            | 4000       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000813   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 102912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12926526 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 4020       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 103424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13428652 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 4040       |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000734   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 103936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 285        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11978195 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0347    |\n",
      "|    n_updates            | 4060       |\n",
      "|    policy_gradient_loss | -0.00963   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000758   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 104448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12974556 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0455    |\n",
      "|    n_updates            | 4080       |\n",
      "|    policy_gradient_loss | -0.00949   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000747   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 104960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11783967 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0485     |\n",
      "|    n_updates            | 4100       |\n",
      "|    policy_gradient_loss | -0.00805   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 105472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 277       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1324625 |\n",
      "|    clip_fraction        | 0.654     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.1      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0216    |\n",
      "|    n_updates            | 4120      |\n",
      "|    policy_gradient_loss | -0.0103   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000773  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 105984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12552337 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14         |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0707    |\n",
      "|    n_updates            | 4140       |\n",
      "|    policy_gradient_loss | -0.012     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 106496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12632371 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 4160       |\n",
      "|    policy_gradient_loss | -0.0106    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000726   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 289        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13849697 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0357    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 107520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12672503 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0707     |\n",
      "|    n_updates            | 4200       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 108032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.861     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 283       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1258036 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0258    |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | -0.0099   |\n",
      "|    std                  | 0.126     |\n",
      "|    value_loss           | 0.000717  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 108544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.861       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 277         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114462554 |\n",
      "|    clip_fraction        | 0.66        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.2        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0986      |\n",
      "|    n_updates            | 4240        |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000684    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 109056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14020082 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00198   |\n",
      "|    n_updates            | 4260       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000667   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 109568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14257902 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0297     |\n",
      "|    n_updates            | 4280       |\n",
      "|    policy_gradient_loss | -0.00936   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.0007     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 110080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10586004 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0792     |\n",
      "|    n_updates            | 4300       |\n",
      "|    policy_gradient_loss | -0.00742   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000655   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 110592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11373003 |\n",
      "|    clip_fraction        | 0.657      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00697   |\n",
      "|    n_updates            | 4320       |\n",
      "|    policy_gradient_loss | -0.0137    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.00064    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 111104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11476414 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0429    |\n",
      "|    n_updates            | 4340       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000669   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 111616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12964064 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.1       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 4360       |\n",
      "|    policy_gradient_loss | -0.00988   |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000663   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 112128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 274       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1259735 |\n",
      "|    clip_fraction        | 0.663     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0174    |\n",
      "|    n_updates            | 4380      |\n",
      "|    policy_gradient_loss | -0.00953  |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000689  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 112640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.861      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11875677 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 4400       |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000672   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 113152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 277        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13486148 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0226    |\n",
      "|    n_updates            | 4420       |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000628   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 113664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13132244 |\n",
      "|    clip_fraction        | 0.656      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 4440       |\n",
      "|    policy_gradient_loss | -0.00911   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 114176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12809719 |\n",
      "|    clip_fraction        | 0.658      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 4460       |\n",
      "|    policy_gradient_loss | -0.0107    |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000585   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 114688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09804313 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 4480       |\n",
      "|    policy_gradient_loss | -0.00586   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000654   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 115200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.862       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 273         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.118028305 |\n",
      "|    clip_fraction        | 0.65        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00996     |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    std                  | 0.126       |\n",
      "|    value_loss           | 0.000628    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 115712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.862       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 279         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.105342075 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.3        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0627     |\n",
      "|    n_updates            | 4520        |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000596    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 44 seconds\n",
      "\n",
      "Total episode rollouts: 116224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 283        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10887472 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0586    |\n",
      "|    n_updates            | 4540       |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000616   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 116736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 280        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11414518 |\n",
      "|    clip_fraction        | 0.646      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 4560       |\n",
      "|    policy_gradient_loss | -0.00929   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000624   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 117248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12412429 |\n",
      "|    clip_fraction        | 0.654      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0159    |\n",
      "|    n_updates            | 4580       |\n",
      "|    policy_gradient_loss | -0.00535   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 117760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14625695 |\n",
      "|    clip_fraction        | 0.652      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 4600       |\n",
      "|    policy_gradient_loss | -0.00391   |\n",
      "|    std                  | 0.126      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 118272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15181927 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.3       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0373     |\n",
      "|    n_updates            | 4620       |\n",
      "|    policy_gradient_loss | -0.00922   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000724   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 118784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.862       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114845894 |\n",
      "|    clip_fraction        | 0.657       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.4        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 4640        |\n",
      "|    policy_gradient_loss | -0.00546    |\n",
      "|    std                  | 0.125       |\n",
      "|    value_loss           | 0.000638    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 119296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 276        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14280722 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0214    |\n",
      "|    n_updates            | 4660       |\n",
      "|    policy_gradient_loss | -0.0125    |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.00063    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 119808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09937282 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0127     |\n",
      "|    n_updates            | 4680       |\n",
      "|    policy_gradient_loss | -0.00799   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 120320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12702143 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0666    |\n",
      "|    n_updates            | 4700       |\n",
      "|    policy_gradient_loss | -0.00991   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000662   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 120832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 274        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12424638 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00608   |\n",
      "|    n_updates            | 4720       |\n",
      "|    policy_gradient_loss | -0.00814   |\n",
      "|    std                  | 0.125      |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 121344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 273        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11367073 |\n",
      "|    clip_fraction        | 0.661      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.000627   |\n",
      "|    n_updates            | 4740       |\n",
      "|    policy_gradient_loss | -0.00526   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000633   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 121856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 282        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14753021 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0117    |\n",
      "|    n_updates            | 4760       |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000676   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 122368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.863       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.124761775 |\n",
      "|    clip_fraction        | 0.666       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.6        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0138      |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000733    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 122880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.863       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108655766 |\n",
      "|    clip_fraction        | 0.665       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0275      |\n",
      "|    n_updates            | 4800        |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000675    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 123392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14390695 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0309    |\n",
      "|    n_updates            | 4820       |\n",
      "|    policy_gradient_loss | -0.00978   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000737   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 123904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 273        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12524857 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0411     |\n",
      "|    n_updates            | 4840       |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00073    |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 124416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 279        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15231304 |\n",
      "|    clip_fraction        | 0.662      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.179      |\n",
      "|    n_updates            | 4860       |\n",
      "|    policy_gradient_loss | 0.000297   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00069    |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 124928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15900758 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00476   |\n",
      "|    n_updates            | 4880       |\n",
      "|    policy_gradient_loss | -0.00546   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000643   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 125440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 273       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506212 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.00333  |\n",
      "|    n_updates            | 4900      |\n",
      "|    policy_gradient_loss | -0.00457  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000633  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 125952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13559136 |\n",
      "|    clip_fraction        | 0.679      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00159    |\n",
      "|    n_updates            | 4920       |\n",
      "|    policy_gradient_loss | -0.0055    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000642   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 126464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 273        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11957121 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0328     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.00822   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.00065    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 126976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 275        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12285318 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0443    |\n",
      "|    n_updates            | 4960       |\n",
      "|    policy_gradient_loss | -0.00571   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000634   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 127488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12918326 |\n",
      "|    clip_fraction        | 0.664      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 4980       |\n",
      "|    policy_gradient_loss | -0.00657   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000633   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 128000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.863     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 272       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1506567 |\n",
      "|    clip_fraction        | 0.659     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0212    |\n",
      "|    n_updates            | 5000      |\n",
      "|    policy_gradient_loss | -0.0028   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000688  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 128512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.863     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 269       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1396682 |\n",
      "|    clip_fraction        | 0.677     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.000235  |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | -0.00586  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000627  |\n",
      "---------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 129024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.863     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 270       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1564022 |\n",
      "|    clip_fraction        | 0.668     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0408   |\n",
      "|    n_updates            | 5040      |\n",
      "|    policy_gradient_loss | -0.00559  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000575  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 129536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13418767 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0458    |\n",
      "|    n_updates            | 5060       |\n",
      "|    policy_gradient_loss | -0.00962   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000587   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 130048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15177983 |\n",
      "|    clip_fraction        | 0.669      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0731    |\n",
      "|    n_updates            | 5080       |\n",
      "|    policy_gradient_loss | -0.00194   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000619   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 130560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 267        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14199147 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 5100       |\n",
      "|    policy_gradient_loss | -0.00927   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 131072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 272        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12910073 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0368     |\n",
      "|    n_updates            | 5120       |\n",
      "|    policy_gradient_loss | -0.0027    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000633   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 131584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.862      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 268        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15005274 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0283     |\n",
      "|    n_updates            | 5140       |\n",
      "|    policy_gradient_loss | -0.0102    |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000615   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 132096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 265        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11837826 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0335    |\n",
      "|    n_updates            | 5160       |\n",
      "|    policy_gradient_loss | -0.00862   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000627   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 132608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 265        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13894962 |\n",
      "|    clip_fraction        | 0.674      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0197     |\n",
      "|    n_updates            | 5180       |\n",
      "|    policy_gradient_loss | -0.00648   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000629   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 133120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.863       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 269         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.114105895 |\n",
      "|    clip_fraction        | 0.661       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0753      |\n",
      "|    n_updates            | 5200        |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    std                  | 0.123       |\n",
      "|    value_loss           | 0.000661    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 133632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.862     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 267       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1411175 |\n",
      "|    clip_fraction        | 0.674     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.00293   |\n",
      "|    n_updates            | 5220      |\n",
      "|    policy_gradient_loss | -0.00581  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000678  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 134144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 268        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11877066 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00361   |\n",
      "|    n_updates            | 5240       |\n",
      "|    policy_gradient_loss | -0.00279   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000639   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 134656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12937574 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 5260       |\n",
      "|    policy_gradient_loss | -0.00684   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000675   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 135168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11613701 |\n",
      "|    clip_fraction        | 0.673      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00739   |\n",
      "|    n_updates            | 5280       |\n",
      "|    policy_gradient_loss | -0.00424   |\n",
      "|    std                  | 0.124      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 135680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 271        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12495923 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.6       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0742    |\n",
      "|    n_updates            | 5300       |\n",
      "|    policy_gradient_loss | -0.00629   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 136192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 266        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11936647 |\n",
      "|    clip_fraction        | 0.672      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.00936   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 136704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13176468 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0108     |\n",
      "|    n_updates            | 5340       |\n",
      "|    policy_gradient_loss | -0.00988   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 137216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.863     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 266       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1317292 |\n",
      "|    clip_fraction        | 0.667     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.7      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0202   |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | -0.0054   |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.00063   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 137728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13943426 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 5380       |\n",
      "|    policy_gradient_loss | -0.00694   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000631   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 138240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 267        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13325885 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 5400       |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000675   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 138752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11782193 |\n",
      "|    clip_fraction        | 0.678      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0216    |\n",
      "|    n_updates            | 5420       |\n",
      "|    policy_gradient_loss | -0.00782   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000642   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 16 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 139264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 264        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15683822 |\n",
      "|    clip_fraction        | 0.663      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0691    |\n",
      "|    n_updates            | 5440       |\n",
      "|    policy_gradient_loss | -0.00736   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000666   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 139776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.863      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 263        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12745829 |\n",
      "|    clip_fraction        | 0.668      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 5460       |\n",
      "|    policy_gradient_loss | -0.0075    |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000673   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 140288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 266        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13800575 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.7       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0126     |\n",
      "|    n_updates            | 5480       |\n",
      "|    policy_gradient_loss | -0.000616  |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000696   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 140800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 266        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12807874 |\n",
      "|    clip_fraction        | 0.666      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0469    |\n",
      "|    n_updates            | 5500       |\n",
      "|    policy_gradient_loss | -0.00822   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000665   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 141312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.864     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 267       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1472361 |\n",
      "|    clip_fraction        | 0.681     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.046     |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | -0.00718  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000745  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 141824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.864     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 269       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1370528 |\n",
      "|    clip_fraction        | 0.661     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.8      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0188   |\n",
      "|    n_updates            | 5540      |\n",
      "|    policy_gradient_loss | -0.00682  |\n",
      "|    std                  | 0.123     |\n",
      "|    value_loss           | 0.000613  |\n",
      "---------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 142336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 265        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15243801 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0112     |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.00497   |\n",
      "|    std                  | 0.123      |\n",
      "|    value_loss           | 0.000655   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 142848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 261        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12109586 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0218    |\n",
      "|    n_updates            | 5580       |\n",
      "|    policy_gradient_loss | -0.00947   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000624   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 143360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 265        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15287805 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0731    |\n",
      "|    n_updates            | 5600       |\n",
      "|    policy_gradient_loss | -0.00749   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000591   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 143872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 265        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12657128 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.8       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00211   |\n",
      "|    n_updates            | 5620       |\n",
      "|    policy_gradient_loss | -0.0037    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000618   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 144384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11803015 |\n",
      "|    clip_fraction        | 0.671      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0625    |\n",
      "|    n_updates            | 5640       |\n",
      "|    policy_gradient_loss | -0.00412   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000661   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 144896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 267        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14568534 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00644   |\n",
      "|    n_updates            | 5660       |\n",
      "|    policy_gradient_loss | -0.00388   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.00058    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 145408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 269        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13033381 |\n",
      "|    clip_fraction        | 0.675      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00638   |\n",
      "|    n_updates            | 5680       |\n",
      "|    policy_gradient_loss | -0.00409   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000585   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 145920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.864       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102666065 |\n",
      "|    clip_fraction        | 0.67        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0314      |\n",
      "|    n_updates            | 5700        |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 0.000585    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 146432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13990289 |\n",
      "|    clip_fraction        | 0.67       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 15         |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 5720       |\n",
      "|    policy_gradient_loss | -0.00566   |\n",
      "|    std                  | 0.121      |\n",
      "|    value_loss           | 0.000574   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 146944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.864       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 270         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.122774385 |\n",
      "|    clip_fraction        | 0.673       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 14.9        |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 5740        |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    std                  | 0.122       |\n",
      "|    value_loss           | 0.000585    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at step 15 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 147456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 270        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15095328 |\n",
      "|    clip_fraction        | 0.667      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00225    |\n",
      "|    n_updates            | 5760       |\n",
      "|    policy_gradient_loss | -0.00261   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000569   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 147968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.864      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14212899 |\n",
      "|    clip_fraction        | 0.68       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.991      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0343    |\n",
      "|    n_updates            | 5780       |\n",
      "|    policy_gradient_loss | -0.00706   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000551   |\n",
      "----------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 148480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.865     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 268       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1521889 |\n",
      "|    clip_fraction        | 0.671     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 14.9      |\n",
      "|    explained_variance   | 0.991     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0162   |\n",
      "|    n_updates            | 5800      |\n",
      "|    policy_gradient_loss | -0.00535  |\n",
      "|    std                  | 0.122     |\n",
      "|    value_loss           | 0.000569  |\n",
      "---------------------------------------\n",
      "Early stopping at step 14 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 148992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.865      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 262        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15273853 |\n",
      "|    clip_fraction        | 0.66       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0245    |\n",
      "|    n_updates            | 5820       |\n",
      "|    policy_gradient_loss | 0.000994   |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000566   |\n",
      "----------------------------------------\n",
      "Early stopping at step 19 due to reaching max kl: 0.15\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 149504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.865      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 267        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15467769 |\n",
      "|    clip_fraction        | 0.677      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 14.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0358    |\n",
      "|    n_updates            | 5840       |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    std                  | 0.122      |\n",
      "|    value_loss           | 0.000617   |\n",
      "----------------------------------------\n",
      "Early stopping at step 17 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 150016\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlgAAAH0CAYAAADhUFPUAAAAAXNSR0IArs4c6QAAIABJREFUeF7snQd4FcXext8khBJIQu+9ShMpUqVJU7gKKDa8IMoVsaIUsVyaCigocEVpcimCCqJeG6ggXbr0GnoNLZQACUlI+Z4ZvnMoCTl7zu45Ozt553l4rjeZ8p/fO7vzZnZ2NigtLS0NTCRAAiRAAiRAAiRAApYRCKLBsowlKyIBEiABEiABEiABSYAGiwOBBEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVBWRwIkQAIkQAIkQAI0WBwDJEACJEACJEACJGAxARosi4GyOhIgARIgARIgARKgweIYIAESIAESIAESIAGLCdBgWQyU1ZEACZAACZAACZAADRbHAAmQAAmQAAmQAAlYTIAGy2KgrI4ESIAESIAESIAEaLA4BkiABEiABEiABEjAYgI0WBYDZXUkQAIkQAIkQAIkQIPFMUACJEACJEACJEACFhOgwbIYKKsjARIgARIgARIgARosjgESIAESIAESIAESsJgADZbFQFkdCZAACZAACZAACdBgcQyQAAmQAAmQAAmQgMUEaLAsBsrqSIAESIAESIAESIAGi2OABEiABEiABEiABCwmQINlMVCVqktNTUV0dDTCw8MRFBSkUmiMhQRIgASyNIG0tDRcvnwZxYsXR3BwcJZmoWvnabB0VRbA8ePHUapUKY17yK6RAAmQgLMJHDt2DCVLlnR2Jxh9hgRosDQeGLGxscibNy/EBRwREeF1T69du4aFCxeibdu2CA0N9bq86gXYP9UVyjw+3fUTvde9j1m5f5cuXZJ/AF+8eBGRkZHOvhgZPQ1WVhsD4gIWF64wWr4arAULFqB9+/baGiz2z7lXhZicddbPZbB07qPuGmbWP7P3Z+deuVkncq5gaay12Qs4K9/8dBgW1M/5KlJDZ2tIg+Vs/cxGT4NllqDC5WmwPD9i4uqAwgPYQ2i6mw+uYDl3bLoip8FyvoZmekCDZYae4mVpsGiwaCAVv0izuInU3STTYDn7+jMbPQ2WWYIKl6fBosGiwVL4AjUQWlY2IAbwKJ+FBkt5ifwaIA2WX/HaWzkNFg0WDZa916DZ1mmwzBK0tzwNlr387W6dBstuBfzYPg0WDRYNlh8vsABUTYMVAMh+bIIGy49wHVA1DZYDRPI1RBosGiwaLF+vHjXK0WCpoYOvUdBg+UpOj3I0WHromGEvaLBosGiwnH2B02Dpq5/Z+7OzyWSN6GmwNNbZ7AXMm7uzBwf1c7Z+Inpq6GwNuYLlbP3MRk+DZZagwuVpsLiCxRUshS9QA6HRYBmAFOAsCddScCnhGhZsO4lKRcLRpGLBO0ZAgxVgcRRrjgZLMUGsDIcGiwaLBsvKKyrwddFgWcv8csI1xF69hqIROZEtJFhWLgzT1aQUhOUIwfbjsTgUE4dTsQloVKEA6pXNj7S0NGw7Hot1h87hj52nsfHIBXdQrasWxtRn7qXBslYmbWqjwdJGyvQdocGiwaLBcvYFToPlu37RF6/icEwccmUPQZGInPh4YRT+t/kE0tKAwuE5cG/Z/Dh+8Sp2nohFcmoacmQLRmJy6i0NFsyTQxqwK4nJ6QKpVTISXeqWRLdGZWmwfJdJ65I0WBrLS4NFg0WD5ewLXGWDJVZ5Vu2PwalLCdKclM4fhny5s8vVoPy5s6NcwdzInSPbHQU4czkBi3acxKL1O3AhJB/KF8qDl1tWRMXCeQyJJlaaok5dRrHInLgQn4R5fx9HofAcyBsWit93nMKeU5fd9QQFQRorkbIFB0lDlVEShqpqsXDkzp4Ni3afRsr/58sVGoKmlQqiTpl8eKhWcUTmCkWeTPrmqpuPCA1JqW0mGixtpQVosGiwaLDsu8DFoyWRgsTsfltKTE7BjhOxqFgoHJFhoT6tgNjRM7H3SBiZNQfOYfGe027TcqdYikfmRM7QEFQrHoFH65SUBuzkxavSvPy0JdptYFzlhVFrX7MY8oVlR6n8uaRR+237KaSmpSFHaAgqFsqDuKRkaex2Rl/KFEFIcJA0eaK9uKQU3Fs2H/7doRqqFovAir1ncexCvDRj9crkR3jObDhzOVHWHxx8Xa8LcUk4GZuA7NmCUTJfLtkPbxMNlrfE9MpPg6WXnrf0hgaLBosGK/AX+LWUVExdeQiTlh+QE7dYMRGPnopG5kT14hHIkyMUP285gWgxeYcE47F6JfFGm8oQqyeuJMyZ2CuUKwT49uff0OWhB5ArZw65H0iszIgVGmFyYuIS0aZqETx+bylUKHRj5ed8XJLcRyRWdyJyheJKQjJyhAbLlSaxeiNiWrEvBlGnLqFMgdwoHpkL2UKCpOFYuS8Gu6IvoVT+MLS6q7A0NMcvXEVyShrG/bkX+85cccd5T6m8qFxEmJ4UHDsfj4vx1xCWPQRnLyfiXFySR/jiMVtE8gU82LAmftl2CmsOnvNYxpVB+CBhlmKuJOJyQjI63lMcObKFIC4xWXLuXLukNK9i31XMlSSULRCWodk13KAPGWmwfICmUREaLI3EvL0rNFg0WDRYgb3Atx2/iIHfb8fuk5mvroiohBGJT0qRAYpVkvIFc8tVmipF8uDg2Tj8feSC+3GWeBxVpWi4NE0nLl7NsFN1SufFpYRkacKOno/HtZT0j8GEKRFPvYSZE8bEl1QkIgeeblAGD9YoKt+iu1MSK0AHY67IPi7YfhLrD52X/y2MZrkCufFM47KoVjQ3XGMUwSH4+I8oGbswd8LkHT4Xhw53F0OZ/Lmlcdp+IhbicV3jigXkJvTC4Tll86LPGa0U+tI/K8vQYFlJ03l10WA5TzPDEdNg0WDRYBm+XAxnFCtL/9t0HKHZgnHs/FVsP3ER+05fkfuAxOqQeDIoVoLebV9Vbq4WK0NiT48wDjuiY3H+SpI0CA/WKIYtxy5i5ILd2Ho81nD7YhWqWeVCEG+widWwHzYdx9KoM9I43ZzEPiER652SeHJ5f5XCOHslEacvJchVNrECVa1YBBqWLyD7JeISq12VCueRb91VKJQbr7WqhJL5wgzHm1lGlfeYWdFBGiwrKDq3Dhos52rnMXIaLBosGiyPl4nHDGIj9S9bo3EuLlFubP5u43FciL+zcel0T3EM+kc1FLjpkV9mjYjVF7GfSDzWE4+6ok5fhnjM+NS9pXEt+Ro2r16GSnXvw7GLicibKzvqlMmLsOy3bh4XK1t/7j4tV6bEypj4X7HvSbwBJ+qMyJUNScmpcgVJGCZh7MSG8LtL5r0lNLFBXbx150quTd5iP5M/Eg1WJGJjYxEREeEPvKzTZgI0WDYL4M/mabBosGiwfL/ChPERe6lG/bEn3eM28aab2N8kNmE3rlAAlYuES8MijEixyFy+N3pbyaxsQCyDaGNFXMGyEb4CTdNgKSCCv0KgwaLBosECUlPTcDAmDiXy5pIbtsUq1M1vhInfizfIxMrN0XPx2Hv6MvaeuSz3QS3adVoOohZVCsmN0+KtspZVCsv9R66DKv11/Yp6abD8Sdf/ddNg+Z+xyi3QYKmsjsnYaLBosLKawRJvrx08ewWrD5yT+4/EW27//euQ3BztOkhSrDp1qFkMF69egziM8viFeJy+dOcN32Iv1b+alrNlEzUNlsmboM3FabBsFsDm5mmwbBbAn83TYNFgZRWDdTEhFW99vw1Los5keDbTzQdNZjQqXL8vkDs7KhXJIx/5idO+a5fOl+m35vx5/XIFy990/V8/DZb/GavcAg2WyuqYjI0GiwbLyQZL7IESG77Fm21i1Uk82hMrVLlzhKBi4XD347O77m2Of83eJN/oE0mcKC7edhOPAcVxCW2qFcHzzcrLsmKflDg/SmxcF6eMi6MRxNt24oRu8fbfzRu8TV5+lhTnCpYlGG2rhAbLNvRKNEyDpYQM/gmCBosGy4kGS5ifORuOyr1Qq/ZnfPDk/XcVRpn8uXDw0CFsvZhDPu4TxuqL7vXkeVG6JBosZytJg+Vs/cxGT4NllqDC5WmwaLBUNljisyt5smdzf5pEHB3w246TmLXmyI0DOEOuf6ZEHIqZcC1VHjcgTgh3fVfOpXAtsdfqmXq3nIau8KVpODQaLMOolMxIg6WkLAELigYrYKgD3xANFg2WKgbryLk4bDxyQT7qE4dYio3nnyyMkv+/e6Oy8u2+KSsOuo1TmQJh6FirODrWLnHLJ2CEouKx37Koszh/JQF79h/EI01roX2tEvIzKbolGixnK0qD5Wz9zEZPg2WWoMLlabBosKw0WOJTJfO3nZQGqXSBjE/yFgdkbjpyATVLRsrDMEWZaX8dwvgl+5GUkioFEQddihPPM0ridPKaJfLKt/bEHqnMku7mQ/Rd9z5m5f6ZvT8rPPUwtP8nQIOl8VAwewFn5ZufDsPCSv3EcQbPzdggPzScMzQYbasVRe3SefFAjaIQB2ueu5KIWWuP4IdNJ+QnYcSZU+LcKLGh/HJissRZo0SE/Jae+PCu+JRMvzaV5bf3Bv24Q54+/uGjd6Nd9aKG0VvZP8ONBjij7n3Myv0ze38O8FBkcz4QoMHyAZpTipi9gLPyzc8pGluxwhOflIz9Z66gevFIeRK5SOINvujYBHlIpzg/6qkv1uLA2TiEhgSlO9VcGK39p6+4jdTtMZUtEIY32lTGw7WKy+/dHYqJk4/9xAeORRIfBc4RGpzu8y+eNNB9fHIFy9MIUP/3fESovkb+jJAGy590ba6bBitzAXSfoD31T5inT5fsw4RlB+R36sQHhMWhmrPXHpGbzcVK082peGROzHuxsTRjO07EYumeM9h49IJ735Q4AmFA2yq4v2ph/LbjFJJTUnF3yUjULpXPvZHdykvCU/+sbMuuunTvY1bun9n7s11jku0aJ0CDZZDVhAkTMHr0aJw8eRLVq1fHuHHj0LRp0zuWFr+fOHEijh49ioIFC6JLly4YOXIkcubMKcsMHToUw4YNu6V8kSJFcOrUKffPxCqCyDNlyhRcuHABDRo0wOeffy7bN5LMXsBZ+eZnhK/qeTLTT+yN6vvtFvyx8/qnYDJKYq9UcFCQ3DsljNInj9VCpSK3HoEgNq+//PUmHDobh+nP1kf9cvkDhkX38SlA6t7HrNw/s/fngF1obMhnAjRYBtDNnTsX3bp1gzBZTZo0weTJkzF16lTs2rULpUuXTlfDV199hZ49e2LatGlo3Lgx9u7dix49euCJJ57A2LFj3Qbru+++w59//ukuHxISgkKFCrn//0cffYThw4djxowZqFy5Mj744AOsWLECUVFRCA/3fNaP2Qs4K9/8DAwL5bMI/ebPX4AGzVth16k4bD0ei5TUVLkCJc6XupKYjOwhwRjeuQbEx4vf/d8O7Dl1CXeXzCsf6TUqXwCpaWlyo3qBPDnu2F/xh8DVayleP+IzC1D38UmDZXaE2F+ejwjt18DOCGiwDNAXK0d16tSRK1KuVLVqVXTq1EmuSt2eXnnlFezevRuLFy92/6pfv35Yv349Vq5c6TZYP/74I7Zs2ZJhBGLSKl68OF5//XUMHDhQ5klMTIRY5RLG64UXXvAYOQ1W5oh0n6DnrDuMIT/vQGLK9X1VtyexEX1819qoUzqf+1cpqWnufVgeB5jNGXTXjwbL5gFmQfM0WBZAdHAVNFgexEtKSkJYWBjmzZuHzp07u3P36dNHmqPly5enq2HOnDno3bs3Fi5ciPr16+PgwYPo0KEDnnnmGbz11ltugyUeOUZGRiJHjhzy8d+IESNQvnx5+XtRpkKFCti0aRNq167tbqNjx47ImzcvZs6cma5dYcDEP1cSBqtUqVKIiYlBRESE18NU3BwWLVqENm3aIDQ01OvyqhfQuX/i0M6npm5wH4cgPh1TSx6dEIJCeXKgaaUCqF4swi97owKlu876uRjq3ses3D9xfxbbR2JjY326PwfqOmM7vhOgwfLALjo6GiVKlMCqVavk4z5XEmZImBzxuC6jNH78eIhVK7ESlZycjBdffFE+YnSl3377DfHx8fLR3+nTp+Xjvz179mDnzp0oUKAAVq9eLR9HnjhxQq5kuVKvXr1w5MgR/PHHH+mazWhfl8j09ddfS5PIlDUIxF0DRm8LwYWkIFTPl4pnK6ci9PoLe0wkQAKKEBD3/65du9JgKaKHP8KgwTJosIThadSokTu32Bs1a9YsaYpuT8uWLcOTTz4pTZNYmdq/fz/Eitfzzz+PQYMGZdhiXFycXLF688030bdvX7fBEgavWLFi7jKijmPHjuH3339PVw9XsLy7RHT56/nS1Wvyw8biCIWle2MwdtE+7Dl9BaXz58JLFS7j4Qe5AundyFAnty5j9E5Es3L/uIKlznXmr0hosDyQ9eURoXi7sGHDhvKtQ1eaPXs2xOrTlStXEByc8XKCeBRXsWJFudfLl0eEt3eFe7AyF9fJe3jOXk7E1L8OYtuxWKw5eE4e/pknRzb30QrhObJhds96OLz5L7Rv317bR7xWnlTvr5usmXqdPEaN9Dsr98/s/dkIX+axlwANlgH+YhWqbt26tzziq1atGsR+qIw2uYu8rVu3lpvRXembb77Bc889Jw2WeFvw9iRWn8QKljBhgwcPlo8WxaPBN954Q65qiSTMXuHChbnJ3YBmRrI49eYuzpd6dOJq+Vbg7SlXaAi6NyqD55uVR2SOYOhsQJyqn5Gx6cqjex+zcv9osLy5EpyZlwbLgG6uYxomTZokHxOKc6m++OILuV+qTJky6N69u9yn5TJbYi/UmDFjZD7XI0KxB0sYL1GXSP3798dDDz0kj3k4c+aMfJwoNsxv375d1imSMGiizunTp6NSpUpyE7x4/MhjGgyIZiCLE2/uwngPn78bU/86JB8J9m1TBe1rFpWnqydcS5HfCIzIef2FBCf2z4Bs7iy6948aejMa1MzLtwjV1CVQUdFgGSQtNqiPGjVKHjRao0YNeZ5Vs2bNZOkWLVqgbNmy8rwqkcSmdtceLbFJXZxtJcyU+Jl4A1AksUdLnGkl3vATvxePFN9//32IlTFXch00Ks7duvmgUdG+kWT2LyTdJzCn9U+cvD745x2YvfaolP8/T96DjveUuONQcFr/jIzpm/Po3j8aLG9HhHr5abDU0ySQEdFgBZJ2gNuiwcocuJMmaHHY55vfbcP87ScRFAR80KkGnm5wfaXzTslJ/fPl0tC9fzRYvowKtcrQYKmlR6CjocEKNPEAtkeD5WyDtfnoBbw4e5P8KPLlhGu4EH8N4vM1Y564R3442VPS3YDo3j8aLE8jXP3f02Cpr5E/I6TB8iddm+umwXKuwRKfrHls0hpcTkh2d6JYZE58+lRt3FvW2Pf+dDcguvePBsvmG6gFzdNgWQDRwVXQYDlYPE+h02A502BdjE/Cw5+twtHz8ahWLEJ+F7BAnuyoXjwCObKlfwOVjwj1PIaCBsvTHU7939Ngqa+RPyOkwfInXZvrpsFypsEa+vNOzFh9GKXy58Ivr9yHvGHZfRpJuq/w6N4/Giyfhr1ShWiwlJIj4MHQYAUceeAapMFynsESe60ajliMuKQUfPlcfTSrXMjnAaO7AdG9fzRYPg99ZQrSYCkjhS2B0GDZgj0wjdJgOc9gTV15EB/M3w3xceY/+zZHkHhl0MekuwHRvX80WD4OfIWK0WApJIYNodBg2QA9UE3SYDnDYB2/EI8Jyw5g3cFzOHA2Tgb9fqca6NYw82MYPI0j3Q2I7v2jwfI0wtX/PQ2W+hr5M0IaLH/StbluGiz1Ddax8/F46LO/cDH+mjvYllUKYeoz9yIk2PfVK07ONl98FjWvu4nMyv0ze3+2aIixGj8SoMHyI1y7qzZ7AWflm18gtLuWkorHJ6/B5qMXcVfRcPzj7mI4H3cNr7ep5P7cjZk4qJ8ZemqUpYZq6OBrFFzB8pWcHuVosPTQMcNe0GCpvYL10e97MHHZAYTnzIYFrzVFqfxhlo5GTs6W4rSlMmpoC3bLGqXBsgylIyuiwXKkbMaCpsFS12Ct2HsW3aetlwFOeLoO2tcsZkxUL3JxcvYClqJZqaGiwhgMiwbLIChNs9FgaSqs6BYNlnoG63xcEiYtP4Dpqw7hWkoanm5QGsM71/TLKOTk7BesAa2UGgYUt+WN0WBZjtRRFdJgOUou74KlwVLPYL36zWb8sjVaBib2Xf34chPkDDV+Ors3I4CTsze01MxLDdXUxWhUNFhGSemZjwZLT11lr2iw1DFYGw6fx+jfo7D+8HkZVO/mFfBiiwqIzBXqtxHIydlvaANWMTUMGGq/NESD5ResjqmUBssxUnkfKA2WGgZLvC3YfNRSRMcmyIAerVMSnzxey3tBvSzBydlLYApmp4YKiuJFSDRYXsDSMCsNloaiurpEg6WGwfp+43H0m7dVBvNC8/J4qXlFRIb5b+XK1WtOzs6/uKmhszWkwXK2fmajp8EyS1Dh8jRY9husM5cS0HnCapy4eBVvPlAFL7WoGLARw8k5YKj91hA19BvagFRMgxUQzMo2QoOlrDTmA6PBstdgxV69hicmr8GeU5dRtkAYfnn1PoTn9P/KFVewzF87qtRAg6WKEr7FQYPlGzddStFg6aJkBv2gwbLPYIlP4Lw2Z7M8pb1QeA788GJjyw8S9TR0OTl7IqT+76mh+hplFiENlrP1Mxs9DZZZggqXp8EKvMG6mpSCIT/vwI+bo5GUkipPaf/2hUaoWiwi4COFk3PAkVveIDW0HGlAK6TBCihu5RqjwVJOEusCosEKvMH6dPE+jFm0VzbcuEIBfNCpBsoXymOdqF7UxMnZC1iKZqWGigpjMCwaLIOgNM1Gg6WpsKJbNFiBNVgJ11LQ+MMlEKe1f/hITTxZv7Sto4uTs634LWmcGlqC0bZKaLBsQ69EwzRYSsjgnyBosAJnsC4nXEP/eVvxx87TKJkvF5b1b4FsIcH+EdZgrZycDYJSOBs1VFgcA6HRYBmApHEWGiyNxaXBCpzBGvbLTkxfdRjZgoMw/qnaeNAPH2/2dqhycvaWmHr5qaF6mngTEQ2WN7T0y0uDpZ+m7h7RYAXGYKWlpaHZ6KU4dv6qNFcP1SquxKji5KyEDKaCoIam8NlemAbLdglsDYAGy1b8/m2cBiswButQTBxafrwMoSFB2DK4LXLnyOZfYQ3WzsnZICiFs1FDhcUxEBoNlgFIGmehwdJYXBqswBismasPY8jPO9GofAF806uhMiOKk7MyUvgcCDX0GZ0SBWmwlJBS5DP1AAAgAElEQVTBtiBosGxD7/+GabACY7Cenb4eS6POYuADd+HFFhX8L6zBFjg5GwSlcDZqqLA4BkKjwTIASeMsNFgGxZ0wYQJGjx6NkydPonr16hg3bhyaNm16x9Li9xMnTsTRo0dRsGBBdOnSBSNHjkTOnDllGfHfP/zwA/bs2YNcuXKhcePG+Oijj1ClShV3nS1atMDy5ctvaeOJJ57AnDlzDEVNg+V/g3Up4Rrqvr8I11LSsOiNZqhUJNyQNoHIxMk5EJT92wY19C9ff9dOg+VvwmrXT4NlQJ+5c+eiW7duECarSZMmmDx5MqZOnYpdu3ahdOn0Zx199dVX6NmzJ6ZNmyaN0969e9GjRw8IczR27FjZ4gMPPIAnn3wS9957L5KTk/Huu+9i+/btss7cuXPLPMJgVa5cGe+99547SmHGIiMjDUTNc7A8QbJi8vpx8wm8PncLKhTKjcX9WnhqMqC/t6J/AQ3Yy8Z075/AoXsfs3L/zP4B7OXlwuw2EKDBMgC9QYMGqFOnjlyRcqWqVauiU6dOciXq9vTKK69g9+7dWLx4sftX/fr1w/r167Fy5coMWzx79iwKFy4sV6yaNWvmNlj33HOPXC3zJZm9gLPyzc8I7yV7TqPXlxuRnJqGV1pWRP92N1YfjZT3dx7q52/C/q+fGvqfsT9b4AqWP+mqXzcNlgeNkpKSEBYWhnnz5qFz587u3H369MGWLVvSPcITGcQjvN69e2PhwoWoX78+Dh48iA4dOuCZZ57BW2+9lWGL+/fvR6VKleQqVo0aNdwGa+fOnRDHABQpUgQPPvgghgwZgvBwY4+haLAyF9fM5LX/zBU8MG6FNFfi7Kvf+jRV6vEgVz/Uv/kaidDMGDVSv915snL/zN6f7daO7XsmQIPlgVF0dDRKlCiBVatWycd9rjRixAjMnDkTUVFRGdYwfvx4iFUrYY7EI8AXX3xRPmLMKIk8HTt2xIULF25Z4friiy9Qrlw5FC1aFDt27MDbb7+NihUrYtGiRRnWk5iYCPHPlcQFXKpUKcTExCAiwvuPDYubn2irTZs2CA0N9TyaHJbD1/4JvXp+uQkr959D4wr5MbxjdXl6u2rJ1/6p1o87xaN7/1wmmdegU0Zk+jgzG6Pi/iz258bGxvp0f3YulawTOQ2WQYO1evVqNGrUyJ17+PDhmDVrltykfntatmyZ3F/1wQcfQDxeFKtTYsXr+eefx6BBg9Llf/nllzF//nz89ddfKFmy5B0j2rhxI+rVqwfxv+KR5e1p6NChGDZsWLqff/3113IVjsk8gSNXgFWngrHubDBCgtLwdq0UFFLPW5nvKGsgARLwK4H4+Hh07dqVBsuvlO2tnAbLA39fHhGKtwsbNmwo3zp0pdmzZ6NXr164cuUKgoNvfKPu1VdfxY8//ogVK1bI1arMklg5yZEjhzR2YsP87YkrWN5dTN6ugPxvczTe/GGHu5FBHe5C94b2ftA5sx572z/v6NmfW/f+CcK69zEr948rWPbfQ/wdAQ2WAcJiFapu3bq3POKrVq2afKyX0SZ3kbd169by2AVX+uabb/Dcc89JgxUSEiIfHQpz9b///Q9ixUvsv/KUxGPCmjVr3rIRPrMyZp/xZ+X9EbdzPRwThw6frkRcUoo8sf25JuXw1oN3ISgoyJNstv2e+tmG3rKGqaFlKG2piJvcbcGuTKM0WAakcB3TMGnSJPmYcMqUKRD7o8QG9DJlyqB79+5yn5bLbIlHdWPGjJH5XI8IxR4sYbxEXSK99NJLEI/ufvrpp1vOvhJHMIijGA4cOABx3EP79u3lc3pxfIPY0yV+t2HDBmnSPCUarMwJGZ28rqWk4rFJa7Dl2EXUL5cf3zzfECHB6horV6+N9s/TOFL197r3z7WCtWDBAnkf0HUfZFbtn9n7s6rXJeO6QYAGy+BoEBvUR40aJQ8aFW/5ifOsbj5OoWzZspgxY4asTWxqd+3ROnHiBAoVKoSHHnpI/ixv3rwyz51WPqZPny7PzDp27Bj++c9/ys3tYtVLbFYXbyKKtwjz589vKGqzF7DuE5iR/iVcS8HA77fhpy3RCM+ZDb+/3gwl8jpj05WR/hkaSIpm0r1/NFiKDjwvwuIKlhewNMxKg6WhqK4u0WCZX8F6+4dt+Gb9Mbli9XnX2nigRjHHjBjdDYju/aPBcsyldsdAabCcr6GZHtBgmaGneFkaLHMG6/SlBNz30RL5GZz/PlMPraoWUVzxW8PT3YDo3j8aLEddbhkGS4PlfA3N9IAGyww9xcvSYJkzWB/+tgeTlh/AvWXzYV7vG2egKS67OzzdDYju/aPBcsqVduc4abCcr6GZHtBgmaGneFkaLN8N1tI9Z9Bz5gakpgGTu9VFu+pFFVc7fXi6GxDd+0eD5bhLLl3ANFjO19BMD2iwzNBTvCwNlm8GKy4xWT4avBB/DY/VLYlRXe5W+jiGO/VSdwOie/9osBS/wRoIjwbLACSNs9BgaSwuDZZvBuu/fx3C+7/uQpkCYfizb3OEhtw4GNZJw0V3A6J7/2iwnHS1ZRwrDZbzNTTTAxosM/QUL0uD5b3BSk1Nk6tX0bEJGNG5Jro2UPekdk/DT3cDonv/aLA8jXD1f0+Dpb5G/oyQBsufdG2umwbLe4O1/8xltB6zArlCQ7B5cBvkDPV8oKvNMt+xed0NiO79o8FS9coyHhcNlnFWOuakwdJR1f/vEw2W9wbru43H0X/eVse+OXhzj3U3ILr3jwbL+TdnGizna2imBzRYZugpXpYGy3uDNfinHfhyzRH8675y+Pc/qimusPf9c3SHbgueBsv5auquIQ2W88eomR7QYJmhp3hZGizvDUjHz1dh67GL+PSp2ni4VnHFFfa+f47uEA2WTvLJvtBgRSI2NhYRERHaacsOATRYGo8CGizvDEhScipqDPkDSSmpWDGgJUoXCHP06MjKk5ejhbspeGrobCW5guVs/cxGT4NllqDC5WmwvDNY/9t8HG/M3YqCebJjw7utHXn21c095uSs8MVpMDRqaBCUotlosBQVJkBh0WAFCLQdzdBgGTdYaUEhuP+TZTh+4SrefKAKXmpR0Q7JLG2Tk7OlOG2pjBragt2yRmmwLEPpyIposBwpm7GgabCMG6zPlh3CfxbvQ6HwHPLxYK7szj2ewdVrTs7GrhOVc1FDldXxHBsNlmdGOuegwdJYXRosYwarYt2m6DhhLZJT0zD+qdp4yOGb22mw9LmoabCcrSUNlrP1Mxs9DZZZggqXp8EyZrC2BVfAf1cdQau7CuO/Pe5VWFHvQuPk7B0vFXNTQxVVMR4TDZZxVjrmpMHSUdX/7xMNljGDNX5/JPafjdNq9Ur0nJOz8y9uauhsDWmwnK2f2ehpsMwSVLg8DZZngzX7fwswbFM2BAcBmwe1RWRYqMKKehcaJ2fveKmYmxqqqIrxmGiwjLPSMScNlo6qcgXLkKri5vfOtN8w71CIFp/Gub3TnJwNDQOlM1FDpeXxGBwNlkdEWmegwdJYXq5geV7B6jzmd+y4EIwB7arg5ZbOP5rh5h5zcnb+xU0Nna0hDZaz9TMbPQ2WWYIKl6fBylycK/EJqPPBn0hKDcKC15qiWnG9PlfByVnhi9NgaNTQIChFs9FgKSpMgMKiwQoQaDuaocHKnPpPm46hz7fbUCQ8B9a+08rxJ7fzEaEdV5l/26TB8i9ff9dOg+VvwmrXT4Oltj6moqPBujO+oT/vxIzVh2WGx+uWwKjH7jHFWsXCnJxVVMW7mKihd7xUy02DpZoigY2HBiuwvAPaGg1WetxjF+3FzDWHcTH+mvxl7mxpmNmzIeqVKxhQbQLRGCfnQFD2bxvU0L98/V07DZa/CatdPw2W2vqYio4G61Z8q/bH4Omp69w/7N2sHKpe24f27dsjNFSf4xlcHeTkbOryUaIwNVRCBp+DoMHyGZ0WBWmwtJAx407QYN3gkpaWhk4TVmPrsYvyhy+1qICXm5fDnwt/p8Fy6DWgu/kQsujex6zcP7P3Z4detlkqbBosjeU2ewHrdPM7cPYKWn2yHNmzBWPVwPvlR5116l9Gw5j9c/7FTQ2drSFXsJytn9noabDMElS4PA3WDXF+33ESvWdvQs0Skfjl1fvkLzh5KTx4DYSmu34cowYGgeJZaLAUF8jP4dFg+RmwndVnRYO1ZM9pfLp4P07FJuDzp+ugbpl8UoLPluzDxwv34pHaJTDmietvDOo+QbN/dl591rRNDa3haFctNFh2kVejXRosgzpMmDABo0ePxsmTJ1G9enWMGzcOTZs2vWNp8fuJEyfi6NGjKFiwILp06YKRI0ciZ86c7jKe6kxMTET//v3xzTff4OrVq2jVqhVEmZIlSxqKOisarOajl+LIuXjJRzwG/OWV+1A0Miden7MZP26JvuXEdk5ehoaRspl0149/BCg79AwHRoNlGJWWGWmwDMg6d+5cdOvWTZqbJk2aYPLkyZg6dSp27dqF0qVLp6vhq6++Qs+ePTFt2jQ0btwYe/fuRY8ePfDEE09g7NixMr+ROl988UX88ssvmDFjBgoUKIB+/frh/Pnz2LhxI0JCQjxGntUMVnJKKqoM+h0pqWluNveUyou5LzTEIxNWY2f0JUzpVhdtqxflCpbH0aN+Bhos9TXyFKHuGtJgeRoBev+eBsuAvg0aNECdOnXkipQrVa1aFZ06dZKrUrenV155Bbt378bixYvdvxLmaP369Vi5cqX8mac6Y2NjUahQIcyaNUsaM5Gio6NRqlQpLFiwAO3atfMYeVYzWMcvxOO+j5YiNCQIC99ojk6fr0Ls1Wt4vXUlTFp+AAnXUrGkX3OUL5SHBsvj6FE/g+6TM1ew1B+DniKkwfJESO/f02B50DcpKQlhYWGYN28eOnfu7M7dp08fbNmyBcuXL09Xw5w5c9C7d28sXLgQ9evXx8GDB9GhQwc888wzeOutt2CkziVLlshHgmLFKl++6/uIRKpVq5Y0dsOGDUvXrnikKP65kjBYwpDFxMQgIsL77+yJm8OiRYvQpk0bR5wTte7Qefxz2t8okz8Mf75xH77ZcAyDf96NohE5cOpSojRe2wa1QraQYLfBclL/vL0VOU0/9i89AWro7ahQK39m+on7s9g+Iv6Y9uX+rFZPGU1GBGiwPIwLsWpUokQJrFq1Sj7uc6URI0Zg5syZiIqKyrCG8ePHy0d64vyl5ORkiMd94hGjSEbq/Prrr/Hss8/eYphE2bZt26JcuXLyMeXtaejQoRkaL1GXMIm6p/VngvDVgRBUjkzFy9VScfoqMGJLNne3S4Sl4c1aKbpjYP9IgAQcQCA+Ph5du3alwXKAVr6GSINl0GCtXr0ajRo1cucePny4fHy3Z8+edDUsW7YMTz75JD744AP5KHD//v0QK17PP/88Bg0a5DZYmdV5J4MlVpMqVKiASZMmpWs3q69gjV9yAJ8uPSC/LTi8U3Wkpqah3siluJyQLFn1aloWA9pWdnPj6oCvtw01yumun6Csex+zcv+4gqXGfcSfUdBgeaBr5HHe7VWItwsbNmwo3zp0pdmzZ6NXr164cuWKXNHy9NjRl0eEt8eR1fZg9Z+3Fd9tPI7+bSvjlfsrSRxPTVmLNQfPyf/+rncj1Cub/xaDJfaz8VM5/rzF+K9u7sHyH9tA1ay7htyDFaiRpGY7NFgGdBGrUHXr1nU/4hNFqlWrho4dO2a4yV3kbd26NT766CN37eKoheeee04aLPEGoKc6XZvchTF7/PHHZT3iiAhxRAM3uWcs2hOT10Dswxr3xD3oVLuEzPTSVxuxYPsp+d8HRrRHSHAQDZaBMe+ELLpPzq4VLP4R4ITRmHGMNFjO1c6KyGmwDFB0HakgHsuJx4RTpkzBF198gZ07d6JMmTLo3r273KfleqNQ7IUaM2aMzOd6RCj2YAnjJeoSyVOdIo8o8+uvv8pjGvLnzy/PxDp37hyPabiDZk0+XIITF6/i+xcboW6Z6ytVu09ekm8TvtCsPPq2rXJLSd0naPbPwMWteBZqqLhAHsKjwXK2fmajp8EySFBsUB81apRcRapRo4Y8z6pZs2aydIsWLVC2bFlphEQSjwBde7ROnDghj1t46KGH5M/y5s3rbjGzOkWmhIQEDBgwAGI/1s0HjYo3A42krPSI8OYzsNa90wpFIm4c6CpeNBApKOjG6hVXB4yMILXz6G4+OEbVHn9GoqPBMkJJ3zw0WPpqi6xksI6dj0fTUUuRPSQYe95/AME3PQq8k8S6T9Dsn/MvbmrobA1psJytn9noabDMElS4fFYyWGsOnMNTX6xFuYK5sbR/C0OqcPIyhEnZTLrrxxUsZYee4cBosAyj0jIjDZaWsl7vVFYyWPP+PoYB321D00oFMatnA0Oq6j5Bs3+GhoHSmaih0vJ4DI4GyyMirTNkGYMlzIY4+qBKlSoQn7nJCikrGawxi/bi08X78FT9Uhj5yN2G5OXkZQiTspl0148rWMoOPcOB0WAZRqVlRm0NljjaQGxCF98FFBvExSdmDh8+LE9WF5+yefTRR7UU9OZOZSWD1ffbLfhh0wkMaFcFL7esaEhb3Sdo9s/QMFA6EzVUWh6PwdFgeUSkdQZtDVbRokXxxx9/SGMl3sIbMmQItm7dKj9vI45P2Lx5s9bCZrVHhI9PXoP1h87jP0/eg473XD8Dy1Pi5OWJkNq/110/rmCpPf6MREeDZYSSvnm0NVi5cuXC3r175ceOxTlVxYsXx4cffoijR4/KQ0LFgZ+6p6y0gnXjDKzGqFvmxsexM9NY9wma/XP+FU4Nna0hDZaz9TMbvbYGq3LlyvJbgB06dJAfRxaPBe+//365itWqVSvExMSYZad8+axisK6lpKLKv39Dahqw/p1WKHzTGVg0WPwUkPIXaiYB0mA5Wb3r35K800n8Zu/PziaTNaLX1mCJQzzFB5bz5MkjT1vftGkTgoODMX78ePzwww9YunSp9gqbvYCdcnNfGnUGz07fgIic2bBlcFtDZ2Dx8Yvzh79TxqcZ0rr3MSv3z+z92cy4YtnAENDWYAl8f//9N44dO4Y2bdpIoyXS/Pnz5WnqTZo0CQxhG1sxewE75ebXY/p6LIs6i573lcOgf1QzTNwp/TPcodsysn++klOnHDVURwtfIuEKli/U9CmjtcHSRybfepIVDNaRc3FoPnoZxFdwlvZrgbIFcxuGxcnLMColM+quH1dZlRx2XgVFg+UVLu0ya2Ww+vbta1gg8TFm3VNWMFiu86+8OWDUpbvuEzT75/wrnBo6W0MaLGfrZzZ6rQxWy5Ytb+GxceNGpKSkyMNFRRJvFYaEhKBu3bry0FHdk+4GKzU1Dc1GL8XxC1e9Op6BBkuPka+7+eAKlvPHKQ2W8zU00wOtDNbNIMQK1bJly+S5V/nyXX9t/8KFC3j22WfRtGlT9OvXzww3R5TV3WCtO3gOT0xZizw5smHDu62RK3uIV7roPkGzf14NByUzU0MlZTEcFA2WYVRaZtTWYJUoUQILFy5E9erVbxFux44daNu2LaKjo7UU9OZO6W6wBn63DXP/PoYn6pXCR12MfR7nZj6cvJx9CeiuH1ewnD0+Peln9v7sfDr690BbgxUeHo6ffvpJnn11cxKPBjt27IjLly9rr67ZC1jlCexqUgruHf4nriQmY26vhmhQvoDXeqrcP687k0EB9s8KivbWQQ3t5W+2da5gmSXo7PLaGixxevvy5cvxySefoGHDhlKltWvXYsCAAfIbheLRoe5JZ4P167ZovPL1ZpTKnwvL+7c0fPYVV7D0GfW6mw9PKyA6KKm7hjRYOoxS3/ugrcGKj49H//79MW3aNHmarkjZsmVDz549MXr0aOTObfx1ft/x2ltSZ4P18R9R+GzpfjzdoDSGd67pE+isfHP3CZhihXTXjwZLsQHnQzg0WD5A06iItgbLpVFcXBwOHDiAtLQ0VKxYMUsYK1ffdTZY/b7diu83HceAdlXwcsuKPl2Suk/Q7J9Pw0KpQtRQKTm8DoYGy2tkWhXQ0mAlJycjZ86c2LJlC2rUqKGVYN50RmeD9c+p6/DX/hh88lgtPFq3pDdY3Hk5efmETZlCuuvHFSxlhprPgdBg+YxOi4JaGiyhTIUKFeQ3B2vVqqWFUL50QmeD1eqTZThwNg5f/asBmlQs6AueTD/E6lOFihXS3YDo3j8aLMUuKB/CocHyAZpGRbQ1WNOnT8e8efMwe/Zs5M+fXyPJjHdFZ4NVY8gf8g3Cxf2ao0Kh69+Z9DbpPkGzf96OCPXyU0P1NPEmIhosb2jpl1dbg1W7dm3s379frlKUKVMm3d6rTZs26afmbT3S1WBdTriGmkMXyt7uHNYOuXNk80lLTl4+YVOmkO76cQVLmaHmcyA0WD6j06KgtgZr2LBhmQo0ZMgQLQTMrBO6Gqz9Zy6j9ZgVCM+RDduHtfNZR90naPbP56GhTEFqqIwUPgVCg+UTNm0KaWuwtFHIREd0NVh/7YvBP/+7DpUK58Givs19JsTJy2d0ShTUXT+uYCkxzEwFQYNlCp/jC9NgOV7CO3dAV4M17+9jGPDdNjStVBCzejbwWUHdJ2j2z+ehoUxBaqiMFD4FQoPlEzZtCmlrsFJSUjB27Fh8++23OHr0KJKSkm4R7fz589qIeKeO6GawDpy9gqhTl3Hw7BV8vHAvutQtiY8f8/0tUU5ezr4EdNePK1jOHp+e9DN7f3Y+Hf17oK3BGjx4MKZOnYq+ffti0KBBePfdd3H48GH8+OOPEL977bXXtFfX7AWs2gTWcMRinLqUgNzZQxCXlII+rSrhjTaVfdZRtf753JE7FGT/rCYa+PqoYeCZW9kiV7CspOm8urQ1WOIcrE8//RQdOnSA+PCzOHTU9TPxTcKvv/7aeWp5GbFOBis5JRUV3/3tFgI/vNQYdUrn85LKjeycvHxGp0RB3fXztAKihAgmg9BdQxoskwPE4cW1NVjiW4O7d+9G6dKlUaxYMcyfPx916tTBwYMHIY5wiI2N9Uq6CRMmyG8Ynjx5EtWrV8e4cePQtGnTDOto0aKF/ND07al9+/YyDpGCgoIyLDtq1Cj5QWqRypYtiyNHjtySb+DAgfjwww8Nxa6TwToUE4eWHy9z97tYZE6sGni/Tx95dlWSlW/uhgaQ4pl0148GS/EBaCA8GiwDkDTOoq3BqlKlCr788ks0aNBAGiGxkvXWW29h7ty5ePXVV3HmzBnDsooy3bp1gzBZTZo0weTJk+Xjx127dkkDd3sS+7tu3vN17tw5eaK8KNOjRw+Z/dSpU7cU++233+SHqMXZXeXLl3cbLPGz559/3p03T548EP+MJJ0M1qJdp/H8l3+7u/1ck3IY/FA1IxjumEf3CZr9MzU8lChMDZWQwecgaLB8RqdFQW0NljBTEREReOedd/Ddd9/hqaeekitCYsP7G2+8YXgVSKgsTJpY/Zo4caJb9KpVq6JTp04YOXKkx4EgVrvEvi+x+iVW1jJKoq7Lly9j8eLF7l+LeF9//XX5z5eki8ESB4tOXHYAE5YdQHAQULdMPox5/B6Uyh/mCxZ3GU5epvDZXlh3/QRg3fuYlftn9v5s+wXIADwS0NZg3d7zdevWYdWqVahYsSIefvhhj2BcGcRKVFhYmPzsTufOnd3l+vTpI/d1ZfQo8PbKa9asiUaNGmHKlCkZtnv69GmULFkSM2fORNeuXW8xWImJiXI1rFSpUnjsscfk48Ps2bMbit/sBazCzS8pORXNRi2Vm9tFeqN1ZfRpXclQ/z1lUqF/nmI083v2zww9NcpSQzV08DUKrmD5Sk6PclnGYPkqV3R0NEqUKCHNWePGjd3VjBgxQhqiqKioTKtev369XAETBq9+/foZ5hX7rsS+KtFWzpw53XnEMRNi5SxfvnwQ9bz99tvo2LGjfNSYURJmTPxzJWGwhDGLiYmRq3neJnFzWLRoEdq0aYPQ0FBvi1uSX3zQ+YFPV7nr+vSJu/FgjaKW1K1C/yzpyB0qYf/8STcwdVPDwHD2VyuZ6SfuzwULFpT7gX25P/srZtZrHQFtDVbx4sUhNpuLf82bN4fYk+VLchms1atXy1UoVxo+fDhmzZqFPXv2ZFrtCy+8AFF2+/btd8x31113SRMzfvz4TOv6/vvv0aVLF2mYChQokC7v0KFDkdEngsQbk2IVzolp54UgTNkT4g79nXuSUSSXE3vCmEmABEjgBoH4+Hj5xIIGS99Roa3B+uabb+Tju2XLlmHv3r0oUqSINFouwyX2UBlJZh4RigtIvMH43nvvQTxSzCitXLkSzZo1k48bxUb4zNKJEyfko0RxzIRYFbs96biC9eXao3h//h5kzxaMIR3uwuP1ShqRzVAerg4YwqRsJt31E+B172NW7h9XsJS9tVgWmLYG62ZCYo/T0qVL8euvv8q3CFNTUyFOejeahJmpW7eufIvQlapVqyYf12W2yX3GjBno3bs3hDHKaMVJ1CXeKtyxYwf+/vvGG3J3ikvE/9BDD8mjGzJ6e/H2cjrswXrvl12YtuoQnm9aDu92MPfW4O18uL/F6BWgZj7d9XMZrAULFkAc8WLXY3p/qq+7htyD5c/Ro37dWhusK1eu4K+//nKvZG3evBnCGImVLLG/yWhyHdMwadIk92b1L774Ajt37kSZMmXQvXt3uU/rdrMljocQP58zZ06GTQkDJFa4PvnkE2nEbk5r1qyRK1UtW7ZEZGQkNmzYIN9+rFevHn766SdDoetgsP4182/8ufs03u9YHd0alTXUb6OZsvLN3SgjlfPprh8Nlsqjz1hsNFjGOOmaS1uDJVadtm3bhho1asjHguIxnDA8efPm9UlLsXolNqOLoxZEncKgiTpFEvWLIxXEipUriceSYt/XwoUL5f6qjJJ4q1AcwSDqFCbq5rRp0ya89NJLco+XePQnjNyTTz6JN9980/B+Kh0MVruxKxB1+jJmPHsvWvMXMssAACAASURBVFQp7JN2dyqk+wTN/lk6XGypjBragt2yRmmwLEPpyIq0NVj58+eXp6W3bt3avdnd6L4rRyqZQdBON1hpaWmoNvgPXL2WgqX9W6BcwYzPEPNVL05evpJTo5zu+nEFS41xZiYKGiwz9JxfVluDJaQRK1hik7vY7C42kwcHB8vHg+Kx2+2P5JwvZfoeON1guT6PI74qFPX+g3Kju5VJ9wma/bNytNhTFzW0h7tVrdJgWUXSmfVobbBulmTjxo347LPPMHv2bK83uTtTWsDJBuv0pQQ8/NlfOH0pETVLROKXV++zXAZOXpYjDWiFuuvHFayADie/NEaD5ResjqlUW4MlNrSL1SvxT6xeic/QiGMQxH4psYIlvk2oe3KywRr5225MXn4Q5Qvlxowe9VG6gPXneOk+QbN/zr/CqaGzNaTBcrZ+ZqPX1mBly5YNtWvXdp99JTakZ7XTcp1qsBKupaDRyMW4EH8NU7rVRdvq1pzcfvvFwsnL7O3D3vK668cVLHvHlxWt02BZQdG5dWhrsIS5yGqG6vZh6CSDFXMlEQfPxuHesvnw3q+7MH3VYZTImwsr3myJEPGFZz8k3Sdo9s8PgybAVVLDAAO3uDkaLIuBOqw6bQ2W0OHixYv47rvvcODAAfmRZPFmoTj+QJzqLs6n0j05yWA9NWUt1hw8hwbl8mPdofNSmk8eq4VH61p3cjtXsPQa8bqbD65gOX+80mA5X0MzPdDWYIk3CFu1aiXPvTp8+LD8KHP58uUxaNAgeRL6l19+aYabI8o6xWCdvZyIe4f/eQtTfxwsSoPliGFrOEgaLMOolM2ou4Y0WMoOvYAEpq3BEudf1alTRx4OGh4ejq1bt0qDJT68LD6wKUyX7skpBuvbDcfw5vfb3HJ0qFkMn3WtLc8x82fKyjd3f3INVN2668cVrECNJP+1Q4PlP7ZOqFlbgyVORhePAytUqHCLwRKrV+KE9YSEBCfoYypG1Q1Wamoanpu5Acuizsp+dq5dAhUL50H3RmUQnjPUVN+NFNZ9gmb/jIwCtfNQQ7X18RQdDZYnQnr/XluDJfZZ/f777/JNwptXsMSna3r27Iljx47prSzUPwdr45HzeHTiGqmDWKxa8FpTVC0WETBdOHkFDLVfGtJdP65g+WXYBLRSGqyA4lauMW0NVq9evXD27Fl8++23cnO72JMVEhKCTp06yW8Ijhs3TjkxrA5I9RWs937ZhWmrDqFA7uz4rGsdNKpQwGoEmdan+wTN/gV0OPmlMWroF6wBq5QGK2ColWxIW4MlzIU4THTHjh3ykNHixYvj1KlTaNSoERYsWIDcua39rp2K6qpssMTjwSYfLcHJ2AS/nnWVmS6cvFQctcZj0l0/rmAZHwuq5qTBUlWZwMSlpcESg7pt27aYOHEioqOj5V6s1NRUueldbH7PKkllg7Xv9GW0GbsCuUJDsHlwG+QMDQm4LLpP0OxfwIeU5Q1SQ8uRBrRCGqyA4lauMS0NlqBcqFAh+cZgpUqVlIMeqIBUNlgLtp/ES19tQq1SefHTy00CheSWdjh52YLdskZ1148rWJYNFdsqosGyDb0SDWtrsPr164fQ0FB8+OGHSoC2IwiVDdb4xfvwyaK9eLROSXzyeC078ED3CZr9s2VYWdooNbQUZ8Aro8EKOHKlGtTWYL366qvyMNGKFSuiXr166fZcjRkzRikh/BGMygarz5zN+GlLNN58oApealHRH933WCcnL4+IlM6gu35cwVJ6+BkKjgbLECZtM2lrsFq2bHlH0cQBlkuWLNFWVFfHVDZYHT5diZ3Rl2zb4M7Jy/nDnwaLGqpOgAZLdYX8G5+2Bsu/2JxRu6oGS7xBWG3I70i4lool/ZqjfKE8tgDVfYJm/2wZVpY2Sg0txRnwymiwAo5cqQZpsJSSw9pgVDVYx87Ho+mopQgNCcLu9x5AtpBgaztusDZOXgZBKZpNd/24yqrowPMiLBosL2BpmJUGS0NRVX9EuGLvWXSftl5+FufPvs1tU0D3CZr9s21oWdYwNbQMpS0V0WDZgl2ZRmmwlJHC+kBUXcGau+EoBn6/Hc0rF8LM5+pb33GDNXLyMghK0Wy668cVLEUHnhdh0WB5AUvDrDRYGoqq+grW2EV78Z/F+/BU/VIY+cjdtimg+wTN/tk2tCxrmBpahtKWimiwbMGuTKM0WMpIYX0gqq5gDfxuG+b+fQx921TGa63sOwiWk5f1Yy6QNequH1ewAjma/NMWDZZ/uDqlVhospyjlQ5yqGqxu/12HlftiMKrL3Xi8XikfemZNEd0naPbPmnFiZy3U0E765tumwTLP0Mk10GA5WT0PsatqsFqPWY79Z65gds8GuK9SQdsU4ORlG3pLGtZdP65gWTJMbK2EBstW/LY3ToNluwT+C0BFg5WWloYaQ/5AXFIKFvdrjgo2nYHFyct/4y5QNdNgBYq0/9rRXUMaLP+NHSfUTIPlBJV8jFFFg3Up4RruHrpQ9mjXe+0Qlj2bj70zXywr39zN07O/Bt314x8B9o8xsxHQYJkl6OzyNFjO1i/T6FU0WFGnLqPduBWIzBWKrUPa2kpf9wma/bN1eFnSODW0BKNtldBg2YZeiYZpsJSQwT9BqGiwFu48hV6zNuKuouH4/fVm/um4wVo5eRkEpWg23fXjCpaiA8+LsGiwvIClYVYaLIOiTpgwAaNHj8bJkydRvXp1jBs3Dk2bNs2wdIsWLbB8+fJ0v2vfvj3mz58vf96jRw/MnDnzljwNGjTA2rVr3T9LTExE//798c033+Dq1ato1aoVRBwlS5Y0FLVKBmvdwXMY+dse7Iq+hKSUVLSpVgRfdK9nqB/+yqT7BM3++WvkBK5eahg41v5oiQbLH1SdUycNlgGt5s6di27duklz06RJE0yePBlTp07Frl27ULp06XQ1nD9/HklJSe6fnzt3DrVq1ZJlhLFyGazTp09j+vTp7nzZs2dH/vz53f//xRdfxC+//IIZM2agQIEC6NevH0TdGzduREhIiMfIVTFYYmP7A+NWIur0ZRlzg3L5MbxzTfmpHDsTJy876ZtvW3f9BCHd+5iV+2f2/mz+CmIN/iZAg2WAsFhZqlOnDiZOnOjOXbVqVXTq1AkjR470WINY7Ro8eLBc/cqdO7fbYF28eBE//vhjhuVjY2NRqFAhzJo1C0888YTMEx0djVKlSmHBggVo166dx3bNXsBW3fyWRp3Bs9M3ICgI+OnlJqhZIhJB4v/YnKzqn83duGPz7J+qyhiPixoaZ6ViTq5gqahK4GKiwfLAWqxEhYWFYd68eejcubM7d58+fbBly5YMHwXeXmXNmjXRqFEjTJkyxf0rsZIlzJVYtcqbNy+aN2+O4cOHo3DhwjLPkiVL5CNBsWKVL18+dzmxEiaM3bBhw9JFLh4pin+uJAyWMGQxMTGIiIjwelSJm8OiRYvQpk0bhIaGel3eVaD3V5uxeM9Z9GxSBm89UMXneqwuaFX/rI7LqvrYP6tI2lcPNbSPvRUtZ6afuD8XLFgQ4o9pX+7PVsTHOvxLgAbLA1+xalSiRAmsWrUKjRs3duceMWKE3EMVFRWVaQ3r16+HWAFbt24d6te/8WFj8dgxT548KFOmDA4dOoRBgwYhOTlZPv7LkSMHvv76azz77LO3GCbRUNu2bVGuXDn5mPL2NHTo0AyNl6hLmES70rBNITifGITXqiejgvc+z66w2S4JkAAJ+I1AfHw8unbtSoPlN8L2V0yDZdBgrV69Wq5CuZJYbRKP7/bs2ZNpDS+88AJE2e3bt2eaTzw+FGZrzpw5eOSRR+5osMRqUoUKFTBp0qR09am4gpWYnIq73/sTqWnA6jebo1B4DvtH/f9HwNUBZaTwKRDd9RNQdO9jVu4fV7B8uuwdVYgGy4NcZh4Rir9QihUrhvfeew/ikaKnVKlSJfzrX//CwIEDfXpEeHv9KuzB2n/mMlqPWYHc2UOwY1g7JfZeuThxf4unEan273XXz2WwxJ5L8Qaymcf0qiqpu4bcg6XqyAtMXDRYBjiLR3x169aVbxG6UrVq1dCxY8dMN7mLt/969+6NEydOyLcAM0viTUPxKFLs0+revbtcNhab3GfPno3HH39cFhWrXOKIBidtcv9z12n868u/Ub14BOa/lvGxFgYk8EuWrHxz9wvQAFequ340WAEeUH5ojgbLD1AdVCUNlgGxXMc0iMdyrs3qX3zxBXbu3Ckf6wlDJMzR7W8UinOyxM/FY7+b05UrVyD2Sz366KNyhevw4cN45513cPToUezevRvh4eEyuzim4ddff5XHNIjjG8SZWMKIOemYhqkrD+KD+bvR4e5i+LxrHQO0A5dF9wma/QvcWPJXS9TQX2QDUy8NVmA4q9oKDZZBZcTq1ahRo+QqUo0aNTB27Fg0a3b9JHJxsGjZsmWlEXKlvXv3okqVKli4cKF8C+/mJA4NFW8Cbt68GeKoBmGyWrZsiffff1++9edKCQkJGDBggNyPdfNBozfnySx8FR4Rvvu/7fhq3VG80rIi+rdT5w1Crg4YHPgKZ9PdfHCMKjz4DIZGg2UQlKbZaLA0FVZ0SwWD9fTUtVi1/xw+fqwWutQ1dgJ9oCTRfYJm/wI1kvzXDjX0H9tA1EyDFQjK6rZBg6WuNqYjU8Fg3ffREhy/cBXf9W6EemVvnFJvunMWVMDJywKINlahu35cwbJxcFnUNA2WRSAdWg0NlkOFMxK23QYrOSUVVQb9jpTUNKx/pxUKR+Q0EnbA8ug+QbN/ARtKfmuIGvoNbUAqpsEKCGZlG6HBUlYa84HZbbCOX4jHfR8tRfaQYOx5/wEEB9v/eZybqXLyMj/G7KxBd/24gmXn6LKmbRosazg6tRYaLKcqZyBuuw3W2oPn8OSUtShXMDeW9m9hIOLAZtF9gmb/Ajue/NEaNfQH1cDVSYMVONYqtkSDpaIqFsVkt8H6buNx9J+3FU0rFcSsng0s6pV11XDyso6lHTXprh9XsOwYVda2SYNlLU+n1UaD5TTFvIjXboM17s+9GPfnPjxVvxRGPnK3F5EHJqvuEzT7F5hx5M9WqKE/6fq/bhos/zNWuQUaLJXVMRmb3QZLrF6JVawB7arg5ZYVTfbG+uKcvKxnGsgaddePK1iBHE3+aYsGyz9cnVIrDZZTlPIhTrsN1pNT1mDtwfP4z5P3oOM9JXzogX+L6D5Bs3/+HT+BqJ0aBoKy/9qgwfIfWyfUTIPlBJV8jNFug+U6A+v7Fxuhbhm1zsDi6oCPg0qhYrqbD45RhQabj6HQYPkITpNiNFiaCJlRN+wyWDFXEpEnRzZUH/KHPANr7dutUDRSrTOwOHk5f+DTYFFD1QnQYKmukH/jo8HyL19ba7fDYB07H4+mo5aiarEI7D55CUFBwL4PHkS2kGBbWWTUuO4TNPun3JDzOiBq6DUypQrQYCklR8CDocEKOPLANWiHwRqzMAqfLtnv7mTBPDnw979bB67TXrTEycsLWApm1V0/rrIqOOi8DIkGy0tgmmWnwdJM0Ju7Y4fB+uDXXZj61yF3GNWKRWBBn6ZKUtZ9gmb/lBx2XgVFDb3CpVxmGizlJAloQDRYAcUd2MbsMFivfbMZP2+Ndne0RZVCmPFs/cB23GBrnLwMglI0m+76cQVL0YHnRVg0WF7A0jArDZaGorq6ZIfB+sf4ldhx4pKb6uP1SmJUl1pKUtZ9gmb/lBx2XgVFDb3CpVxmGizlJAloQDRYAcUd2MYCbbDS0tJQY8gfiEtKcXf0lZYV0b9dlcB23GBrnLwMglI0m+76cQVL0YHnRVg0WF7A0jArDZaGotq1gnUqNgENRy6+heh7Hauje6OySlLWfYJm/5Qcdl4FRQ29wqVcZhos5SQJaEA0WAHFHdjGAr2CtXp/DLpOXXdLJyf9sw4eqFEssB032BonL4OgFM2mu35cwVJ04HkRFg2WF7A0zEqDpaGodq1gfbvhGN78ftstRFU9xZ2Tl/MHPg0WNVSdAA2W6gr5Nz4aLP/ytbX2QK9gfb50P0b/EYXC4Tlw5nKi7PvKN1uiVP4wWzncqXHdJ2j2T8lh51VQ1NArXMplpsFSTpKABkSDFVDcgW0s0AZr6M87MWP1YbSuWhh/7j4jO7vn/QeQMzQksB032BonL4OgFM2mu35cZVV04HkRFg2WF7A0zEqDpaGori4F2mC9/NUmzN9+Eu+0vws/bDohvz+o6hlYnLycP/BpsKih6gRosFRXyL/x0WD5l6+ttQfaYD0+eQ3WHzqP8U/Vxj/uvr6xPUh8jFDRpPsEzf4pOvC8CIsaegFLwaw0WAqKEsCQaLACCDvQTQXaYN3/8TIcjInDnF4N0bB8gUB31+v2OHl5jUypArrrx1VWpYabT8HQYPmETZtCNFjaSJm+I4E2WDWH/IHLiclY3K85KhTKozxZ3Sdo9k/5IegxQGroEZHSGWiwlJbH78HRYPkdsX0NBNJgJVxLwV2Dfped3Ta0LSJyhtrXcYMtc/IyCErRbLrrxxUsRQeeF2HRYHkBS8OsNFgaiurqUiAN1rHz8Wg6aimyZwtG1PsPKL33ysVH9wma/XP+xU0Nna0hDZaz9TMbPQ2WWYIKlw+kwdp09AIembAaJfLmwqq37leYyo3QOHk5QqY7Bqm7flzBcvb49KSf2fuz8+no3wMaLIMaT5gwAaNHj8bJkydRvXp1jBs3Dk2bNs2wdIsWLbB8+fJ0v2vfvj3mz58PMTH8+9//xoIFC3Dw4EFERkaidevW+PDDD1G8eHF3ubJly+LIkSO31DNw4ECZz0gyewF7M4H9sfMUXpi1EfeUyosfX25iJDzb83jTP9uD9SEA9s8HaIoVoYaKCeJlOFzB8hKYZtlpsAwIOnfuXHTr1g3CZDVp0gSTJ0/G1KlTsWvXLpQuXTpdDefPn0dSUpL75+fOnUOtWrVkmR49eiA2NhZdunTB888/L39+4cIFvP7660hOTsbff/99i8Hq2bOnzOdKefLkgfhnJAXSYM1eewT//nEH2lQrgi+61zMSnu15OHnZLoGpAHTXz9MKiCl4ihTWXUMaLEUGmk1h0GAZAN+gQQPUqVMHEydOdOeuWrUqOnXqhJEjR3qsQax2DR48WK5+5c6dO8P8GzZsQP369eWKlcu0iRUsYbzEP19SIA3WJwujMH7JfjzdoDSGd67pS7gBL5OVb+4Bh+2HBnXXjwbLD4MmwFXSYAUYuGLN0WB5EESsRIWFhWHevHno3LmzO3efPn2wZcuWDB8F3l5lzZo10ahRI0yZMuWOrf35559o27YtLl68iIiICJlPGKzExES5GlaqVCk89thjGDBgALJnz25oGPnTYEWduozYq9dQv1x+GUufOZvx05ZovP3gXXiheQVD8dmdSfcJmv2ze4SZb58ammdoZw00WHbSt79tGiwPGkRHR6NEiRJYtWoVGjdu7M49YsQIzJw5E1FRUZnWsH79eogVsHXr1skVqoxSQkIC7rvvPtx1112YPXu2O8vYsWPlylm+fPkg6nn77bfRsWNH+agxoyTMmPjnSsJgCWMWExPjNm3eDDlxc1i0aBHatGmD0NDrxy58s+EYCubOgZe+2SL//9q3WqBA7ux4bMo6bDkWi/FP1sID1Yt404xteTPqn23B+KFh9s8PUANcJTUMMHCLm8tMP3F/LliwoNwy4vqj2uLmWZ3NBGiwDBqs1atXy1UoVxo+fDhmzZqFPXv2ZFrDCy+8AFF2+/btGeYTF6BYmTp69CiWLVuW6YX2/fffy71bwjAVKJD+pPShQ4di2LBh6dr5+uuv5Sqc2XQiDhi1Ldst1Qy8OxnFcwPvbgjBleQgDLg7GSUzfgpqtnmWJwESIAFtCMTHx6Nr1640WNoomr4jNFgexDXziFBcQMWKFcN7770H8Ujx9iTM1eOPPy7fJFyyZEmGpunmMidOnEDJkiWxdu1auSp2e/L3CtaiXWfcK1eutr99vj4qFcmD2h8skT/a9G5LhDvgkFERK1cHnH1n010/jlFnj09P+nEFy/n6euoBDZYnQoA0M3Xr1pVvEbpStWrV5OO6zDa5z5gxA71794YwRrevOLnM1b59+7B06VIUKlTIYyS//vorHnrooVs2wmdWyOo9WLPWHsGgH3fc0uSsnvVRME8OPPiflcgXForNg9t67IcqGbi/RRUlfItDd/1cE7Q4zkUc8eJ6TO8bLTVL6a4h92CpOe4CFRUNlgHSrmMaJk2a5N6s/sUXX2Dnzp0oU6YMunfvLvdp3W62xDlZ4udz5sy5pRVxHMOjjz6KTZs2QZimIkVu7FnKnz+/3MS+Zs0auVLVsmVLeU6WeMvwjTfeQL169fDTTz8ZiBqw2mCNWRiFT5fsR+faJSA2ue86eQmT/llHntouzsCqVTISP71yn6HYVMiUlW/uKvA3G4Pu+tFgmR0h9penwbJfAzsjoMEySF+sXo0aNUoetVCjRg2IDejNmjWTpcXBouKNP7Fi5Up79+5FlSpVsHDhQrlJ/OZ0+PBhlCtXLsOWxWqWqE+Yr5deeknu8RKP/oSRe/LJJ/Hmm28a3k9ltcEa+N02zP37GPq2qYwNh89j5b4YfPJYLZyPS8LwBbvxj7uL4bOudQwStT+b7hM0+2f/GDMbATU0S9De8jRY9vK3u3UaLLsV8GP7VhusHtPXY1nUWXz0aE35v7/tOIX3O1bH5mMX8cOmE3ipRQW8+cBdfuyRtVVz8rKWZ6Br010/rmAFekRZ3x4NlvVMnVQjDZaT1PIyVqsNVvv/rJSPBac/ey9+3XoS3286jlfvr4j//nUI8UkpmNurIRqUT/92o5dhByy77hM0+xewoeS3hqih39AGpGIarIBgVrYRGixlpTEfmNUGq94HixBzJQkLXmuKORuO4ss1R1A4PAfOXE5EuYK5saRfc7kfyymJk5dTlMo4Tt314wqWs8enJ/3M3p+dT0f/HtBgaayx2Qv45gkMwSGo9O5vktbf/24tV60mLjvgpjegXRW83LKio2jqPkGzf44ajhkGSw2drSFXsJytn9noabDMElS4vJUG62xcMhp/uATZgoOw94MHMWHZfny8cK+79/99ph5aVXXGCe6uoDl5KTx4DYSmu36eVkAMIFI+i+4a0mApPwT9GiANll/x2lu5lQZr56k4dPp8FYpH5sTqt1th+qpDGPbLLncHf3ipMeqUzmdvh71sPSvf3L1EpWR23fWjwVJy2HkVFA2WV7i0y0yDpZ2kNzpkpcFauvccet101tW3G47hze+3uRtb1r8FyhZ01jdydJ+g2T/nX9zU0Nka0mA5Wz+z0dNgmSWocHkrDdZP206j/7ytaFa5EL58rj5+3RaNV77e7O791iFtEZnr+gehnZI4eTlFqYzj1F0/rmA5e3x60s/s/dn5dPTvAQ2WxhqbvYBvnsC+Wn8cQ3/ZhQ41i+Hzp+tgadQZPDt9g6Qn9mXtG/6go94g9HTz02FY6G5AdO8fx6jzr0KuYDlfQzM9oMEyQ0/xslYarEkrDuOTRXvx5L2l8OGjd2P9ofN4fPIaSaBQeA5seLe14jTSh6f7BM3+OW5IpguYGjpbQxosZ+tnNnoaLLMEFS5vpcH6eNF+TF5xEP+6rxz+/Y9q2HEiFv8Y/5fsfZUi4fjjjeufDXJS4uTlJLWynkHmCpazx6cn/czen51PR/8e0GBprLHZC/hmAzLk1z34et1RvNG6Mvq0roTDMXFo8fEySa9h+fyY06uR40jSYDlOslsC1l0/TxO0s9W7Hr3uGnIFS4dR6nsfaLB8Z6d8SSsNVt/vduCXrdEY9I9q6HlfOZy5nID6wxdLBg/WKIqJ/6yrPI/bA8zKN3fHiZVBwLrrl9UNiO5j1Oz9WQc+uveBBktjhc1ewDdPYL1mb8bSqLMY1eVuPF6vFOKTklFt8B+S3iO1S2DME/c4jqTuEzT757ghmS5gauhsDbmC5Wz9zEZPg2WWoMLlrTRYT03dgL+PXMDEp+vgwZrFkJqahvLvLJC9f6p+KYx85G6FSWQcGicvx0l2S8C668cVLGePT0/6mb0/O5+O/j2gwdJYY7MX8M0T2EOfr8GeU5cxu2cD3FepoKRW9q358n97NC6LoQ9XdxxJ3Sdo9s9xQ5IrWM6XzPAfAWbvz5qh0rI7NFhaynq9U2Yv4Jsn6BafrMSJi1fx48tNcE+pvLcYrBealcfb7as6jiQNiOMkMzx5ObtnN6LnGHW2knxE6Gz9zEZPg2WWoMLlrTRYdYcvwaWEZPzZtzkqFs5zi8H6d4eq+FfT8gqTyDg0Tl6Ok4wGy9mScYXuJgJm78+aDQUtu0ODpaWs1q5gPfjgg7hryCKkpgHr32mFwhE5ZQNfrTuCpXvO4LOudZAzNMRxJGmwHCcZDZazJaPBosHSbARn3h0aLI3lNvsXksuANG/VFvd8sESS2vVeO4Rlz6YFNRosZ8uou35CHd37mJX7Z/b+7OyrN2tET4Olsc5mL2DXza/ufffjvtErEBIchP0O/ObgnSTOyjd3HYa97vrRYDl/lHIPlvM1NNMDGiwz9BQva5XBqlyvGR4cvxqRuUKxdUhbxXttPDzdJ2j2z/hYUDUnNVRVGWNx0WAZ46RrLhosXZW18C3C4jUb47Ep61EyXy78NfB+bYhx8nK2lLrrxxUsZ49PT/qZ/QPY+XT07wENlsYam72AXRNYeOX6eG7mJtxVNBy/v+68jzrzEWF7hIaGajfSabCcL6nuGnIFy/lj1EwPaLDM0FO8rFUGK6h0Hbw2dxvql82Pb3s776PONFg0WIpfqncMLysbEKdqdnPcNFg6qOh7H2iwfGenfEmrDFZckVp458eduP+uwpjW417l+200QE5eRkmpmU93/Tw9YlJTFe+i0l1DGizvxoNuuWmwdFP0pv5YZbBO562OEb9F4eFaxfHpU7W1IZaVb+46iKi7fjRYzh+lNFjO19BMD2iw0lm7iQAAIABJREFUzNBTvKxVButAzir4dOkBPN2gNIZ3rql4r42Hp/sEzf4ZHwuq5qSGqipjLC4aLGOcdM1Fg6Wrsha+RbglqAKmrz6CF5qXx9sPOu+bg3eSmJOXswe/7vpxBcvZ49OTfmb/AHY+Hf17QIOlscZmL2DXBPZXUhnM23gCA9pVwcstK2pDTPcJmv1z/lClhs7WkCtYztbPbPQ0WAYJTpgwAaNHj8bJkydRvXp1jBs3Dk2bNs2wdIsWLbB8+fJ0v2vfvj3mz58vf56WloZhw4ZhypQpuHDhAho0aIDPP/9c1u1K4uevvfYafv75Z/mjhx9+GOPHj0fevHkNRW2VwfrtUgn8vvM0hj1cHc80LmuobSdk4uTlBJXuHKPu+nlaAXG2etej111DGiwdRqnvfaDBMsBu7ty56NatG4TJatKkCSZPnoypU6di165dKF26dLoazp8/j6SkJPfPz507h1q1askyPXr0kD//6KOPMHz4cMyYMQOVK1fGBx98gBUrViAqKgrh4eEyj/jI8vHjx6UJE6lXr14oW7YsfvnlFwNRA1YZrHlni+Cv/ecw5vFaeKROSUNtOyFTVr65O0EfTzHqrl9WNyCe9HfC72mwnKCS/2KkwTLAVqwu1alTBxMnTnTnrlq1Kjp16oSRI0d6rEGsdg0ePFiufuXOnVuuXhUvXhyvv/46Bg4cKMsnJiaiSJEi0ni98MIL2L17N6pVq4a1a9fK1S2RxH83atQIe/bsQZUqVTy2a5XBmnasALYej8WUbnXRtnpRj+06JYPuEzT755SReOc4qaGzNaTBcrZ+ZqOnwfJAUKxEhYWFYd68eejcubM7d58+fbBly5YMHwXeXmXNmjWlMXKtRB08eBAVKlTApk2bULv2jWMPOnbsKB//zZw5E9OmTUPfvn1x8eLFW6oTvx87diyeffbZdJELkyb+uZIwWKVKlUJMTAwiIiK8Hivi5rBo0SL8Z18EDsbEY/Zz9dCgXH6v61G1gKt/bdq00fakc6Ef+6fqCPQcF8eoZ0Yq58hMP3F/LliwIGJjY326P6vcb8Z2nQANloeREB0d/X/tnQmUVMX1h6/IooAYHYMoIggiihBXEDHihizGXeNCDNEoaKIIuBBwIUIMUdxFjQpBRYIoyCZMADEhKARRiAgYIS5hc8dIiKhEzf/86p/XZ2h6Zrrp9/ptX53DOcxMv6p7v1vd79e37quyxo0b2/z5861jx46ZVw8bNswJIS3pVdUWLVrkMlAvv/yytW/f3r10wYIFbqlx/fr1LpPlNS0Brl692mbNmmXqX8uHq1at2qp7LSdKXA0aNGibYW+55RZX15Xdxo0b50Ti9rbBi3e0jVt2sOvafm1N6m9vL1wHAQhAAAIegc2bN1uPHj0QWAmeEgisPAWWRJGyUF5T/dSTTz7pluuqalru07XLli3LvMwTWBJve+21V+b3vXr1srVr19rMmTOdwMol4Fq2bGmXXnqpDRw4cJthg8pgDVpcxzZv+cbm9Pu+NS3bfqEWtfcR2YGoRaQwe5IeP9FIuo9p9o8MVmHv9zi+GoFVTdSKWSLUNxQJqKFDh5qWFL0W1BJhtit+1GBNn1Fu/RfWdF0vvqmzldWvE8d5ntNm6lviHcqkx88TWOXl5aYnkDmwO37zlRqs+MXMT4sRWHnQ1BLfEUcc4Z4i9JoK0FUzVVWRu5b4rrjiCrcUWFZWlrnWK3Lv37+/DRgwwP1eQq5hw4bbFLlXXFrU/zt06FDSIveJ08pt0Cv/L7BW3trN6tTcMQ9i8XhJ0m/Q+BePeViVlcQw3jFEYMU7fsVaj8DKg6C3TcPDDz+cKVYfOXKkrVixwpo2bWo9e/Z0dVrZYkv7ZOn348eP32YUPS2o1z/22GOmZT8tCc6dO3ebbRq0jKhtIdRUo6XxSrlNw9jJ5TZkSU2rXbOGrbq1ex604vMSbl7xiVUuS5MePzJY8Z6f1cWv2BWG+NNJvgcIrDxjrOzV8OHD3VYLbdq0cU/yderUyV2tjUW1P5UyVl5Tcbq2Upg9e7Z7iiu7eRuNSjxV3GhUfXtN+2llbzT6wAMPlHSj0VETyu3212taWb3atvjmbf3IE18kX5b0GzT+RXLaFWQUMSwIV+ReTAYrciEpqUEIrJLiLu1gxX5D0ofDiPHldv+KmtasrK7Nvf6E0joQ8GjcvAIGHHD3SY9fdRmQgPGWpPukxxCBVZJpFNlBEFiRDU3xhhUrsAZPWWZjFq5xhrRp3MCm98l9NFDxlobTQ5o/3MMh7u+oSY8fAsvf+RJGbwisMKhHZ0wEVnRi4bslxQqsvk8tsalL33d2Hd28zJ7q3cF3G8PsMOk3aPwLc3b5MzYx9IdjWL0gsMIiH41xEVjRiEMgVhQrsJ5ZtNoGTFrubDu59Z42sueRgdgZVqfcvMIi78+4SY8fGSx/5kmYvSCwwqQf/tgIrPBjEJgFxQqsdRs22ffvmOfsO/HAhjb64naB2RpGx0m/QeNfGLPK3zGJob88S90bAqvUxKM1HgIrWvHw1ZpiBZY+HFrePNvZtEf9OvbqTZ19tS/szrh5hR2B4sZPevzIYBU3P6JwNQIrClEIzwYEVnjsAx/ZD4F1zj0z7fVPa9hVJ+xv13VtFbjNpRwg6Tdo/CvlbApmLGIYDNdS9YrAKhXpaI6DwIpmXHyxyg+BNfm5cqvf4kg7sXUj26lWcnZxJzvgyxQLtZOkiw/maKjTy5fBEVi+YIxtJwis2IauesP9EFicg1Y956i+IukCJOn+IbCi+s7K3y4EVv6skvhKBFYSo/o/nxBYVQc36Tdo/Iv/m5sYxjuGCKx4x69Y6xFYxRKM8PUILAQWGcgIv0HzMA2BlQekCL8EgRXh4JTANARWCSCHNQQCC4GFwArr3efPuAgsfziG1QsCKyzy0RgXgRWNOARiBQILgYXACuStVbJOEVglQx3IQAisQLDGplMEVmxCVbihCCwEFgKr8PdNlK5AYEUpGoXbgsAqnFmSrkBgJSmaWb4gsBBYCKx4v8ERWMmNX7Gfz/Emkw7rEVgJjnOxb2A+3OM9OYhfvOMn64lhvGNIBive8SvWegRWsQQjfD0CiwwWGawIv0HzMA2BlQekCL8EgRXh4JTANARWCSCHNQQCC4GFwArr3efPuAgsfziG1QsCKyzy0RgXgRWNOARiBQILgYXACuStVbJOEVglQx3IQAisQLDGplMEVmxCVbihGzdutO985zu2du1aa9CgQcEd6MNh9uzZ1qVLF6tVq1bB10f9AvyLeoSqF8hJnp/ynjma3DmqL8BNmjSxzz77zHbdddd4O4r1OQkgsBI8MdatW+fewDQIQAACEIgmAX0B3meffaJpHFYVRQCBVRS+aF/87bff2nvvvWe77LKL7bDDDgUb633D2t4MWMEDlvgC/CsxcJ+HS3r8hCvpPqbZv//+97+2adMm23vvva1GjRo+vzvoLgoEEFhRiEJEbSi2hiuibmXMwr+oR6hq+5IeP09gaflIy/3bs8wf9QgnPYZJ9y/q8yts+xBYYUcgwuMn/cMB/yI8+fIwLenxQ2DlMQki/pI0zNGIhyBU8xBYoeKP9uBJ/3DAv2jPv+qsS3r8EFjVzYDo/z0NczT6UQjPQgRWeOwjP/JXX31lv/nNb2zQoEFWp06dyNtbqIH4VyixaL0+6fET7aT7iH/Rek9hjb8EEFj+8qQ3CEAAAhCAAAQgYAgsJgEEIAABCEAAAhDwmQACy2egdAcBCEAAAhCAAAQQWMwBCEAAAhCAAAQg4DMBBJbPQOkOAhCAAAQgAAEIILCYA5USeOihh+yOO+6w999/3w4++GC799577dhjj40dsVtuucWGDBmyld177rmnffDBB+532lFZf3/00Uftn//8px111FH24IMPOp+j2ObNm+fisnjxYhebyZMn25lnnpkxNR9/5OfVV19t06ZNc9edfvrpNmLECHd2ZditOv8uvvhie+KJJ7YyUzFbuHBh5nd6Ou26666zp556yr744gs76aSTTPM57CNJ9FTupEmT7M0337Sdd97ZOnbsaLfffru1atWqINvXrFljV155pf3xj390/fTo0cPuvPNOq127dqjhy8e/448/3v785z9vZef5559v48ePz/wuyvPzt7/9renfP/7xD2evPicGDx5s3bt3dz/nM/eiGr9QJ08CB0dgJTCofrj09NNP249//GN3UzrmmGPskUcesVGjRtkbb7xh++67rx9DlKwPCayJEyfanDlzMmPuuOOO9t3vftf9rBvcr3/9a3v88cftgAMOsFtvvdV0k1+5cqU7Zihq7Q9/+IPNnz/fDj/8cDvnnHO2EVj5+KObgc6qlKhU6927tzVr1syee+650N2tzj8JrA8//NAee+yxjK0SFrvvvnvm55/97GfOF8W0rKzMrr32Wvv000+dKFXsw2rdunWzCy64wNq1a2dff/213XjjjbZs2TL3vqpXr54zqzrbv/nmGzv00EPd/L3rrrtsw4YN9pOf/MTOPvtsJ5LDbPn4J4Gl99nQoUMzpkokVjzwOMrzU/NKc2j//fd39kvs6wvPX//6Vye24hy/MOdOEsdGYCUxqj74pIyAbuD6pua1gw46yGVK9C01Tk0Ca8qUKfbaa69tY7ayPToLrF+/fvaLX/zC/V3fQJXhklC5/PLLI+2qzpismMHKx5+//e1v1rp1a5fxUZzV9P+jjz7aZVYqZlPCdj7bP9kjgfXZZ5+5mOZqOlZG4uPJJ580ZUbUdCanDj4vLy+3rl27hu1WZvyPP/7YGjZs6DI6nTp1ckfiVGe7BOipp55qOiNUc1dN2R9x+eijjyJ1pE62f7JVAksCURnxXC1O89OzX+JeIuvcc89NVPwi80aJqSEIrJgGLkizt2zZYnXr1rUJEybYWWedlRmqb9++TqRkp/eDtMWPviWw9OGnb8jaMFWiYtiwYda8eXN75513rEWLFrZkyRI77LDDMsOdccYZbrkseynKD3v87CNbgOTjz+jRo+2aa65xIqVik7/33HOPXXLJJX6aWFRflQksiStlrWTzcccd5zKQEipqWjbTkqAyVrvttltm/EMOOcR9QcheLi7KwCIvfuutt6xly5Yui9WmTZu8bNdy1NSpU23p0qWZ0bWkppu8fD/hhBOKtMq/y7P98wTWihUr3NK8vsgoW/XLX/4yky2O0/xUNlGfk8ogKoOlsoPq5l6c4uffTEhnTwisdMa9Sq/1bb9x48ZuGUo1Il6TKJHg0NJZnJq+8W/evNktS2hpSUuAytToQ16+aAl0/fr1mWyAfNOS2erVq23WrFmRdjVbgCxYsKBafxRHLZ2tWrVqK9/ER+JKO/dHpeUSWFq+rl+/vjVt2tTeffddu/nmm91ym5b/JKDHjRvn/FAmsmLr0qWL7bfffm65OwpNAkNCXuLoxRdfdCblY7vmpup/Zs+evZUb8l1xvfDCC6PgnhNQ2f7JsJEjR7o4NGrUyJYvX+7mm5bbnn/+eWd3HOanBLEyvl9++aWbi4rbKaeckqj4RWISxdwIBFbMAxiE+Z7A0s1aHyJeU5ZAyy4SJ3Fun3/+uctaDRgwwDp06OAEiXzea6+9Mm716tXLLcHMnDkz0q5WJrCq8qcyoaxMyqWXXmoDBw6MjM+5BFa2cSr0l9jSMpnqkCoTKSeffLKL+8MPPxwJ/1SkPmPGDHvppZcyxff52F6Z+FdGb8yYMa7GKwotl3+57JIwPvLII51AVllCHOansvwqVFcW+Nlnn3X1qcrsK8OfS9xXnHtxiV8U5lDcbUBgxT2CAdiftCXCXIj0gadvzddffz1LhP8DFJclwlzxlDi87LLLXB1dHJYI+/Tp42rI9DCFsjley8f2OCwxVeZfrtgp06Xsm1czF6clQs+fzp07u88R1fyxRBjATSmmXSKwYhq4oM1WndIRRxzhniL0mgqjlfKPW5F7NistHenDUN8ktbykQuH+/fu7jJaaBKbqeeJc5F6VP14R8csvv2zt27d3Puv/yubFocg9O556ik5L2noismfPnplC8bFjx9p5553nXq4sl7ZoCLvIXWJC4kMPJsydO9fVX1VsXpF7VbZ7Re56CtTLumrZVHVAYRe5V+dfrs8tLRO2bds2U+gfp/np+SNRpYco7rvvPlfkHtf4BX1fSVv/CKy0RTxPf71tGrScomVC3bxUO6G6JS3HxKlpP6TTTjvNbS+hG5BqsJTOVx2FfJGQkmjUY/+64WmJQje/qG7T8O9//9tUPKymwvy7777bFTaryFk+5uOPCou1jOjVI0lsikUUtmmoyj/5qIcWtD2FxIVqkW644Qa3XKMbs7ethh6Vnz59uqtJ0jWaAxJiYW/T8POf/9wtYapIveLTmnoAQ1sVqFVnu7dNgwrE9fCGivn1BKEK+MPepqE6/95++237/e9/7+qV9thjD7c9hbbQkO+vvPJKZguNKM9PzTfZJ0G1adMmtzR92223uXICZcbjHL84fa7HwVYEVhyiFJKNyl4NHz7cffvXE056wkyPksetqSZFSzGffPKJ+3apTM2vfvUrt1WBmrcxp8RGxY1G5XMUm8RfrifFlMGQoMjHH92UszcafeCBByKx0WhV/mnbEAkJPbGl+heJLLFQPHXD85qKj7X8KzFTcaPRiq8JI7aqKcvVJO4lktTysV2CUmIme6NRLbWF2arzT3WNF110kStul5BWPH7wgx+4pwgr7mMW5fmpOsUXXnjBfS5KGH/ve99zS9MSV3GPX5hzJ4ljI7CSGFV8ggAEIAABCEAgVAIIrFDxMzgEIAABCEAAAkkkgMBKYlTxCQIQgAAEIACBUAkgsELFz+AQgAAEIAABCCSRAAIriVHFJwhAAAIQgAAEQiWAwAoVP4NDAAIQgAAEIJBEAgisJEYVnyAAAQhAAAIQCJUAAitU/AwOAQhAAAIQgEASCSCwkhhVfILAdhI4/vjj7dBDD7V77713O3vw9zJtmnr55ZfbxIkT3Saw2mBU9uXTmjVrZv369XP/aBCAAARKTQCBVWrijAeBCBOImsDSuXs6/1K7uzdv3twdr1KzZs2tCGr3eoko7exesX388cdWr149q1u3bmjEEXmhoWdgCIROAIEVeggwAALRIRCEwNLZeTpCpUaNGgU7quN7dN7e6tWrK722MoFV8GABXIDACgAqXUIgJgQQWDEJFGamh4BEjs4322mnnWzUqFFWu3Ztu+KKK9whx2o64Hi//fbbarlM2ZvddtvN/vSnP5mu987z0wG0AwcOtDfffNMd2q2DaXXg8TXXXGPr169358D97ne/y2R5dK13BuPYsWPd4bs6vFZn/XnnzG3ZssVuuukmd2ivxtXrdcC0rlXzBI+uHzBggK1atcr+/ve/O5uzmw7d1pmBS5cudWfR6TxFHcatLJXO5nviiScyl+gwavleseU6t1Dn2olVtriR/Tq8XAda6ww/9Td69Gh3PuVll13mDhsWd9ndokWLzDB6vfrTQed77723s/HGG2/MZNL0N/Xz4YcfWllZmZ177rl2//33Ox7yr2LTkqfaggULXFw0prJyZ511ljtwXBk3NdmuM+90gPW0adOsQYMGNmjQIOvTp0+mu8rGTc87BU8hEG0CCKxoxwfrUkhAN2bVGkkE9ejRw/7yl784sTFr1ix3oGwhAksHW995551OQJ133nnWuHFj04HAt912mztsVzd2CRwdVqumsSXAdHOXsHr11Vetd+/eriarV69e7jU/+tGPnA3qQ4Jj8uTJTnAtW7bMWrZs6QSWrmnXrp3LPkl07LPPPhnx4IVUAu+AAw5wvkk4SARqjCuvvNIJmo0bNzqh8uijjzohIrEnMVSxSezpAOjBgwfbypUr3Z/q16/v/uUSWPL/7rvvdnVc8vm1115zS48Sgvvuu6/99Kc/dQdea2lSTczFTXYce+yx9vbbbzvfZLOEnGrDxErC9eCDD7YPPvjAiUX5oQOLDznkEPd6j12jRo0cp44dOzrRKoGrpcyrrrrKvVaHPnsCS9ffcMMNdvbZZzs7+vfv7+zSHKhq3BS+ZXAZApEkgMCKZFgwKs0EJHK0rPbiiy9mMLRv395OPPFEJ2oKEVhz5syxk046yfWja5UFkUiQqFBTZkz9KdPlCayPPvrIZWu8jJUyLcqivPHGG+5aiah169Y5ceW1zp07m2wcNmyYE1iXXHKJEy8SDZU1ZYGeffZZl6XxxnrooYec8JG40pKihJ3+ZWeuKvZZ2RJhLoElIShho7Zw4UKX1VMGT8JKTUJJtn/xxRfu506dOln37t0dN695mbn33nvPibVHHnnEli9fbrVq1drG1VxLhD179rSdd97ZXee1l156yY477jj7/PPPXeZS1x100EEZoafXXXDBBfavf/3LysvLqx03ze8ffIdAVAggsKISCeyAwP8ISGApG/Lggw9mmKjQW5kgLUUVIrAklrysj7IjypToJu41ZWG0BLZkyZKMwJL40jhemzp1qlv2+vLLL23SpEkuo+MtZXmv+eqrr1ym5emnn3YCS0/+6fWecMoVXL1+1113zWRt9Bplf5RdUs2VMkp+C6xnnnnGfvjDHzpz3n33XSc0Fy1a5LJtalpilZCVwNOynPz89ttvXfbMaxK/8k0cN2zYYMccc4xp6a9bt252yimn2GmnnZZZPswlsBTbt956aytBpus3b97sRKyEla6T6FNmzmv33Xef4yG7165dW+W4vJkgAIHwCSCwwo8BFkBgKwK5Cs3PPPNMt3Ql8bJmzRpXPyRRdNhhh7lrtczUsGHDbWqwtLWBrlPLlenRUtyUKVNctklNY1clsLQ0pSVCZbgqig5dq2U5LYHlW3Su5UnVjVUUc7JDPsnHJk2a+C6wtJwplmq5hKpX0+VxU6ZpyJAhTjxmN3FSlk3Zrueff96ULZwwYYKrNVPtlTJauQSWBJSW+a6++upt+pSoVM1dZQJLIuudd95x11U1Lm8pCEAgfAIIrPBjgAUQKEhg6caqmqoZM2a4jImabvBdunTxRWAp66VMite0PKYsln6ngvVWrVrZvHnzXE1SrpavwKpsiVBLkiqez3eJcNy4cS5jtmnTpq3MybVEWKjAUnbqwAMPdMuI+TTVgen1qmM7/PDDXY2ZbLv22mszl0ugqlbrhRdeqLRL2d66dWu3HOi1Cy+80GXWKv7O+1v2uPnYymsgAIFgCSCwguVL7xAomEB1GSx1qNohZUj0VNwnn3ziCtW11JX9FOH2ZLAkDlSULWGgLJn+f9ddd7mf1S666CKbP3+++52yTRpfT+W1bdvWCb58BZZX5K6aJy1dSiToaT6vyF1j5bNEqCfyJISUQVLNl8Sn/vkhsFRcfuqpp7qnBrW0KNH3+uuvu0J1Pe0oX7VkeNRRR7kxlY1TXZaW8LSkK9GrLJhqy/RwgZ4Y1PV6+EB+i62WIVWHJpE8YsQIx1i2K3YaVxk3/a1v375OVHft2rXacQuedFwAAQj4TgCB5TtSOoRAcQTyEVi6IatGRzVLyigNHz7ctwyWaoRUd6TMkJYBJaxUvO7VU/3nP/9x4mLMmDFuqwcJCQk+LaVJZOUrsESpqm0a8hVYep2eeNTynGqiqtqmodAMlvqWyBo6dKh7slOiVhkqCUGJIy2v6uEBxUNCS/6LjfdggQrpxU/iUXVq3jYNeipS4klPiOp32hbi/PPPd08NegJL8dVS7PTp022XXXZxhfYSWWrVjVvcDORqCEDADwIILD8o0gcEIAABHwmwQamPMOkKAiERQGCFBJ5hIQABCFRGAIHF3IBA/AkgsOIfQzyAAAQSRgCBlbCA4k4qCSCwUhl2nIYABCAAAQhAIEgCCKwg6dI3BCAAAQhAAAKpJIDASmXYcRoCEIAABCAAgSAJILCCpEvfEIAABCAAAQikkgACK5Vhx2kIQAACEIAABIIkgMAKki59QwACEIAABCCQSgIIrFSGHachAAEIQAACEAiSAAIrSLr0DQEIQAACEIBAKgkgsFIZdpyGAAQgAAEIQCBIAgisIOnSNwQgAAEIQAACqSSAwEpl2HEaAhCAAAQgAIEgCSCwgqRL3xCAAAQgAAEIpJIAAiuVYcdpCEAAAhCAAASCJIDACpIufUMAAhCAAAQgkEoCCKxUhh2nIQABCEAAAhAIkgACK0i69A0BCEAAAhCAQCoJILBSGXachgAEIAABCEAgSAIIrCDp0jcEIAABCEAAAqkkgMBKZdhxGgIQgAAEIACBIAkgsIKkS98QgAAEIAABCKSSAAIrlWHHaQhAAAIQgAAEgiSAwAqSLn1DAAIQgAAEIJBKAgisVIYdpyEAAQhAAAIQCJIAAitIuvQNAQhAAAIQgEAqCSCwUhl2nIYABCAAAQhAIEgCCKwg6dI3BCAAAQhAAAKpJIDASmXYcRoCEIAABCAAgSAJILCCpEvfEIAABCAAAQikkgACK5Vhx2kIQAACEIAABIIkgMAKki59QwACEIAABCCQSgIIrFSGHachAAEIQAACEAiSAAIrSLr0DQEIQAACEIBAKgkgsFIZdpyGAAQgAAEIQCBIAgisIOnSNwQgAAEIQAACqSSAwEpl2HEaAhCAAAQgAIEgCSCwgqRL3xCAAAQgAAEIpJIAAiuVYcdpCEAAAhCAAASCJIDACpIufUMAAhCAAAQgkEoCCKxUhh2nIQABCEAAAhAIkgACK0i69A0BCEAAAhCAQCoJILBSGXachgAEIAABCEAgSAIIrCDp0jcEIAABCEAAAqkkgMBKZdhxGgIQgAAEIACBIAkgsIKkS98QgAAEIAABCKSSAAIrlWHHaQhAAAIQgAAEgiSAwAqSLn1DAAIQgAAEIJBKAgisVIYdpyEAAQhAAAIQCJIAAitIuvQNAQhAAAIQgEAqCSCwUhl2nIYABCAAAQhAIEgCCKwg6dI3BCAAAQhAAAKpJIDASmXYcRoCEIAABCAAgSAJILCNj1d7AAAAaElEQVSCpEvfEIAABCAAAQikkgACK5Vhx2kIQAACEIAABIIkgMAKki59QwACEIAABCCQSgIIrFSGHachAAEIQAACEAiSAAIrSLr0DQEIQAACEIBAKgkgsFIZdpyGAAQgAAEIQCBIAv8HP6bzPYl3NokAAAAASUVORK5CYII=\" width=\"600\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.3, \n",
    "                                n=np.inf,\n",
    "                                grid_fidelity_factor_array =[1.0],\n",
    "                                episode_limit_array=[150000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
