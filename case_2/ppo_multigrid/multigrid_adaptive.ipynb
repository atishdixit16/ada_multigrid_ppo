{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access functions from root directory\n",
    "import sys\n",
    "sys.path.append('/data/ad181/RemoteDir/ada_multigrid_ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ad181/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "from utils.custom_eval_callback import CustomEvalCallback, CustomEvalCallbackParallel\n",
    "from utils.env_wrappers import StateCoarse, BufferWrapper, EnvCoarseWrapper, StateCoarseMultiGrid\n",
    "from typing import Callable\n",
    "from utils.plot_functions import plot_learning\n",
    "from utils.multigrid_framework_functions import env_wrappers_multigrid, make_env, generate_beta_environement, parallalize_env, multigrid_framework\n",
    "\n",
    "from model.ressim import Grid\n",
    "from ressim_env import ResSimEnv_v0, ResSimEnv_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=1\n",
    "case='case_2_multigrid_adaptive'\n",
    "data_dir='./data'\n",
    "log_dir='./data/'+case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../envs_params/env_data/env_train.pkl', 'rb') as input:\n",
    "    env_train = pickle.load(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define RL model and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(env_train, seed):\n",
    "    dummy_env =  generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    dummy_env_parallel = parallalize_env(dummy_env, num_actor=64, seed=seed)\n",
    "    model = PPO(policy=MlpPolicy,\n",
    "                env=dummy_env_parallel,\n",
    "                learning_rate = 1e-4,\n",
    "                n_steps = 40,\n",
    "                batch_size = 16,\n",
    "                n_epochs = 20,\n",
    "                gamma = 0.99,\n",
    "                gae_lambda = 0.95,\n",
    "                clip_range = 0.15,\n",
    "                clip_range_vf = None,\n",
    "                ent_coef = 0.001,\n",
    "                vf_coef = 0.5,\n",
    "                max_grad_norm = 0.5,\n",
    "                use_sde= False,\n",
    "                create_eval_env= False,\n",
    "                policy_kwargs = dict(net_arch=[70,70,50], log_std_init=-1.7),\n",
    "                verbose = 1,\n",
    "                target_kl =0.1,\n",
    "                seed = seed,\n",
    "                device = \"auto\")\n",
    "    return model\n",
    "\n",
    "def generate_callback(env_train, best_model_save_path, log_path, eval_freq):\n",
    "    dummy_env = generate_beta_environement(env_train, 0.5, env_train.p_x, env_train.p_y, seed)\n",
    "    callback = CustomEvalCallbackParallel(dummy_env, \n",
    "                                          best_model_save_path=best_model_save_path, \n",
    "                                          n_eval_episodes=1,\n",
    "                                          log_path=log_path, \n",
    "                                          eval_freq=eval_freq)\n",
    "    return callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multigrid framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 1: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b929fcf60> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b94aeb588>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 5        |\n",
      "|    mean_reward     | 0.679    |\n",
      "| time/              |          |\n",
      "|    fps             | 399      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.667       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071611784 |\n",
      "|    clip_fraction        | 0.443       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.86        |\n",
      "|    explained_variance   | -0.64       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0364     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0156      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.69 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.688      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03454525 |\n",
      "|    clip_fraction        | 0.447      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.83       |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00374    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.701      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03573232 |\n",
      "|    clip_fraction        | 0.458      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.88       |\n",
      "|    explained_variance   | 0.92       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00124    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.00301    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.705       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030601244 |\n",
      "|    clip_fraction        | 0.476       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00539    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00254     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.713       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040795945 |\n",
      "|    clip_fraction        | 0.479       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.942       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0882     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00229     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047549043 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.96        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.048      |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00198     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.73        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045685787 |\n",
      "|    clip_fraction        | 0.467       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0433     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.729      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04374043 |\n",
      "|    clip_fraction        | 0.481      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.03       |\n",
      "|    explained_variance   | 0.957      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0557    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00181    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.726      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05085764 |\n",
      "|    clip_fraction        | 0.485      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.07       |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00161    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 490         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041601393 |\n",
      "|    clip_fraction        | 0.502       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.09        |\n",
      "|    explained_variance   | 0.959       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0444     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042629927 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.11        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.067      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0421     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048662223 |\n",
      "|    clip_fraction        | 0.514       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.964       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06281271 |\n",
      "|    clip_fraction        | 0.494      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.2        |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0497    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00164    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.727       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 482         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052917052 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.24        |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0925     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0468     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.736       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 494         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061812293 |\n",
      "|    clip_fraction        | 0.513       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.963       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0773     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00161     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.736      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05109284 |\n",
      "|    clip_fraction        | 0.514      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.3        |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0348    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.744       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048280817 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.37        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.749       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048708476 |\n",
      "|    clip_fraction        | 0.508       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.066      |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0406     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.755       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052609287 |\n",
      "|    clip_fraction        | 0.531       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.4         |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.044      |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.761       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054532938 |\n",
      "|    clip_fraction        | 0.554       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0467     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059489477 |\n",
      "|    clip_fraction        | 0.537       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00108     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.76      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 477       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0585882 |\n",
      "|    clip_fraction        | 0.552     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.44      |\n",
      "|    explained_variance   | 0.969     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0294   |\n",
      "|    n_updates            | 440       |\n",
      "|    policy_gradient_loss | -0.0482   |\n",
      "|    std                  | 0.178     |\n",
      "|    value_loss           | 0.00137   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05957352 |\n",
      "|    clip_fraction        | 0.534      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.48       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.77        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052519597 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.56        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0393     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058969725 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.61        |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000243   |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0431     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.772      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06373503 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.63       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0564    |\n",
      "|    n_updates            | 520        |\n",
      "|    policy_gradient_loss | -0.0416    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.774      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06501575 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.68       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.778       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051936734 |\n",
      "|    clip_fraction        | 0.549       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.76        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0719     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0395     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 459        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07438328 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.79       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.06      |\n",
      "|    n_updates            | 580        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06692326 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.82       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0421    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062172912 |\n",
      "|    clip_fraction        | 0.563       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06330013 |\n",
      "|    clip_fraction        | 0.562      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.98       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0451    |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0391    |\n",
      "|    std                  | 0.174      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.781       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 481         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059190728 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.03        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0412     |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0407     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.785      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06229939 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.08       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0794    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.784      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06865124 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0196    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.037     |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05645574 |\n",
      "|    clip_fraction        | 0.574      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.789       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061112154 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.31        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0685     |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.0388     |\n",
      "|    std                  | 0.171       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07695669 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.36       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0478    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.792     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 465       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0773824 |\n",
      "|    clip_fraction        | 0.581     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.41      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0663   |\n",
      "|    n_updates            | 780       |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00117   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068699375 |\n",
      "|    clip_fraction        | 0.58        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.48        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07169143 |\n",
      "|    clip_fraction        | 0.568      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.55       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.064957775 |\n",
      "|    clip_fraction        | 0.582       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.63        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0371     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.802      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07421793 |\n",
      "|    clip_fraction        | 0.569      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.72       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0113    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 5        |\n",
      "|    mean_reward          | 0.805    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 478      |\n",
      "|    iterations           | 1        |\n",
      "|    time_elapsed         | 5        |\n",
      "|    total_timesteps      | 2560     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.066572 |\n",
      "|    clip_fraction        | 0.578    |\n",
      "|    clip_range           | 0.15     |\n",
      "|    entropy_loss         | 7.78     |\n",
      "|    explained_variance   | 0.979    |\n",
      "|    learning_rate        | 0.0001   |\n",
      "|    loss                 | -0.0416  |\n",
      "|    n_updates            | 880      |\n",
      "|    policy_gradient_loss | -0.0332  |\n",
      "|    std                  | 0.168    |\n",
      "|    value_loss           | 0.00111  |\n",
      "--------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05657038 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.82       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0243     |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.807       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061324097 |\n",
      "|    clip_fraction        | 0.576       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.87        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0639     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0342     |\n",
      "|    std                  | 0.168       |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 459         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058894586 |\n",
      "|    clip_fraction        | 0.592       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.94        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.81      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 478       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0796905 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.01      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0314    |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0328   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.0011    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.811      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07091652 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.11       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07064302 |\n",
      "|    clip_fraction        | 0.588      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.23       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0015    |\n",
      "|    n_updates            | 1000       |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08055522 |\n",
      "|    clip_fraction        | 0.579      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.31       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.000952   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0719471 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.4       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0498   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0322   |\n",
      "|    std                  | 0.164     |\n",
      "|    value_loss           | 0.001     |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05825283 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.5        |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0432    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0299    |\n",
      "|    std                  | 0.163      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.813     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 479       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0747148 |\n",
      "|    clip_fraction        | 0.6       |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.6       |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0635   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.000956  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06105361 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.7        |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 1100       |\n",
      "|    policy_gradient_loss | -0.0291    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000894   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.816       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080220796 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.84        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0665     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000874    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07097624 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0304    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.000877   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.817       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079861596 |\n",
      "|    clip_fraction        | 0.596       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.07        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    std                  | 0.159       |\n",
      "|    value_loss           | 0.000821    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05854218 |\n",
      "|    clip_fraction        | 0.6        |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.17       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000831   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07405409 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.27       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00086    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 486         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062092163 |\n",
      "|    clip_fraction        | 0.57        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.37        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0423     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    std                  | 0.157       |\n",
      "|    value_loss           | 0.000828    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08321391 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.47       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0973    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0307    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000844   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.821     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 467       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0797853 |\n",
      "|    clip_fraction        | 0.597     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 9.54      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0424   |\n",
      "|    n_updates            | 1260      |\n",
      "|    policy_gradient_loss | -0.0267   |\n",
      "|    std                  | 0.156     |\n",
      "|    value_loss           | 0.00075   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06940863 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.024     |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000778   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06982125 |\n",
      "|    clip_fraction        | 0.596      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0563    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0245    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000798   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07479205 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.77       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.025     |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000728   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078808546 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.88        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0515     |\n",
      "|    n_updates            | 1340        |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000739    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09472382 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0375    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000742   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07766579 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000741   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.824      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 483        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07774629 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.2       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000708   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09312196 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 1420       |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000659   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.825       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.084135614 |\n",
      "|    clip_fraction        | 0.606       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0731     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000654    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09430981 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0428    |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000688   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06655766 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.18       |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0249    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000684   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07956515 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0185     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000691   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09485544 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0838    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0237    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000685   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.827       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079657935 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 1540        |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000745    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.828       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.073997006 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0514     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000635    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08437343 |\n",
      "|    clip_fraction        | 0.591      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0393    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000645   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06543742 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.063     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0207    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000599   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 493         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.082806155 |\n",
      "|    clip_fraction        | 0.612       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.1        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.145       |\n",
      "|    value_loss           | 0.000566    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07732544 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08545779 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0122    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000646   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.83      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 485       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0909723 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.2      |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.017    |\n",
      "|    n_updates            | 1680      |\n",
      "|    policy_gradient_loss | -0.021    |\n",
      "|    std                  | 0.145     |\n",
      "|    value_loss           | 0.000665  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06803899 |\n",
      "|    clip_fraction        | 0.598      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000652   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09224341 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.084     |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000614   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.83       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09557831 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0776     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000623   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "seed 1: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b94aeb588> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b92a0cdd8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 338        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08764621 |\n",
      "|    clip_fraction        | 0.608      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0144     |\n",
      "|    n_updates            | 1760       |\n",
      "|    policy_gradient_loss | -0.0206    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000647   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13122268 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0315    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080635905 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0497     |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.844      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 381        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09959574 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00339   |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075657174 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0495     |\n",
      "|    n_updates            | 1840        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.844     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 380       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0925343 |\n",
      "|    clip_fraction        | 0.609     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.7      |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0615   |\n",
      "|    n_updates            | 1860      |\n",
      "|    policy_gradient_loss | -0.0193   |\n",
      "|    std                  | 0.141     |\n",
      "|    value_loss           | 0.00107   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09046664 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0479    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.021     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09163907 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.159      |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0186    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07808529 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08670707 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0129     |\n",
      "|    n_updates            | 1940       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 396       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0928154 |\n",
      "|    clip_fraction        | 0.603     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12        |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0696   |\n",
      "|    n_updates            | 1960      |\n",
      "|    policy_gradient_loss | -0.0193   |\n",
      "|    std                  | 0.139     |\n",
      "|    value_loss           | 0.00106   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10326681 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07921845 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0882    |\n",
      "|    n_updates            | 2000       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088304944 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12          |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0468     |\n",
      "|    n_updates            | 2020        |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09328233 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12         |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000953   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 380        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10459099 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000988   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097272135 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 2080        |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000991    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085602894 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00233    |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000945    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 360        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11536169 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000947   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 371        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09651317 |\n",
      "|    clip_fraction        | 0.619      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.005      |\n",
      "|    n_updates            | 2140       |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000908   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 352        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09241971 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000979   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 377        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07249974 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0495    |\n",
      "|    n_updates            | 2180       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000973   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 389       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0936633 |\n",
      "|    clip_fraction        | 0.62      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.3      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0675   |\n",
      "|    n_updates            | 2200      |\n",
      "|    policy_gradient_loss | -0.0167   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.000967  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 378         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102097966 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.004       |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100679934 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0654     |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.00106     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08765429 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0509    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "seed 1: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b92a0cdd8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b900469e8>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09047584 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0148     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0183    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13192892 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.000744  |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00159    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09274401 |\n",
      "|    clip_fraction        | 0.618      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0648    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.00157    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.111204244 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12443457 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 2360       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08815845 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00323    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09474637 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0726    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.854     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 227       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1030844 |\n",
      "|    clip_fraction        | 0.63      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.6      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0249   |\n",
      "|    n_updates            | 2420      |\n",
      "|    policy_gradient_loss | -0.0215   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00135   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09786494 |\n",
      "|    clip_fraction        | 0.632      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.015      |\n",
      "|    n_updates            | 2440       |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00135    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099991426 |\n",
      "|    clip_fraction        | 0.629       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0366     |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096085764 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0661     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11493138 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0522    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00131    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 41 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.855     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 228       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0969674 |\n",
      "|    clip_fraction        | 0.626     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.7      |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0694   |\n",
      "|    n_updates            | 2520      |\n",
      "|    policy_gradient_loss | -0.0209   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.0013    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093596436 |\n",
      "|    clip_fraction        | 0.621       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0373     |\n",
      "|    n_updates            | 2540        |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00133     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086306974 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0334     |\n",
      "|    n_updates            | 2560        |\n",
      "|    policy_gradient_loss | -0.0209     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00135     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093355305 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0793      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.854       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096971884 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0139      |\n",
      "|    n_updates            | 2600        |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00131     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10863054 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.056      |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.854      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10341704 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0319     |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11641757 |\n",
      "|    clip_fraction        | 0.639      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0187    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.0014     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09962135 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00749    |\n",
      "|    n_updates            | 2680       |\n",
      "|    policy_gradient_loss | -0.0214    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00129    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10940137 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0292     |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11309926 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11458918 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13         |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.096419655 |\n",
      "|    clip_fraction        | 0.632       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0054     |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.853      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09407102 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00516   |\n",
      "|    n_updates            | 2780       |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 2: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b6c0e68d0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b6c0e6e10>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.671      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 457        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07637861 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0528    |\n",
      "|    n_updates            | 2800       |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.683       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051376868 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | -0.0716     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.035      |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.0117      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.701       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048658594 |\n",
      "|    clip_fraction        | 0.447       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.91        |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00365     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.698       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033352904 |\n",
      "|    clip_fraction        | 0.469       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.923       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.044      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0452     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00292     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.702       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040718857 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.927       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0528     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00282     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.70 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.702      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 458        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03411284 |\n",
      "|    clip_fraction        | 0.463      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.97       |\n",
      "|    explained_variance   | 0.939      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.071     |\n",
      "|    n_updates            | 100        |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00229    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.711       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037490245 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.99        |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00201     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.72        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041925915 |\n",
      "|    clip_fraction        | 0.48        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6           |\n",
      "|    explained_variance   | 0.953       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00168    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0447     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00183     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.726       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043623336 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0475     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00181     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04546948 |\n",
      "|    clip_fraction        | 0.473      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.09       |\n",
      "|    explained_variance   | 0.959      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0383     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00166    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.733       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030160943 |\n",
      "|    clip_fraction        | 0.499       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.14        |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00162     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05025809 |\n",
      "|    clip_fraction        | 0.492      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.19       |\n",
      "|    explained_variance   | 0.963      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 220        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00154    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.735       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054097105 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.22        |\n",
      "|    explained_variance   | 0.962       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.735      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04217591 |\n",
      "|    clip_fraction        | 0.499      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.23       |\n",
      "|    explained_variance   | 0.962      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0801    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.0016     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.732      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05191766 |\n",
      "|    clip_fraction        | 0.498      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0558    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00147    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048750035 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.2         |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00144     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.731     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 463       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0551486 |\n",
      "|    clip_fraction        | 0.51      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.24      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0647   |\n",
      "|    n_updates            | 320       |\n",
      "|    policy_gradient_loss | -0.0444   |\n",
      "|    std                  | 0.18      |\n",
      "|    value_loss           | 0.0014    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.73       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05785197 |\n",
      "|    clip_fraction        | 0.515      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.29       |\n",
      "|    explained_variance   | 0.968      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0239    |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.725      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04822092 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.33       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00138    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.731      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04862503 |\n",
      "|    clip_fraction        | 0.529      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.38       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.732       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057611883 |\n",
      "|    clip_fraction        | 0.533       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.42        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0621     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.739      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06000798 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.49       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0761    |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.741       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 483         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054572217 |\n",
      "|    clip_fraction        | 0.542       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.55        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0438     |\n",
      "|    std                  | 0.177       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06736399 |\n",
      "|    clip_fraction        | 0.538      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.62       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0181    |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.745       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056733884 |\n",
      "|    clip_fraction        | 0.546       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.67        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0432     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.742      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05385695 |\n",
      "|    clip_fraction        | 0.535      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.69       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0417    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.744     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 468       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0693997 |\n",
      "|    clip_fraction        | 0.538     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.74      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.046    |\n",
      "|    n_updates            | 520       |\n",
      "|    policy_gradient_loss | -0.0422   |\n",
      "|    std                  | 0.176     |\n",
      "|    value_loss           | 0.00132   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.747      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05284319 |\n",
      "|    clip_fraction        | 0.546      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.81       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0734    |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00137    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.748      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 460        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04929874 |\n",
      "|    clip_fraction        | 0.553      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.86       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.75        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062127255 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.89        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.066382244 |\n",
      "|    clip_fraction        | 0.556       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.9         |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.065      |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0428     |\n",
      "|    std                  | 0.175       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061261117 |\n",
      "|    clip_fraction        | 0.55        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.92        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0403     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.756     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0666964 |\n",
      "|    clip_fraction        | 0.57      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.97      |\n",
      "|    explained_variance   | 0.973     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0621   |\n",
      "|    n_updates            | 640       |\n",
      "|    policy_gradient_loss | -0.0435   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.0012    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.755     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 475       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0651351 |\n",
      "|    clip_fraction        | 0.567     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.01      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0809   |\n",
      "|    n_updates            | 660       |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    std                  | 0.174     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061286975 |\n",
      "|    clip_fraction        | 0.567       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0698     |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00108     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.764      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06732507 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.06       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0308    |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00119    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056050707 |\n",
      "|    clip_fraction        | 0.566       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.1         |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0856     |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0375     |\n",
      "|    std                  | 0.173       |\n",
      "|    value_loss           | 0.00117     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06630887 |\n",
      "|    clip_fraction        | 0.565      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.15       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 740        |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.766      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07023197 |\n",
      "|    clip_fraction        | 0.575      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.22       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0823    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00124    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.767      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 475        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05852921 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.29       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 780        |\n",
      "|    policy_gradient_loss | -0.0404    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07357506 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.38       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 0.171      |\n",
      "|    value_loss           | 0.00116    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.77       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07846562 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.44       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0383    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.773     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 490       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0608586 |\n",
      "|    clip_fraction        | 0.58      |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.51      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 840       |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    std                  | 0.17      |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.776       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 462         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067037866 |\n",
      "|    clip_fraction        | 0.586       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.55        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0632     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.0383     |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.779      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 464        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07450347 |\n",
      "|    clip_fraction        | 0.58       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.58       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 880        |\n",
      "|    policy_gradient_loss | -0.0364    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.782       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 474         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076563425 |\n",
      "|    clip_fraction        | 0.585       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.62        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0461     |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.785       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075655535 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.69        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0405     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.0378     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.789      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06556271 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.79       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 940        |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.168      |\n",
      "|    value_loss           | 0.00113    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.791     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 471       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0922491 |\n",
      "|    clip_fraction        | 0.594     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.88      |\n",
      "|    explained_variance   | 0.976     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 960       |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.167     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.793      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07615379 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.98       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0353    |\n",
      "|    std                  | 0.166      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.795       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072590634 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.04        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.166       |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081946425 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.1         |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0459     |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0343     |\n",
      "|    std                  | 0.165       |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07336445 |\n",
      "|    clip_fraction        | 0.597      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.2        |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0631    |\n",
      "|    n_updates            | 1040       |\n",
      "|    policy_gradient_loss | -0.0345    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.805      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08942877 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.28       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.034     |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.808     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 472       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0684288 |\n",
      "|    clip_fraction        | 0.606     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.37      |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0287   |\n",
      "|    n_updates            | 1080      |\n",
      "|    policy_gradient_loss | -0.0337   |\n",
      "|    std                  | 0.163     |\n",
      "|    value_loss           | 0.00102   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.811       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.098124005 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.48        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.0339     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.812       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083682366 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.58        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0335     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    std                  | 0.162       |\n",
      "|    value_loss           | 0.000931    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.816      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 490        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06746669 |\n",
      "|    clip_fraction        | 0.602      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.68       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0417     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0285    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.000966   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.819       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076223746 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.77        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0646     |\n",
      "|    n_updates            | 1160        |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.000954    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08127213 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.89       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.113      |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07096313 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9          |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.000846   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.82        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 485         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078128785 |\n",
      "|    clip_fraction        | 0.615       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.11        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0276     |\n",
      "|    std                  | 0.158       |\n",
      "|    value_loss           | 0.000906    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 492        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07393008 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.21       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0154    |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.823       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072332405 |\n",
      "|    clip_fraction        | 0.61        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.34        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0244      |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.000809    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.825      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07787631 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.4        |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00189   |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.000842   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.826      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07440407 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.51       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0592    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07948859 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.59       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0644    |\n",
      "|    n_updates            | 1320       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000819   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07599194 |\n",
      "|    clip_fraction        | 0.589      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.69       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0535    |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.0209    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.000775   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.828      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09329804 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.81       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.026     |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.000727   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07452371 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.91       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0397    |\n",
      "|    n_updates            | 1380       |\n",
      "|    policy_gradient_loss | -0.0238    |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.000731   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.829      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08580129 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0406     |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000709   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 488         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.063462935 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0745     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000764    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.829       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 487         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090517186 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000729    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.062073715 |\n",
      "|    clip_fraction        | 0.603       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.3        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00811     |\n",
      "|    n_updates            | 1460        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.149       |\n",
      "|    value_loss           | 0.000666    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 487        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09799924 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000723   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.83        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 484         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078958824 |\n",
      "|    clip_fraction        | 0.607       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.5        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000652    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07313873 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000601   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.831      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06864514 |\n",
      "|    clip_fraction        | 0.605      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0566    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000584   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079242826 |\n",
      "|    clip_fraction        | 0.608       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.7        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 1560        |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    std                  | 0.147       |\n",
      "|    value_loss           | 0.000562    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.831       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 480         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087600395 |\n",
      "|    clip_fraction        | 0.598       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.8        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0554     |\n",
      "|    n_updates            | 1580        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.146       |\n",
      "|    value_loss           | 0.00061     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09132691 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0557     |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000636   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08345769 |\n",
      "|    clip_fraction        | 0.614      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000602   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 466        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08290584 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000588   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08842277 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00493   |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000651   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.832      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 493        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09605677 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000648   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08552261 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.989      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00556   |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0178    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000638   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.833     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 487       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0883674 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.3      |\n",
      "|    explained_variance   | 0.99      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.000851 |\n",
      "|    n_updates            | 1720      |\n",
      "|    policy_gradient_loss | -0.0158   |\n",
      "|    std                  | 0.143     |\n",
      "|    value_loss           | 0.000606  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10462477 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00396    |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.833       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 487         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086456716 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.4        |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000611    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 488        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09217997 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000577   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "seed 2: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b9463e8d0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b92a0cd30>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 370        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11325834 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.6       |\n",
      "|    explained_variance   | 0.99       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00588    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0177    |\n",
      "|    std                  | 0.141      |\n",
      "|    value_loss           | 0.000622   |\n",
      "----------------------------------------\n",
      "Early stopping at step 18 due to reaching max kl: 0.16\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 404        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15603372 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0938    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0204    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10612108 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0078     |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.14       |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 399        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10517597 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0503    |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.02      |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 401        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08979307 |\n",
      "|    clip_fraction        | 0.612      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 1880       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.845       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 393         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080941096 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.9        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.052      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    std                  | 0.139       |\n",
      "|    value_loss           | 0.000999    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.845      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09441046 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0209    |\n",
      "|    n_updates            | 1920       |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000954   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.845     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 391       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0913993 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12        |\n",
      "|    explained_variance   | 0.982     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0146    |\n",
      "|    n_updates            | 1940      |\n",
      "|    policy_gradient_loss | -0.0187   |\n",
      "|    std                  | 0.138     |\n",
      "|    value_loss           | 0.00104   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.846     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 395       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0945799 |\n",
      "|    clip_fraction        | 0.636     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.1      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0711   |\n",
      "|    n_updates            | 1960      |\n",
      "|    policy_gradient_loss | -0.0209   |\n",
      "|    std                  | 0.137     |\n",
      "|    value_loss           | 0.00101   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09587671 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000965   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.846       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081113204 |\n",
      "|    clip_fraction        | 0.618       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000936    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08089286 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0787    |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000897   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 397        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08976973 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0313    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000803   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.846      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 402        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07865347 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0553    |\n",
      "|    n_updates            | 2060       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10246134 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.86e-05   |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0188    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000806   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 395        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10804482 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.018     |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000793   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 403        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10364065 |\n",
      "|    clip_fraction        | 0.627      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.4       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.023      |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.000782   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.102684736 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.4        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.058      |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.136       |\n",
      "|    value_loss           | 0.000754    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 365        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09943137 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.000885   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 393       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1020941 |\n",
      "|    clip_fraction        | 0.645     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.6      |\n",
      "|    explained_variance   | 0.987     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0386   |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0133   |\n",
      "|    std                  | 0.135     |\n",
      "|    value_loss           | 0.000771  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.847      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 394        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09993969 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.988      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.019     |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.000781   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 399         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.108605884 |\n",
      "|    clip_fraction        | 0.64        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.6        |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0631     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.000816    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.847       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 394         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.085756145 |\n",
      "|    clip_fraction        | 0.628       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.7        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00367    |\n",
      "|    n_updates            | 2240        |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000777    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 400        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10178621 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00373   |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0164    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000794   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.848       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087739944 |\n",
      "|    clip_fraction        | 0.634       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0258     |\n",
      "|    n_updates            | 2280        |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.000786    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.848      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 403        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09166778 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.018      |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00083    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "seed 2: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b4c2d7748> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b4c216860>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11141207 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0154    |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.000759   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.855      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11196454 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0594    |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00112    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 228         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.091608986 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.9        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0634     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    std                  | 0.133       |\n",
      "|    value_loss           | 0.00113     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09209176 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0501    |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087698504 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00116     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097138315 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0565     |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 220         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.087518856 |\n",
      "|    clip_fraction        | 0.626       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13          |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09896113 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0334    |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 226       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0992937 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.1      |\n",
      "|    explained_variance   | 0.983     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.067    |\n",
      "|    n_updates            | 2480      |\n",
      "|    policy_gradient_loss | -0.0179   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.00103   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.090033606 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 2500        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101276875 |\n",
      "|    clip_fraction        | 0.643       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.2        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00903     |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.0011      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.856     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 227       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1193557 |\n",
      "|    clip_fraction        | 0.643     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.2      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0539   |\n",
      "|    n_updates            | 2540      |\n",
      "|    policy_gradient_loss | -0.0203   |\n",
      "|    std                  | 0.131     |\n",
      "|    value_loss           | 0.00112   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.856      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10419345 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0396     |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0139    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09904534 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 2580       |\n",
      "|    policy_gradient_loss | -0.0134    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.000995   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12256571 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.000972   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10798021 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.4       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0911     |\n",
      "|    n_updates            | 2620       |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00109    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.110525474 |\n",
      "|    clip_fraction        | 0.633       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 2640        |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.088700615 |\n",
      "|    clip_fraction        | 0.641       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.5        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 2660        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    std                  | 0.129       |\n",
      "|    value_loss           | 0.000993    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.856       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 223         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.116840124 |\n",
      "|    clip_fraction        | 0.641       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.6        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.035      |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 69120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09056701 |\n",
      "|    clip_fraction        | 0.64       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.6       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0548    |\n",
      "|    n_updates            | 2700       |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 69632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09403777 |\n",
      "|    clip_fraction        | 0.637      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0818    |\n",
      "|    n_updates            | 2720       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 70144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10235927 |\n",
      "|    clip_fraction        | 0.649      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00887   |\n",
      "|    n_updates            | 2740       |\n",
      "|    policy_gradient_loss | -0.0202    |\n",
      "|    std                  | 0.128      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 70656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11977138 |\n",
      "|    clip_fraction        | 0.651      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.7       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0544    |\n",
      "|    n_updates            | 2760       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000929   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 71168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 227         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100330964 |\n",
      "|    clip_fraction        | 0.644       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.7        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0538     |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    std                  | 0.128       |\n",
      "|    value_loss           | 0.000999    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 71680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.857     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 231       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1063384 |\n",
      "|    clip_fraction        | 0.652     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.7      |\n",
      "|    explained_variance   | 0.985     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0053    |\n",
      "|    n_updates            | 2800      |\n",
      "|    policy_gradient_loss | -0.0164   |\n",
      "|    std                  | 0.127     |\n",
      "|    value_loss           | 0.000931  |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 42 seconds\n",
      "\n",
      "Total episode rollouts: 72192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.86 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.857      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12233629 |\n",
      "|    clip_fraction        | 0.647      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0718    |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000861   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 72704\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "seed 3: grid fidelity factor 0.25 learning ..\n",
      "environement grid size (nx x ny ): 7 x 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b4c1ab470> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b4c1ab4e0>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.68       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 455        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10171328 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0674    |\n",
      "|    n_updates            | 2840       |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    std                  | 0.127      |\n",
      "|    value_loss           | 0.000867   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.67 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.674      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06958654 |\n",
      "|    clip_fraction        | 0.438      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.89       |\n",
      "|    explained_variance   | -0.376     |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.183      |\n",
      "|    value_loss           | 0.0125     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 1024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.68 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.684       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 452         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047490202 |\n",
      "|    clip_fraction        | 0.459       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0861     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0459     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00369     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 1536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.711      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 462        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03339555 |\n",
      "|    clip_fraction        | 0.465      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.94       |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0512    |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.0439    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00295    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 2048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.71 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.709       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044822954 |\n",
      "|    clip_fraction        | 0.482       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.98        |\n",
      "|    explained_variance   | 0.93        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0477     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00252     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 2560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.721       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037756953 |\n",
      "|    clip_fraction        | 0.474       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.97        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0585     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00213     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.718       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040763944 |\n",
      "|    clip_fraction        | 0.481       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.94        |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00209     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 3584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.728       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 465         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043451317 |\n",
      "|    clip_fraction        | 0.501       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.92        |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0471     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0469     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00177     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 4096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.719       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041048933 |\n",
      "|    clip_fraction        | 0.485       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.9         |\n",
      "|    explained_variance   | 0.96        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0463     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00163     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 4608\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.72 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.724       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.048675895 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.93        |\n",
      "|    explained_variance   | 0.958       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0461     |\n",
      "|    std                  | 0.182       |\n",
      "|    value_loss           | 0.00167     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 5120\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.727      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04694677 |\n",
      "|    clip_fraction        | 0.51       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 5.96       |\n",
      "|    explained_variance   | 0.965      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0176     |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    std                  | 0.182      |\n",
      "|    value_loss           | 0.00153    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 5632\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.734       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046830624 |\n",
      "|    clip_fraction        | 0.498       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 5.95        |\n",
      "|    explained_variance   | 0.965       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0689     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0439     |\n",
      "|    std                  | 0.183       |\n",
      "|    value_loss           | 0.00145     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6144\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.73 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.734     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 463       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0459747 |\n",
      "|    clip_fraction        | 0.503     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 5.98      |\n",
      "|    explained_variance   | 0.964     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0434   |\n",
      "|    n_updates            | 240       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.182     |\n",
      "|    value_loss           | 0.00152   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 6656\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.742       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036458228 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.04        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.047      |\n",
      "|    std                  | 0.181       |\n",
      "|    value_loss           | 0.00137     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7168\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.74       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04491026 |\n",
      "|    clip_fraction        | 0.519      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.11       |\n",
      "|    explained_variance   | 0.966      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    std                  | 0.181      |\n",
      "|    value_loss           | 0.00143    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 7680\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.738       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046822768 |\n",
      "|    clip_fraction        | 0.51        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.17        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0657     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0435     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 8192\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.74 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.741      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 452        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04647106 |\n",
      "|    clip_fraction        | 0.523      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.21       |\n",
      "|    explained_variance   | 0.969      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 320        |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    std                  | 0.18       |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 8704\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.747       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056340653 |\n",
      "|    clip_fraction        | 0.522       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.25        |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0644     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00139     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 9216\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.754       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053496473 |\n",
      "|    clip_fraction        | 0.524       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.28        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0437     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.18        |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 9728\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.756       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060202397 |\n",
      "|    clip_fraction        | 0.518       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.32        |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0763     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0427     |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0014      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 10240\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.756      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05329554 |\n",
      "|    clip_fraction        | 0.537      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.35       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    std                  | 0.179      |\n",
      "|    value_loss           | 0.00128    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 10752\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045318723 |\n",
      "|    clip_fraction        | 0.535       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.38        |\n",
      "|    explained_variance   | 0.97        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0645     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    std                  | 0.179       |\n",
      "|    value_loss           | 0.0013      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 11264\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.75 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.755      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06090635 |\n",
      "|    clip_fraction        | 0.533      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.44       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0258    |\n",
      "|    n_updates            | 440        |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    std                  | 0.178      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 11776\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.758       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 472         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056513917 |\n",
      "|    clip_fraction        | 0.547       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.5         |\n",
      "|    explained_variance   | 0.971       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0443     |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 12288\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.762     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 471       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0578928 |\n",
      "|    clip_fraction        | 0.548     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 6.57      |\n",
      "|    explained_variance   | 0.97      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 480       |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    std                  | 0.177     |\n",
      "|    value_loss           | 0.0013    |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 12800\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.762      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 458        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05858321 |\n",
      "|    clip_fraction        | 0.532      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.64       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 500        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.177      |\n",
      "|    value_loss           | 0.00127    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 13312\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.76 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.764       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 457         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.056070495 |\n",
      "|    clip_fraction        | 0.536       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.65        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.0402     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.0012      |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 13824\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.771       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.055896807 |\n",
      "|    clip_fraction        | 0.559       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.68        |\n",
      "|    explained_variance   | 0.972       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0704     |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.0446     |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 14336\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.771      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 482        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06903891 |\n",
      "|    clip_fraction        | 0.549      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.74       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0466    |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    std                  | 0.176      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 14848\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.772       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 468         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057420634 |\n",
      "|    clip_fraction        | 0.552       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.77        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.039      |\n",
      "|    std                  | 0.176       |\n",
      "|    value_loss           | 0.00124     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15360\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.775      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 474        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05457897 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 6.83       |\n",
      "|    explained_variance   | 0.971      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0448    |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    std                  | 0.175      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 15872\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.77 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.775       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 475         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054109592 |\n",
      "|    clip_fraction        | 0.557       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 6.96        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0488     |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0401     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 16384\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.777       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 464         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.058915943 |\n",
      "|    clip_fraction        | 0.574       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.02        |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.00935     |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.0415     |\n",
      "|    std                  | 0.174       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 16896\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.781      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 463        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06029724 |\n",
      "|    clip_fraction        | 0.57       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.07       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0177    |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    std                  | 0.173      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 17408\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.78       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07435803 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.17       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 680        |\n",
      "|    policy_gradient_loss | -0.0397    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 17920\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06099558 |\n",
      "|    clip_fraction        | 0.558      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.23       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.0386    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 18432\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.782      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05521869 |\n",
      "|    clip_fraction        | 0.564      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.28       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0329     |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0398    |\n",
      "|    std                  | 0.172      |\n",
      "|    value_loss           | 0.00121    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 18944\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.78 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.783     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 474       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0634351 |\n",
      "|    clip_fraction        | 0.578     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 7.36      |\n",
      "|    explained_variance   | 0.974     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0338   |\n",
      "|    n_updates            | 740       |\n",
      "|    policy_gradient_loss | -0.0407   |\n",
      "|    std                  | 0.171     |\n",
      "|    value_loss           | 0.00123   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 19456\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.786      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07324021 |\n",
      "|    clip_fraction        | 0.586      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.46       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    std                  | 0.17       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 19968\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.787       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 461         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770976 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.6         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0635     |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0409     |\n",
      "|    std                  | 0.169       |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 20480\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.79       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 473        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06914873 |\n",
      "|    clip_fraction        | 0.578      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.64       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0381    |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00123    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 20992\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.791      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06435315 |\n",
      "|    clip_fraction        | 0.581      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.68       |\n",
      "|    explained_variance   | 0.974      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 820        |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    std                  | 0.169      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 21504\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.791       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.072419755 |\n",
      "|    clip_fraction        | 0.568       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.79        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0428     |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0326     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 22016\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.79 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.792      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06710909 |\n",
      "|    clip_fraction        | 0.585      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 7.85       |\n",
      "|    explained_variance   | 0.972      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0684    |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    std                  | 0.167      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 22528\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.796       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068342045 |\n",
      "|    clip_fraction        | 0.587       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 7.91        |\n",
      "|    explained_variance   | 0.975       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0434     |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.0349     |\n",
      "|    std                  | 0.167       |\n",
      "|    value_loss           | 0.00126     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 23040\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.797      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07601108 |\n",
      "|    clip_fraction        | 0.584      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.05       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 900        |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    std                  | 0.165      |\n",
      "|    value_loss           | 0.0013     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 23552\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.799      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 451        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06550066 |\n",
      "|    clip_fraction        | 0.595      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.18       |\n",
      "|    explained_variance   | 0.973      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0342     |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.0356    |\n",
      "|    std                  | 0.164      |\n",
      "|    value_loss           | 0.00133    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 24064\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.798       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093846254 |\n",
      "|    clip_fraction        | 0.604       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.3         |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0915     |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 0.00128     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 24576\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.799       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 471         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070667684 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.38        |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.0338     |\n",
      "|    std                  | 0.163       |\n",
      "|    value_loss           | 0.00129     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 25088\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.8        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08304366 |\n",
      "|    clip_fraction        | 0.576      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.48       |\n",
      "|    explained_variance   | 0.975      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0326    |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.0314    |\n",
      "|    std                  | 0.162      |\n",
      "|    value_loss           | 0.00126    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 25600\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.799     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 473       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0701161 |\n",
      "|    clip_fraction        | 0.599     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.59      |\n",
      "|    explained_variance   | 0.975     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0212   |\n",
      "|    n_updates            | 1000      |\n",
      "|    policy_gradient_loss | -0.0361   |\n",
      "|    std                  | 0.162     |\n",
      "|    value_loss           | 0.00121   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 26112\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.801      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07460266 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.65       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | -0.0332    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 26624\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.802     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 480       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0819386 |\n",
      "|    clip_fraction        | 0.595     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 8.68      |\n",
      "|    explained_variance   | 0.977     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 1040      |\n",
      "|    policy_gradient_loss | -0.0338   |\n",
      "|    std                  | 0.161     |\n",
      "|    value_loss           | 0.00116   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 27136\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 453        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07067679 |\n",
      "|    clip_fraction        | 0.594      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.73       |\n",
      "|    explained_variance   | 0.976      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 1060       |\n",
      "|    policy_gradient_loss | -0.0315    |\n",
      "|    std                  | 0.161      |\n",
      "|    value_loss           | 0.00118    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 27648\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.804       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.077644095 |\n",
      "|    clip_fraction        | 0.593       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.81        |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0178     |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 28160\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.80 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.805       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 492         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086551845 |\n",
      "|    clip_fraction        | 0.601       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 8.89        |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.000756   |\n",
      "|    n_updates            | 1100        |\n",
      "|    policy_gradient_loss | -0.034      |\n",
      "|    std                  | 0.16        |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 28672\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08583589 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 8.97       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0376    |\n",
      "|    n_updates            | 1120       |\n",
      "|    policy_gradient_loss | -0.0335    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 29184\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07476324 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.01       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00233    |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0328    |\n",
      "|    std                  | 0.159      |\n",
      "|    value_loss           | 0.00117    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 29696\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.807      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 477        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09169489 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.08       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 1160       |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    std                  | 0.158      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 30208\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.808      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07583239 |\n",
      "|    clip_fraction        | 0.599      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.19       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 1180       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 30720\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.809      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 469        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08412103 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.29       |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0325    |\n",
      "|    n_updates            | 1200       |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    std                  | 0.157      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 31232\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.809       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 489         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.081914276 |\n",
      "|    clip_fraction        | 0.589       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.39        |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0575     |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    std                  | 0.156       |\n",
      "|    value_loss           | 0.00125     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 31744\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.81       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09545218 |\n",
      "|    clip_fraction        | 0.593      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.45       |\n",
      "|    explained_variance   | 0.978      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0146     |\n",
      "|    n_updates            | 1240       |\n",
      "|    policy_gradient_loss | -0.0288    |\n",
      "|    std                  | 0.156      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 32256\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.812      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 472        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08993331 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.54       |\n",
      "|    explained_variance   | 0.979      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0277    |\n",
      "|    n_updates            | 1260       |\n",
      "|    policy_gradient_loss | -0.0268    |\n",
      "|    std                  | 0.155      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 32768\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.813      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 461        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07361041 |\n",
      "|    clip_fraction        | 0.613      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.64       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0646    |\n",
      "|    n_updates            | 1280       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.154      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 25 seconds\n",
      "\n",
      "Total episode rollouts: 33280\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.814      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 476        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08226754 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.74       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0878    |\n",
      "|    n_updates            | 1300       |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    std                  | 0.153      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 33792\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.81 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.815       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.075846694 |\n",
      "|    clip_fraction        | 0.609       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 9.86        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0255     |\n",
      "|    std                  | 0.153       |\n",
      "|    value_loss           | 0.000996    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 34304\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.815      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 481        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10608163 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 9.97       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00673   |\n",
      "|    n_updates            | 1340       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.152      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 34816\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.817      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 480        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08148558 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0111    |\n",
      "|    n_updates            | 1360       |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.00098    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 35328\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 473         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093813166 |\n",
      "|    clip_fraction        | 0.602       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.1        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000882    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 35840\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 470        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09656456 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0532    |\n",
      "|    n_updates            | 1400       |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    std                  | 0.151      |\n",
      "|    value_loss           | 0.000918   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 36352\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 470         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.061770022 |\n",
      "|    clip_fraction        | 0.595       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0517     |\n",
      "|    n_updates            | 1420        |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    std                  | 0.151       |\n",
      "|    value_loss           | 0.000919    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 36864\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.818       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 477         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089062504 |\n",
      "|    clip_fraction        | 0.611       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 10.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0416     |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0232     |\n",
      "|    std                  | 0.15        |\n",
      "|    value_loss           | 0.000908    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 24 seconds\n",
      "\n",
      "Total episode rollouts: 37376\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 471        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07812403 |\n",
      "|    clip_fraction        | 0.611      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0799     |\n",
      "|    n_updates            | 1460       |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    std                  | 0.15       |\n",
      "|    value_loss           | 0.000899   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 37888\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.818      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 479        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07990335 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.3       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0817    |\n",
      "|    n_updates            | 1480       |\n",
      "|    policy_gradient_loss | -0.0279    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000896   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38400\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.819      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 467        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08633542 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.4       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 1500       |\n",
      "|    policy_gradient_loss | -0.0228    |\n",
      "|    std                  | 0.149      |\n",
      "|    value_loss           | 0.000879   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 38912\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 468        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09878737 |\n",
      "|    clip_fraction        | 0.604      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.5       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0727    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000829   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 39424\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.82       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08626648 |\n",
      "|    clip_fraction        | 0.61       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.6       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0165    |\n",
      "|    n_updates            | 1540       |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    std                  | 0.148      |\n",
      "|    value_loss           | 0.000815   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 39936\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08954557 |\n",
      "|    clip_fraction        | 0.606      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.7       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00716   |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000763   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 40448\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 484        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09657636 |\n",
      "|    clip_fraction        | 0.601      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.8       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 1580       |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    std                  | 0.147      |\n",
      "|    value_loss           | 0.000756   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 40960\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.821      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 465        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08019618 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0501    |\n",
      "|    n_updates            | 1600       |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    std                  | 0.146      |\n",
      "|    value_loss           | 0.000789   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 41472\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.82 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 486        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07889671 |\n",
      "|    clip_fraction        | 0.607      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 10.9       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00167    |\n",
      "|    n_updates            | 1620       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000718   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 41984\n",
      "\n",
      "seed 3: grid fidelity factor 0.5 learning ..\n",
      "environement grid size (nx x ny ): 15 x 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b4c21ea58> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b94608e10>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.833      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 355        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08969821 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00949   |\n",
      "|    n_updates            | 1640       |\n",
      "|    policy_gradient_loss | -0.0236    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.000701   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 29 seconds\n",
      "\n",
      "Total episode rollouts: 42496\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13196279 |\n",
      "|    clip_fraction        | 0.609      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.97       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.00786    |\n",
      "|    n_updates            | 1660       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.00125    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 43008\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.83 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.834      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 374        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08205215 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11         |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0593    |\n",
      "|    n_updates            | 1680       |\n",
      "|    policy_gradient_loss | -0.0248    |\n",
      "|    std                  | 0.145      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 43520\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.835      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 387        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09131403 |\n",
      "|    clip_fraction        | 0.616      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.1       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 1700       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000975   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 44032\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.836      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08723666 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0289    |\n",
      "|    n_updates            | 1720       |\n",
      "|    policy_gradient_loss | -0.0265    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.000994   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 44544\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 385        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08596708 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 1740       |\n",
      "|    policy_gradient_loss | -0.0259    |\n",
      "|    std                  | 0.144      |\n",
      "|    value_loss           | 0.00103    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 45056\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.837       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.094642006 |\n",
      "|    clip_fraction        | 0.622       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.2        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 1760        |\n",
      "|    policy_gradient_loss | -0.0281     |\n",
      "|    std                  | 0.144       |\n",
      "|    value_loss           | 0.001       |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 45568\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 393        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08736302 |\n",
      "|    clip_fraction        | 0.625      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.3       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0813    |\n",
      "|    n_updates            | 1780       |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.001      |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 46080\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.837      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09643123 |\n",
      "|    clip_fraction        | 0.615      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0057    |\n",
      "|    n_updates            | 1800       |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    std                  | 0.143      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 46592\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09267928 |\n",
      "|    clip_fraction        | 0.617      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.4       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0509    |\n",
      "|    n_updates            | 1820       |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 47104\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.838      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07952504 |\n",
      "|    clip_fraction        | 0.603      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0457    |\n",
      "|    n_updates            | 1840       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 0.000984   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 47616\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.838       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.089991786 |\n",
      "|    clip_fraction        | 0.617       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0663     |\n",
      "|    n_updates            | 1860        |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    std                  | 0.142       |\n",
      "|    value_loss           | 0.000975    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 48128\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 390         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.104062974 |\n",
      "|    clip_fraction        | 0.614       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0472     |\n",
      "|    n_updates            | 1880        |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.00091     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 48640\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.099379465 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.6        |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0735      |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    std                  | 0.141       |\n",
      "|    value_loss           | 0.000936    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 49152\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.839       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 387         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.080713764 |\n",
      "|    clip_fraction        | 0.613       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 11.7        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0211      |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0229     |\n",
      "|    std                  | 0.14        |\n",
      "|    value_loss           | 0.000999    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 49664\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.84      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 388       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1065871 |\n",
      "|    clip_fraction        | 0.621     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 11.8      |\n",
      "|    explained_variance   | 0.984     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0611    |\n",
      "|    n_updates            | 1940      |\n",
      "|    policy_gradient_loss | -0.0241   |\n",
      "|    std                  | 0.14      |\n",
      "|    value_loss           | 0.000961  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 50176\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 388        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10714207 |\n",
      "|    clip_fraction        | 0.626      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 11.9       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0543     |\n",
      "|    n_updates            | 1960       |\n",
      "|    policy_gradient_loss | -0.0225    |\n",
      "|    std                  | 0.139      |\n",
      "|    value_loss           | 0.000957   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 26 seconds\n",
      "\n",
      "Total episode rollouts: 50688\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.839      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10322623 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00869   |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000968   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 51200\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 376         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106203936 |\n",
      "|    clip_fraction        | 0.625       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.1        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0661      |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    std                  | 0.138       |\n",
      "|    value_loss           | 0.000969    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 51712\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10090083 |\n",
      "|    clip_fraction        | 0.629      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.1       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.04       |\n",
      "|    n_updates            | 2020       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.138      |\n",
      "|    value_loss           | 0.000923   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 27 seconds\n",
      "\n",
      "Total episode rollouts: 52224\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 386        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10213177 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.2       |\n",
      "|    explained_variance   | 0.984      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0229    |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000931   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 52736\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.84        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 377         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.092085615 |\n",
      "|    clip_fraction        | 0.637       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.2        |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 2060        |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000885    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 36 seconds\n",
      "\n",
      "Total episode rollouts: 53248\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 391        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10903537 |\n",
      "|    clip_fraction        | 0.628      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0639    |\n",
      "|    n_updates            | 2080       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000792   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 53760\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.84       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 382        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09535974 |\n",
      "|    clip_fraction        | 0.643      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.985      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0578    |\n",
      "|    n_updates            | 2100       |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000905   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 54272\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.841      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 373        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10873814 |\n",
      "|    clip_fraction        | 0.624      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.986      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0521    |\n",
      "|    n_updates            | 2120       |\n",
      "|    policy_gradient_loss | -0.0185    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000821   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 54784\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.84 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.841       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 385         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.101856306 |\n",
      "|    clip_fraction        | 0.627       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.3        |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0386     |\n",
      "|    n_updates            | 2140        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    std                  | 0.137       |\n",
      "|    value_loss           | 0.000806    |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 28 seconds\n",
      "\n",
      "Total episode rollouts: 55296\n",
      "\n",
      "seed 3: grid fidelity factor 1.0 learning ..\n",
      "environement grid size (nx x ny ): 31 x 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ad181/RemoteDir/ada_multigrid_ppo/utils/custom_eval_callback.py:291: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f5b4c16d5f8> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f5b4c165b38>\n",
      "  warnings.warn(\"Training and eval env are not of the same type\" f\"{self.training_env} != {self.eval_env}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10014322 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.3       |\n",
      "|    explained_variance   | 0.987      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0251    |\n",
      "|    n_updates            | 2160       |\n",
      "|    policy_gradient_loss | -0.0216    |\n",
      "|    std                  | 0.137      |\n",
      "|    value_loss           | 0.000743   |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 55808\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.848     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 231       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1323661 |\n",
      "|    clip_fraction        | 0.635     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 12.4      |\n",
      "|    explained_variance   | 0.978     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0398   |\n",
      "|    n_updates            | 2180      |\n",
      "|    policy_gradient_loss | -0.0219   |\n",
      "|    std                  | 0.136     |\n",
      "|    value_loss           | 0.00103   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 56320\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09261511 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 2200       |\n",
      "|    policy_gradient_loss | -0.0224    |\n",
      "|    std                  | 0.136      |\n",
      "|    value_loss           | 0.00105    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 56832\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.849       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.097602114 |\n",
      "|    clip_fraction        | 0.619       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.5        |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 2220        |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 0.00105     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 57344\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.849      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 222        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08731206 |\n",
      "|    clip_fraction        | 0.623      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 2240       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00102    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 57856\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09121763 |\n",
      "|    clip_fraction        | 0.635      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0534    |\n",
      "|    n_updates            | 2260       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00104    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 58368\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10696659 |\n",
      "|    clip_fraction        | 0.634      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.6       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0155    |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    std                  | 0.135      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 58880\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.85       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 225        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11097249 |\n",
      "|    clip_fraction        | 0.622      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.7       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 2300       |\n",
      "|    policy_gradient_loss | -0.0203    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 59392\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 232        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11143454 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 2320       |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 59904\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 234        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09158735 |\n",
      "|    clip_fraction        | 0.621      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.8       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.047     |\n",
      "|    n_updates            | 2340       |\n",
      "|    policy_gradient_loss | -0.0212    |\n",
      "|    std                  | 0.134      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 60416\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.113920406 |\n",
      "|    clip_fraction        | 0.645       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 12.8        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0631     |\n",
      "|    n_updates            | 2360        |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    std                  | 0.134       |\n",
      "|    value_loss           | 0.00112     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 34 seconds\n",
      "\n",
      "Total episode rollouts: 60928\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.851      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 230        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12671539 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.036      |\n",
      "|    n_updates            | 2380       |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00111    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 61440\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11784478 |\n",
      "|    clip_fraction        | 0.655      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0662    |\n",
      "|    n_updates            | 2400       |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 40 seconds\n",
      "\n",
      "Total episode rollouts: 61952\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12649992 |\n",
      "|    clip_fraction        | 0.631      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 12.9       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.025     |\n",
      "|    n_updates            | 2420       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.133      |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 62464\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.852     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 222       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1065713 |\n",
      "|    clip_fraction        | 0.638     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13        |\n",
      "|    explained_variance   | 0.98      |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.0233    |\n",
      "|    n_updates            | 2440      |\n",
      "|    policy_gradient_loss | -0.0212   |\n",
      "|    std                  | 0.132     |\n",
      "|    value_loss           | 0.00124   |\n",
      "---------------------------------------\n",
      "policy iteration runtime: 31 seconds\n",
      "\n",
      "Total episode rollouts: 62976\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 233        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10749912 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0196     |\n",
      "|    n_updates            | 2460       |\n",
      "|    policy_gradient_loss | -0.0196    |\n",
      "|    std                  | 0.132      |\n",
      "|    value_loss           | 0.00115    |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 38 seconds\n",
      "\n",
      "Total episode rollouts: 63488\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.115768865 |\n",
      "|    clip_fraction        | 0.63        |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.1        |\n",
      "|    explained_variance   | 0.981       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0282     |\n",
      "|    n_updates            | 2480        |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    std                  | 0.132       |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 64000\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 231        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12533017 |\n",
      "|    clip_fraction        | 0.641      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.1       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0925    |\n",
      "|    n_updates            | 2500       |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00114    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 64512\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10273703 |\n",
      "|    clip_fraction        | 0.63       |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.2       |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0791     |\n",
      "|    n_updates            | 2520       |\n",
      "|    policy_gradient_loss | -0.0161    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00122    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 65024\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11630658 |\n",
      "|    clip_fraction        | 0.636      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0429     |\n",
      "|    n_updates            | 2540       |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00106    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 30 seconds\n",
      "\n",
      "Total episode rollouts: 65536\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 223        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09785478 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0782    |\n",
      "|    n_updates            | 2560       |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    std                  | 0.131      |\n",
      "|    value_loss           | 0.00107    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 35 seconds\n",
      "\n",
      "Total episode rollouts: 66048\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 5           |\n",
      "|    mean_reward          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 2560        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.106438756 |\n",
      "|    clip_fraction        | 0.624       |\n",
      "|    clip_range           | 0.15        |\n",
      "|    entropy_loss         | 13.3        |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0326     |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    std                  | 0.131       |\n",
      "|    value_loss           | 0.00109     |\n",
      "-----------------------------------------\n",
      "policy iteration runtime: 33 seconds\n",
      "\n",
      "Total episode rollouts: 66560\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 227        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12306688 |\n",
      "|    clip_fraction        | 0.638      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.3       |\n",
      "|    explained_variance   | 0.98       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0494    |\n",
      "|    n_updates            | 2600       |\n",
      "|    policy_gradient_loss | -0.0191    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.0012     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 37 seconds\n",
      "\n",
      "Total episode rollouts: 67072\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "New best mean reward!\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 5         |\n",
      "|    mean_reward          | 0.853     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 230       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 2560      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1171417 |\n",
      "|    clip_fraction        | 0.646     |\n",
      "|    clip_range           | 0.15      |\n",
      "|    entropy_loss         | 13.4      |\n",
      "|    explained_variance   | 0.981     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0134   |\n",
      "|    n_updates            | 2620      |\n",
      "|    policy_gradient_loss | -0.0226   |\n",
      "|    std                  | 0.13      |\n",
      "|    value_loss           | 0.00115   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy iteration runtime: 32 seconds\n",
      "\n",
      "Total episode rollouts: 67584\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 229        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13005471 |\n",
      "|    clip_fraction        | 0.648      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.982      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.003      |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0232    |\n",
      "|    std                  | 0.13       |\n",
      "|    value_loss           | 0.0011     |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 68096\n",
      "\n",
      "Eval num_timesteps=2560, episode_reward=0.85 +/- 0.00\n",
      "Episode length: 5.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 5          |\n",
      "|    mean_reward          | 0.852      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 224        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 2560       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10918467 |\n",
      "|    clip_fraction        | 0.633      |\n",
      "|    clip_range           | 0.15       |\n",
      "|    entropy_loss         | 13.5       |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 2660       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    std                  | 0.129      |\n",
      "|    value_loss           | 0.00101    |\n",
      "----------------------------------------\n",
      "policy iteration runtime: 39 seconds\n",
      "\n",
      "Total episode rollouts: 68608\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "window.mpl = {};\n",
       "\n",
       "\n",
       "mpl.get_websocket_type = function() {\n",
       "    if (typeof(WebSocket) !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof(MozWebSocket) !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert('Your browser does not have WebSocket support. ' +\n",
       "              'Please try Chrome, Safari or Firefox  6. ' +\n",
       "              'Firefox 4 and 5 are also supported but you ' +\n",
       "              'have to enable WebSockets in about:config.');\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = (this.ws.binaryType != undefined);\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById(\"mpl-warnings\");\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent = (\n",
       "                \"This browser does not support binary websocket messages. \" +\n",
       "                    \"Performance may be slow.\");\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = $('<div/>');\n",
       "    this._root_extra_style(this.root)\n",
       "    this.root.attr('style', 'display: inline-block');\n",
       "\n",
       "    $(parent_element).append(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen =  function () {\n",
       "            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n",
       "            fig.send_message(\"send_image_mode\", {});\n",
       "            if (mpl.ratio != 1) {\n",
       "                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n",
       "            }\n",
       "            fig.send_message(\"refresh\", {});\n",
       "        }\n",
       "\n",
       "    this.imageObj.onload = function() {\n",
       "            if (fig.image_mode == 'full') {\n",
       "                // Full images could contain transparency (where diff images\n",
       "                // almost always do), so we need to clear the canvas so that\n",
       "                // there is no ghosting.\n",
       "                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "            }\n",
       "            fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "        };\n",
       "\n",
       "    this.imageObj.onunload = function() {\n",
       "        fig.ws.close();\n",
       "    }\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_header = function() {\n",
       "    var titlebar = $(\n",
       "        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n",
       "        'ui-helper-clearfix\"/>');\n",
       "    var titletext = $(\n",
       "        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n",
       "        'text-align: center; padding: 3px;\"/>');\n",
       "    titlebar.append(titletext)\n",
       "    this.root.append(titlebar);\n",
       "    this.header = titletext[0];\n",
       "}\n",
       "\n",
       "\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(canvas_div) {\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = $('<div/>');\n",
       "\n",
       "    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n",
       "\n",
       "    function canvas_keyboard_event(event) {\n",
       "        return fig.key_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    canvas_div.keydown('key_press', canvas_keyboard_event);\n",
       "    canvas_div.keyup('key_release', canvas_keyboard_event);\n",
       "    this.canvas_div = canvas_div\n",
       "    this._canvas_extra_style(canvas_div)\n",
       "    this.root.append(canvas_div);\n",
       "\n",
       "    var canvas = $('<canvas/>');\n",
       "    canvas.addClass('mpl-canvas');\n",
       "    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n",
       "\n",
       "    this.canvas = canvas[0];\n",
       "    this.context = canvas[0].getContext(\"2d\");\n",
       "\n",
       "    var backingStore = this.context.backingStorePixelRatio ||\n",
       "\tthis.context.webkitBackingStorePixelRatio ||\n",
       "\tthis.context.mozBackingStorePixelRatio ||\n",
       "\tthis.context.msBackingStorePixelRatio ||\n",
       "\tthis.context.oBackingStorePixelRatio ||\n",
       "\tthis.context.backingStorePixelRatio || 1;\n",
       "\n",
       "    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "\n",
       "    var rubberband = $('<canvas/>');\n",
       "    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n",
       "\n",
       "    var pass_mouse_events = true;\n",
       "\n",
       "    canvas_div.resizable({\n",
       "        start: function(event, ui) {\n",
       "            pass_mouse_events = false;\n",
       "        },\n",
       "        resize: function(event, ui) {\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "        stop: function(event, ui) {\n",
       "            pass_mouse_events = true;\n",
       "            fig.request_resize(ui.size.width, ui.size.height);\n",
       "        },\n",
       "    });\n",
       "\n",
       "    function mouse_event_fn(event) {\n",
       "        if (pass_mouse_events)\n",
       "            return fig.mouse_event(event, event['data']);\n",
       "    }\n",
       "\n",
       "    rubberband.mousedown('button_press', mouse_event_fn);\n",
       "    rubberband.mouseup('button_release', mouse_event_fn);\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband.mousemove('motion_notify', mouse_event_fn);\n",
       "\n",
       "    rubberband.mouseenter('figure_enter', mouse_event_fn);\n",
       "    rubberband.mouseleave('figure_leave', mouse_event_fn);\n",
       "\n",
       "    canvas_div.on(\"wheel\", function (event) {\n",
       "        event = event.originalEvent;\n",
       "        event['data'] = 'scroll'\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        mouse_event_fn(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.append(canvas);\n",
       "    canvas_div.append(rubberband);\n",
       "\n",
       "    this.rubberband = rubberband;\n",
       "    this.rubberband_canvas = rubberband[0];\n",
       "    this.rubberband_context = rubberband[0].getContext(\"2d\");\n",
       "    this.rubberband_context.strokeStyle = \"#000000\";\n",
       "\n",
       "    this._resize_canvas = function(width, height) {\n",
       "        // Keep the size of the canvas, canvas container, and rubber band\n",
       "        // canvas in synch.\n",
       "        canvas_div.css('width', width)\n",
       "        canvas_div.css('height', height)\n",
       "\n",
       "        canvas.attr('width', width * mpl.ratio);\n",
       "        canvas.attr('height', height * mpl.ratio);\n",
       "        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n",
       "\n",
       "        rubberband.attr('width', width);\n",
       "        rubberband.attr('height', height);\n",
       "    }\n",
       "\n",
       "    // Set the figure to an initial 600x600px, this will subsequently be updated\n",
       "    // upon first draw.\n",
       "    this._resize_canvas(600, 600);\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus () {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            // put a spacer in here.\n",
       "            continue;\n",
       "        }\n",
       "        var button = $('<button/>');\n",
       "        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n",
       "                        'ui-button-icon-only');\n",
       "        button.attr('role', 'button');\n",
       "        button.attr('aria-disabled', 'false');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "\n",
       "        var icon_img = $('<span/>');\n",
       "        icon_img.addClass('ui-button-icon-primary ui-icon');\n",
       "        icon_img.addClass(image);\n",
       "        icon_img.addClass('ui-corner-all');\n",
       "\n",
       "        var tooltip_span = $('<span/>');\n",
       "        tooltip_span.addClass('ui-button-text');\n",
       "        tooltip_span.html(tooltip);\n",
       "\n",
       "        button.append(icon_img);\n",
       "        button.append(tooltip_span);\n",
       "\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    var fmt_picker_span = $('<span/>');\n",
       "\n",
       "    var fmt_picker = $('<select/>');\n",
       "    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n",
       "    fmt_picker_span.append(fmt_picker);\n",
       "    nav_element.append(fmt_picker_span);\n",
       "    this.format_dropdown = fmt_picker[0];\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = $(\n",
       "            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n",
       "        fmt_picker.append(option);\n",
       "    }\n",
       "\n",
       "    // Add hover states to the ui-buttons\n",
       "    $( \".ui-button\" ).hover(\n",
       "        function() { $(this).addClass(\"ui-state-hover\");},\n",
       "        function() { $(this).removeClass(\"ui-state-hover\");}\n",
       "    );\n",
       "\n",
       "    var status_bar = $('<span class=\"mpl-message\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_message = function(type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function() {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function(fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1]);\n",
       "        fig.send_message(\"refresh\", {});\n",
       "    };\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n",
       "    var x0 = msg['x0'] / mpl.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n",
       "    var x1 = msg['x1'] / mpl.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function(fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch(cursor)\n",
       "    {\n",
       "    case 0:\n",
       "        cursor = 'pointer';\n",
       "        break;\n",
       "    case 1:\n",
       "        cursor = 'default';\n",
       "        break;\n",
       "    case 2:\n",
       "        cursor = 'crosshair';\n",
       "        break;\n",
       "    case 3:\n",
       "        cursor = 'move';\n",
       "        break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_message = function(fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function(fig, msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message(\"ack\", {});\n",
       "}\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function(fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = \"image/png\";\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src);\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data);\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig[\"handle_\" + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "}\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function(e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e)\n",
       "        e = window.event;\n",
       "    if (e.target)\n",
       "        targ = e.target;\n",
       "    else if (e.srcElement)\n",
       "        targ = e.srcElement;\n",
       "    if (targ.nodeType == 3) // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "\n",
       "    // jQuery normalizes the pageX and pageY\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    // offset() returns the position of the element relative to the document\n",
       "    var x = e.pageX - $(targ).offset().left;\n",
       "    var y = e.pageY - $(targ).offset().top;\n",
       "\n",
       "    return {\"x\": x, \"y\": y};\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys (original) {\n",
       "  return Object.keys(original).reduce(function (obj, key) {\n",
       "    if (typeof original[key] !== 'object')\n",
       "        obj[key] = original[key]\n",
       "    return obj;\n",
       "  }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function(event, name) {\n",
       "    var canvas_pos = mpl.findpos(event)\n",
       "\n",
       "    if (name === 'button_press')\n",
       "    {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * mpl.ratio;\n",
       "    var y = canvas_pos.y * mpl.ratio;\n",
       "\n",
       "    this.send_message(name, {x: x, y: y, button: event.button,\n",
       "                             step: event.step,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.key_event = function(event, name) {\n",
       "\n",
       "    // Prevent repeat events\n",
       "    if (name == 'key_press')\n",
       "    {\n",
       "        if (event.which === this._key)\n",
       "            return;\n",
       "        else\n",
       "            this._key = event.which;\n",
       "    }\n",
       "    if (name == 'key_release')\n",
       "        this._key = null;\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which != 17)\n",
       "        value += \"ctrl+\";\n",
       "    if (event.altKey && event.which != 18)\n",
       "        value += \"alt+\";\n",
       "    if (event.shiftKey && event.which != 16)\n",
       "        value += \"shift+\";\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, {key: value,\n",
       "                             guiEvent: simpleKeys(event)});\n",
       "    return false;\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function(name) {\n",
       "    if (name == 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message(\"toolbar_button\", {name: name});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function() {\n",
       "        comm.close()\n",
       "    };\n",
       "    ws.send = function(m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function(msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data'])\n",
       "    });\n",
       "    return ws;\n",
       "}\n",
       "\n",
       "mpl.mpl_figure_comm = function(comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = $(\"#\" + id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm)\n",
       "\n",
       "    function ondownload(figure, format) {\n",
       "        window.open(figure.imageObj.src);\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy,\n",
       "                           ondownload,\n",
       "                           element.get(0));\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element.get(0);\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error(\"Failed to find cell for figure\", id, fig);\n",
       "        return;\n",
       "    }\n",
       "\n",
       "    var output_index = fig.cell_info[2]\n",
       "    var cell = fig.cell_info[0];\n",
       "\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function(fig, msg) {\n",
       "    var width = fig.canvas.width/mpl.ratio\n",
       "    fig.root.unbind('remove')\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable()\n",
       "    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n",
       "    fig.close_ws(fig, msg);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.close_ws = function(fig, msg){\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function(remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width/mpl.ratio\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function() {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message(\"ack\", {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () { fig.push_to_output() }, 1000);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function() {\n",
       "    var fig = this;\n",
       "\n",
       "    var nav_element = $('<div/>');\n",
       "    nav_element.attr('style', 'width: 100%');\n",
       "    this.root.append(nav_element);\n",
       "\n",
       "    // Define a callback function for later on.\n",
       "    function toolbar_event(event) {\n",
       "        return fig.toolbar_button_onclick(event['data']);\n",
       "    }\n",
       "    function toolbar_mouse_event(event) {\n",
       "        return fig.toolbar_button_onmouseover(event['data']);\n",
       "    }\n",
       "\n",
       "    for(var toolbar_ind in mpl.toolbar_items){\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) { continue; };\n",
       "\n",
       "        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n",
       "        button.click(method_name, toolbar_event);\n",
       "        button.mouseover(tooltip, toolbar_mouse_event);\n",
       "        nav_element.append(button);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n",
       "    nav_element.append(status_bar);\n",
       "    this.message = status_bar[0];\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n",
       "    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n",
       "    button.click(function (evt) { fig.handle_close(fig, {}); } );\n",
       "    button.mouseover('Stop Interaction', toolbar_mouse_event);\n",
       "    buttongrp.append(button);\n",
       "    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n",
       "    titlebar.prepend(buttongrp);\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function(el){\n",
       "    var fig = this\n",
       "    el.on(\"remove\", function(){\n",
       "\tfig.close_ws(fig, {});\n",
       "    });\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function(el){\n",
       "    // this is important to make the div 'focusable\n",
       "    el.attr('tabindex', 0)\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    }\n",
       "    else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "\n",
       "}\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function(event, name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager)\n",
       "        manager = IPython.keyboard_manager;\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which == 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.handle_save = function(fig, msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "}\n",
       "\n",
       "\n",
       "mpl.find_output_cell = function(html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i=0; i<ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code'){\n",
       "            for (var j=0; j<cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] == html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel != null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"599.4666666666667\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for seed in range(1,4):\n",
    "    model = multigrid_framework(env_train, \n",
    "                                generate_model,\n",
    "                                generate_callback, \n",
    "                                delta_pcent=0.2, \n",
    "                                n=25,\n",
    "                                grid_fidelity_factor_array =[0.25, 0.5, 1.0],\n",
    "                                episode_limit_array=[50000, 50000, 50000], \n",
    "                                log_dir=log_dir,\n",
    "                                seed=seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
